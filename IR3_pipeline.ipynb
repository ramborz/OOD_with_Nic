{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "grad_layer = 4\n",
    "\n",
    "def pretty(vector):\n",
    "    if type(vector) is list:\n",
    "        vlist = vector\n",
    "    elif type(vector) is np.ndarray:\n",
    "        vlist = vector.reshape(-1).tolist()\n",
    "    else:\n",
    "        vlist = vector.view(-1).tolist()\n",
    "\n",
    "    return \"[\" + \", \".join(\"{:+.4f}\".format(vi) for vi in vlist) + \"]\"\n",
    "\n",
    "\n",
    "# Optimizer Class to maximize loss of adversarial dataset\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.9, epsilon=1e-8):\n",
    "        self.device = torch.device(device)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.m_hat = None\n",
    "        self.v_hat = None\n",
    "        self.initialize = False\n",
    "\n",
    "    # Grad = adv_gradient\n",
    "    # Iternum = iteration\n",
    "    # theta = adv_images\n",
    "    # Gradient ascent\n",
    "    def update(self, grad, iternum, theta):\n",
    "        if not self.initialize:\n",
    "            self.m = (1 - self.beta1) * grad\n",
    "            self.v = (1 - self.beta2) * grad ** 2\n",
    "            self.initialize = True\n",
    "        else:\n",
    "            assert self.m.shape == grad.shape\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "\n",
    "        self.m_hat = self.m / (1 - self.beta1 ** iternum)\n",
    "        self.v_hat = self.v / (1 - self.beta2 ** iternum)\n",
    "        return theta + self.lr * self.m_hat / (self.epsilon + torch.sqrt(self.v_hat))\n",
    "\n",
    "\n",
    "class IRNet_intorch(torch.nn.Module):\n",
    "    #'128-64-16'    \n",
    "    def __init__(self, input_size):\n",
    "        super(IRNet_intorch, self).__init__()\n",
    "        self.fc128 =nn.Linear(128, 128)\n",
    "        self.fc64 =nn.Linear(64, 64)\n",
    "        self.fc16 =nn.Linear(16, 16)\n",
    "\n",
    "        self.bn128 =nn.BatchNorm1d(128)\n",
    "        self.bn64 =nn.BatchNorm1d(64)\n",
    "        self.bn16 =nn.BatchNorm1d(16)\n",
    "      \n",
    "        self.relu = nn.ReLU()\n",
    "        self.inputlayer = nn.Linear(input_size, 128)\n",
    "\n",
    "        self.con128_64 = nn.Linear(128, 64)\n",
    "        self.con64_16 = nn.Linear(64,16)\n",
    "        self.output16 = nn.Linear(16,1)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inputlayer(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc128(x)\n",
    "        x = self.bn128(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con128_64(x)\n",
    "    \n",
    "        x_res = x\n",
    "        x = self.fc64(x)\n",
    "        x = self.bn64(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con64_16(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc16(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res   \n",
    "\n",
    "        x = self.output16(x)\n",
    "        return x\n",
    "\n",
    "class StableAL():\n",
    "    def __init__(self, environment):\n",
    "        self.weights = None\n",
    "        self.model = None\n",
    "        self.weight_grad = None\n",
    "        self.xa_grad = None\n",
    "        self.theta_grad = None\n",
    "        self.gamma = None\n",
    "        self.adversarial_data = None\n",
    "        self.loss_criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.adv_based_on = None\n",
    "        self.adv_again = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "       \n",
    "        # init\n",
    "        # Number of covariates\n",
    "        dim_x=150\n",
    "        self.model = IRNet_intorch(dim_x).to(device)\n",
    "        # Covariate Weights\n",
    "        self.weights = torch.zeros(dim_x).reshape(-1, 1) + 100.0\n",
    "        self.weights = self.weights.to(device)\n",
    "    def cost_function(self, x, x_adv):\n",
    "        # Variable cost level where the weights determine the cost level\n",
    "        cost = torch.mean(((x - x_adv) ** 2).mm(self.weights)).to(device)\n",
    "        return cost\n",
    "\n",
    "    # Loss across Training environments\n",
    "    # Self.loss_criterion = MSELoss\n",
    "    def r(self, environments, alpha=10.0):\n",
    "        result = 0.0\n",
    "        env_loss = []\n",
    "        for x_e, y_e in environments:\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            env_loss.append(self.loss_criterion(self.model(x_e), y_e))\n",
    "        env_loss = torch.Tensor(env_loss)\n",
    "        max_index = torch.argmax(env_loss)\n",
    "        min_index = torch.argmin(env_loss)\n",
    "\n",
    "        for idx, (x_e, y_e) in enumerate(environments):\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            if idx == max_index:\n",
    "                result += (alpha+1)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            elif idx == min_index:\n",
    "                result += (1-alpha)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            else:\n",
    "                result += self.loss_criterion(self.model(x_e),y_e)\n",
    "        return result\n",
    "\n",
    "  \n",
    "    # generate adversarial data\n",
    "    # Maximize the loss using their own ADAM.update method(their own optimizer)\n",
    "    def attack(self, gamma, data, step):\n",
    "        attack_lr = 7e-3\n",
    "        images, labels = data\n",
    "        images_adv = images.clone().detach()\n",
    "        \n",
    "        optimizer = Adam(learning_rate=attack_lr)\n",
    "\n",
    "        for i in range(step):\n",
    "            if images_adv.grad is not None:\n",
    "                images_adv.grad.data.zero_()\n",
    "\n",
    "\n",
    "            images_adv=images_adv.to(device)\n",
    "            images_adv.requires_grad_(True)\n",
    "            outputs = self.model(images_adv)\n",
    "           \n",
    "            labels = labels.float().to(device)\n",
    "            images = images.to(device)\n",
    "            loss = self.loss_criterion(\n",
    "                outputs, labels) - gamma * self.cost_function(images, images_adv)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            images_adv.data = optimizer.update(images_adv.grad, i + 1, images_adv)\n",
    "\n",
    "        self.weight_grad = -2 * gamma * attack_lr * (images_adv - images)\n",
    "        temp_image = images_adv.clone().detach()\n",
    "        temp_label = labels.clone().detach()\n",
    "        self.adversarial_data = (temp_image, temp_label)\n",
    "        return images_adv, labels\n",
    "\n",
    "   \n",
    "    # Optimizes the model paremeters such that the loss is minimized\n",
    "    # on the adversarial data from self.attack\n",
    "    def train_theta(self, data, epochs, epoch_attack, gamma, end_flag=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        # For __ Theta Epochs\n",
    "        for i in range(epochs):\n",
    "            if i % 5 == 0 or not end_flag:\n",
    "                images_adv, labels = self.attack(gamma, data, step=epoch_attack)\n",
    "\n",
    "            else:\n",
    "                self.adv_again = self.adversarial_data\n",
    "                images_adv, labels = self.attack(gamma, self.adversarial_data, step=epoch_attack)\n",
    "            self.adv_based_on = data\n",
    "                \n",
    "            #print(f\"original data: {data[0].shape}\")\n",
    "            # print(f\"attack data: {images_adv.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            images_adv =images_adv.to(device)\n",
    "            outputs = self.model(images_adv)\n",
    "            loss = self.loss_criterion(outputs, labels.float()) \n",
    "            \n",
    "\n",
    "            \n",
    "            if self.xa_grad is None:\n",
    "                dtheta_dx = []\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[grad_layer].reshape(-1)\n",
    "\n",
    "                # size dloss = model.para size\n",
    "                for name1, param in self.model.named_parameters():\n",
    "                    print (f\"grad: {name1}          {param.shape}\")\n",
    "\n",
    "                #time.sleep(5.5)    # Pause 5.5 seconds\n",
    "                print(f\"dloss_dtheta.shape:  {dloss_dtheta.shape[0]}\")\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    #print(f\"dloss_dtheta.shape[0]:j     {j}\")\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach()) \n",
    "   \n",
    "                self.xa_grad = torch.stack(dtheta_dx,1).detach()\n",
    "                \n",
    "            else:\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[grad_layer].reshape(-1)\n",
    "                dtheta_dx = []\n",
    "\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach())\n",
    "                self.xa_grad += torch.stack(dtheta_dx, 1).detach()\n",
    "                \n",
    "            #print(f\"xa_grad size: {self.xa_grad.shape}\")\n",
    "            del dtheta_dx\n",
    "            del dloss_dtheta\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            #print('%d | %.4f | %s'%(i, loss, pretty(self.model.layer[4].weight)))\n",
    "            #if i % 1000 == 999:\n",
    "              \n",
    "\n",
    "            #print(f\"loss?\")\n",
    "            loss.backward(retain_graph=True)\n",
    "            #print(f\"step?\")\n",
    "            optimizer.step()\n",
    "            #print(f\"step!\")\n",
    "        self.xa_grad *= (-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "\n",
    "Rsplt_testset =pd.read_csv('../dataset/Rsplt_testset.csv', index_col=None)\n",
    "Xshft_testset =pd.read_csv('../dataset/Xshft_testset.csv', index_col=None)\n",
    "pizeo_testset =pd.read_csv('../dataset/pizeo_testset.csv', index_col=None)\n",
    "statY_testset =pd.read_csv('../dataset/statY_testset.csv', index_col=None)\n",
    "infoY_testset =pd.read_csv('../dataset/infoY_testset.csv', index_col=None)\n",
    "final_train   =pd.read_csv('../dataset/final_trainset.csv',index_col=None)  \n",
    "\n",
    "Rsplt_testset1 =pd.read_csv('../dataset/Rsplt_testset1.csv', index_col=None)\n",
    "Rsplt_testset2 =pd.read_csv('../dataset/Rsplt_testset2.csv', index_col=None)\n",
    "Rsplt_testset3 =pd.read_csv('../dataset/Rsplt_testset3.csv', index_col=None)\n",
    "Rsplt_testset4 =pd.read_csv('../dataset/Rsplt_testset4.csv', index_col=None)\n",
    "Rsplt_testset5 =pd.read_csv('../dataset/Rsplt_testset5.csv', index_col=None)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define a custom PyTorch Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df.drop(columns=['delta_e','pretty_comp']).values\n",
    "        self.labels = df['delta_e'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "    def getSALdata(self):\n",
    "        input = np.array(self.inputs[:].tolist())\n",
    "        label = np.array(self.labels[:].tolist())\n",
    "        return (input, label)\n",
    "        \n",
    "class RecurrentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['data'].values\n",
    "        self.labels = df['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Step 4: Use DataLoader to create batches\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = MyDataset(final_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "Rsplt_test_dataset = MyDataset(Rsplt_testset)\n",
    "Xshft_test_dataset = MyDataset(Xshft_testset)\n",
    "pizeo_test_dataset = MyDataset(pizeo_testset)\n",
    "statY_test_dataset = MyDataset(statY_testset)\n",
    "infoY_test_dataset = MyDataset(infoY_testset)\n",
    "\n",
    "Rsplt_testset1_dataset = MyDataset(Rsplt_testset1)\n",
    "Rsplt_testset2_dataset = MyDataset(Rsplt_testset2)\n",
    "Rsplt_testset3_dataset = MyDataset(Rsplt_testset3)\n",
    "Rsplt_testset4_dataset = MyDataset(Rsplt_testset4)\n",
    "Rsplt_testset5_dataset = MyDataset(Rsplt_testset5)\n",
    "\n",
    "Rsplt_testset1_loader = DataLoader(Rsplt_testset1_dataset, batch_size=len(Rsplt_testset1))\n",
    "Rsplt_testset2_loader = DataLoader(Rsplt_testset2_dataset, batch_size=len(Rsplt_testset2))\n",
    "Rsplt_testset3_loader = DataLoader(Rsplt_testset3_dataset, batch_size=len(Rsplt_testset3))\n",
    "Rsplt_testset4_loader = DataLoader(Rsplt_testset4_dataset, batch_size=len(Rsplt_testset4))\n",
    "Rsplt_testset5_loader = DataLoader(Rsplt_testset5_dataset, batch_size=len(Rsplt_testset5))\n",
    "\n",
    "Rsplt_test_loader = DataLoader(Rsplt_test_dataset, batch_size=len(Rsplt_testset))\n",
    "Xshft_test_loader = DataLoader(Xshft_test_dataset, batch_size=len(Xshft_testset))\n",
    "pizeo_test_loader = DataLoader(pizeo_test_dataset, batch_size=len(pizeo_testset))\n",
    "statY_test_loader = DataLoader(statY_test_dataset, batch_size=len(statY_testset))\n",
    "infoY_test_loader = DataLoader(infoY_test_dataset, batch_size=len(infoY_testset))\n",
    "\n",
    "\n",
    "data=train_dataset.getSALdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = StableAL([train_dataset.getSALdata()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current in epoch    0      batch 0\n",
      "current in epoch    0      batch 1\n",
      "current in epoch    0      batch 2\n",
      "current in epoch    0      batch 3\n",
      "current in epoch    0      batch 4\n",
      "current in epoch    0      batch 5\n",
      "current in epoch    0      batch 6\n",
      "current in epoch    0      batch 7\n",
      "current in epoch    0      batch 8\n",
      "current in epoch    0      batch 9\n",
      "current in epoch    0      batch 10\n",
      "current in epoch    0      batch 11\n",
      "current in epoch    0      batch 12\n",
      "current in epoch    0      batch 13\n",
      "current in epoch    0      batch 14\n",
      "current in epoch    0      batch 15\n",
      "current in epoch    0      batch 16\n",
      "current in epoch    0      batch 17\n",
      "current in epoch    0      batch 18\n",
      "current in epoch    0      batch 19\n",
      "current in epoch    0      batch 20\n",
      "current in epoch    0      batch 21\n",
      "current in epoch    0      batch 22\n",
      "current in epoch    0      batch 23\n",
      "current in epoch    0      batch 24\n",
      "current in epoch    0      batch 25\n",
      "current in epoch    0      batch 26\n",
      "current in epoch    0      batch 27\n",
      "========================================\n",
      "Epoch 1/1000 - partial_train_loss: 328.8534 \n",
      "sorting training set\n",
      "Epoch 1/1000 - Training loss: 52.2736 \n",
      "========================================\n",
      "Epoch: [1/1000], TrainLoss: 55.968190151179456\n",
      "current in epoch    1      batch 0\n",
      "current in epoch    1      batch 1\n",
      "current in epoch    1      batch 2\n",
      "current in epoch    1      batch 3\n",
      "current in epoch    1      batch 4\n",
      "current in epoch    1      batch 5\n",
      "current in epoch    1      batch 6\n",
      "current in epoch    1      batch 7\n",
      "current in epoch    1      batch 8\n",
      "current in epoch    1      batch 9\n",
      "current in epoch    1      batch 10\n",
      "current in epoch    1      batch 11\n",
      "current in epoch    1      batch 12\n",
      "current in epoch    1      batch 13\n",
      "current in epoch    1      batch 14\n",
      "current in epoch    1      batch 15\n",
      "current in epoch    1      batch 16\n",
      "current in epoch    1      batch 17\n",
      "current in epoch    1      batch 18\n",
      "current in epoch    1      batch 19\n",
      "current in epoch    1      batch 20\n",
      "current in epoch    1      batch 21\n",
      "current in epoch    1      batch 22\n",
      "current in epoch    1      batch 23\n",
      "current in epoch    1      batch 24\n",
      "current in epoch    1      batch 25\n",
      "current in epoch    1      batch 26\n",
      "current in epoch    1      batch 27\n",
      "========================================\n",
      "Epoch 2/1000 - partial_train_loss: 315.2643 \n",
      "Epoch: [2/1000], TrainLoss: 29.026453222547257\n",
      "current in epoch    2      batch 0\n",
      "current in epoch    2      batch 1\n",
      "current in epoch    2      batch 2\n",
      "current in epoch    2      batch 3\n",
      "current in epoch    2      batch 4\n",
      "current in epoch    2      batch 5\n",
      "current in epoch    2      batch 6\n",
      "current in epoch    2      batch 7\n",
      "current in epoch    2      batch 8\n",
      "current in epoch    2      batch 9\n",
      "current in epoch    2      batch 10\n",
      "current in epoch    2      batch 11\n",
      "current in epoch    2      batch 12\n",
      "current in epoch    2      batch 13\n",
      "current in epoch    2      batch 14\n",
      "current in epoch    2      batch 15\n",
      "current in epoch    2      batch 16\n",
      "current in epoch    2      batch 17\n",
      "current in epoch    2      batch 18\n",
      "current in epoch    2      batch 19\n",
      "current in epoch    2      batch 20\n",
      "current in epoch    2      batch 21\n",
      "current in epoch    2      batch 22\n",
      "current in epoch    2      batch 23\n",
      "current in epoch    2      batch 24\n",
      "current in epoch    2      batch 25\n",
      "current in epoch    2      batch 26\n",
      "current in epoch    2      batch 27\n",
      "========================================\n",
      "Epoch 3/1000 - partial_train_loss: 193.8632 \n",
      "Epoch: [3/1000], TrainLoss: 21.53730641092573\n",
      "current in epoch    3      batch 0\n",
      "current in epoch    3      batch 1\n",
      "current in epoch    3      batch 2\n",
      "current in epoch    3      batch 3\n",
      "current in epoch    3      batch 4\n",
      "current in epoch    3      batch 5\n",
      "current in epoch    3      batch 6\n",
      "current in epoch    3      batch 7\n",
      "current in epoch    3      batch 8\n",
      "current in epoch    3      batch 9\n",
      "current in epoch    3      batch 10\n",
      "current in epoch    3      batch 11\n",
      "current in epoch    3      batch 12\n",
      "current in epoch    3      batch 13\n",
      "current in epoch    3      batch 14\n",
      "current in epoch    3      batch 15\n",
      "current in epoch    3      batch 16\n",
      "current in epoch    3      batch 17\n",
      "current in epoch    3      batch 18\n",
      "current in epoch    3      batch 19\n",
      "current in epoch    3      batch 20\n",
      "current in epoch    3      batch 21\n",
      "current in epoch    3      batch 22\n",
      "current in epoch    3      batch 23\n",
      "current in epoch    3      batch 24\n",
      "current in epoch    3      batch 25\n",
      "current in epoch    3      batch 26\n",
      "current in epoch    3      batch 27\n",
      "========================================\n",
      "Epoch 4/1000 - partial_train_loss: 79.6234 \n",
      "Epoch: [4/1000], TrainLoss: 5.437325360519545\n",
      "current in epoch    4      batch 0\n",
      "current in epoch    4      batch 1\n",
      "current in epoch    4      batch 2\n",
      "current in epoch    4      batch 3\n",
      "current in epoch    4      batch 4\n",
      "current in epoch    4      batch 5\n",
      "current in epoch    4      batch 6\n",
      "current in epoch    4      batch 7\n",
      "current in epoch    4      batch 8\n",
      "current in epoch    4      batch 9\n",
      "current in epoch    4      batch 10\n",
      "current in epoch    4      batch 11\n",
      "current in epoch    4      batch 12\n",
      "current in epoch    4      batch 13\n",
      "current in epoch    4      batch 14\n",
      "current in epoch    4      batch 15\n",
      "current in epoch    4      batch 16\n",
      "current in epoch    4      batch 17\n",
      "current in epoch    4      batch 18\n",
      "current in epoch    4      batch 19\n",
      "current in epoch    4      batch 20\n",
      "current in epoch    4      batch 21\n",
      "current in epoch    4      batch 22\n",
      "current in epoch    4      batch 23\n",
      "current in epoch    4      batch 24\n",
      "current in epoch    4      batch 25\n",
      "current in epoch    4      batch 26\n",
      "current in epoch    4      batch 27\n",
      "========================================\n",
      "Epoch 5/1000 - partial_train_loss: 38.8962 \n",
      "Epoch: [5/1000], TrainLoss: 2.45315773997988\n",
      "current in epoch    5      batch 0\n",
      "grad: fc128.weight          torch.Size([128, 128])\n",
      "grad: fc128.bias          torch.Size([128])\n",
      "grad: fc64.weight          torch.Size([64, 64])\n",
      "grad: fc64.bias          torch.Size([64])\n",
      "grad: fc16.weight          torch.Size([16, 16])\n",
      "grad: fc16.bias          torch.Size([16])\n",
      "grad: bn128.weight          torch.Size([128])\n",
      "grad: bn128.bias          torch.Size([128])\n",
      "grad: bn64.weight          torch.Size([64])\n",
      "grad: bn64.bias          torch.Size([64])\n",
      "grad: bn16.weight          torch.Size([16])\n",
      "grad: bn16.bias          torch.Size([16])\n",
      "grad: inputlayer.weight          torch.Size([128, 150])\n",
      "grad: inputlayer.bias          torch.Size([128])\n",
      "grad: con128_64.weight          torch.Size([64, 128])\n",
      "grad: con128_64.bias          torch.Size([64])\n",
      "grad: con64_16.weight          torch.Size([16, 64])\n",
      "grad: con64_16.bias          torch.Size([16])\n",
      "grad: output16.weight          torch.Size([1, 16])\n",
      "grad: output16.bias          torch.Size([1])\n",
      "dloss_dtheta.shape:  256\n",
      "RLoss: 20181.412109375\n",
      "current in epoch    5      batch 1\n",
      "RLoss: 15074.18359375\n",
      "current in epoch    5      batch 2\n",
      "RLoss: 7593.19140625\n",
      "current in epoch    5      batch 3\n",
      "RLoss: 1952.63330078125\n",
      "current in epoch    5      batch 4\n",
      "RLoss: 341.28363037109375\n",
      "current in epoch    5      batch 5\n",
      "RLoss: 5.377368450164795\n",
      "current in epoch    5      batch 6\n",
      "RLoss: 2.8716654777526855\n",
      "current in epoch    5      batch 7\n",
      "RLoss: 42.517520904541016\n",
      "current in epoch    5      batch 8\n",
      "RLoss: 67.35313415527344\n",
      "current in epoch    5      batch 9\n",
      "RLoss: 513.6015014648438\n",
      "current in epoch    5      batch 10\n",
      "RLoss: 779.3715209960938\n",
      "current in epoch    5      batch 11\n",
      "RLoss: 40.74705505371094\n",
      "current in epoch    5      batch 12\n",
      "RLoss: 249.31243896484375\n",
      "current in epoch    5      batch 13\n",
      "RLoss: 47.13994598388672\n",
      "current in epoch    5      batch 14\n",
      "RLoss: 147.66806030273438\n",
      "current in epoch    5      batch 15\n",
      "RLoss: 436.69134521484375\n",
      "current in epoch    5      batch 16\n",
      "RLoss: 226.69566345214844\n",
      "current in epoch    5      batch 17\n",
      "RLoss: 52.6995849609375\n",
      "current in epoch    5      batch 18\n",
      "RLoss: 17.10421371459961\n",
      "current in epoch    5      batch 19\n",
      "RLoss: 70.6370620727539\n",
      "current in epoch    5      batch 20\n",
      "RLoss: 489.7483825683594\n",
      "current in epoch    5      batch 21\n",
      "RLoss: 1217.38916015625\n",
      "current in epoch    5      batch 22\n",
      "RLoss: 90.95475769042969\n",
      "current in epoch    5      batch 23\n",
      "RLoss: 136.95074462890625\n",
      "current in epoch    5      batch 24\n",
      "RLoss: 0.8169448375701904\n",
      "current in epoch    5      batch 25\n",
      "RLoss: 30.256017684936523\n",
      "current in epoch    5      batch 26\n",
      "RLoss: 75.11288452148438\n",
      "current in epoch    5      batch 27\n",
      "RLoss: 164.62127685546875\n",
      "========================================\n",
      "Epoch 6/1000 - partial_train_loss: 1344.8302 \n",
      "sorting training set\n",
      "Epoch 6/1000 - Training loss: 220.0785 \n",
      "========================================\n",
      "Epoch: [6/1000], TrainLoss: 232.79962572916392\n",
      "training Loss has not improved for 1 epochs.\n",
      "current in epoch    6      batch 0\n",
      "RLoss: 1564.35205078125\n",
      "current in epoch    6      batch 1\n",
      "RLoss: 800.9047241210938\n",
      "current in epoch    6      batch 2\n",
      "RLoss: 128.75975036621094\n",
      "current in epoch    6      batch 3\n",
      "RLoss: 875.2109375\n",
      "current in epoch    6      batch 4\n",
      "RLoss: 10.369552612304688\n",
      "current in epoch    6      batch 5\n",
      "RLoss: 735.044921875\n",
      "========================================\n",
      "Epoch 7/1000 - partial_train_loss: 521.6843 \n",
      "Epoch: [7/1000], TrainLoss: 498.56187656947543\n",
      "training Loss has not improved for 2 epochs.\n",
      "current in epoch    7      batch 0\n",
      "RLoss: 1152.4375\n",
      "current in epoch    7      batch 1\n",
      "RLoss: 3138.10595703125\n",
      "current in epoch    7      batch 2\n",
      "RLoss: 54.009212493896484\n",
      "current in epoch    7      batch 3\n",
      "RLoss: 1064.0810546875\n",
      "current in epoch    7      batch 4\n",
      "RLoss: 531.1851806640625\n",
      "current in epoch    7      batch 5\n",
      "RLoss: 153.28469848632812\n",
      "========================================\n",
      "Epoch 8/1000 - partial_train_loss: 954.3359 \n",
      "Epoch: [8/1000], TrainLoss: 109.29975400652204\n",
      "training Loss has not improved for 3 epochs.\n",
      "current in epoch    8      batch 0\n",
      "RLoss: 933.748779296875\n",
      "current in epoch    8      batch 1\n",
      "RLoss: 411.25604248046875\n",
      "current in epoch    8      batch 2\n",
      "RLoss: 3568.987548828125\n",
      "current in epoch    8      batch 3\n",
      "RLoss: 1746.7618408203125\n",
      "current in epoch    8      batch 4\n",
      "RLoss: 93.5585708618164\n",
      "current in epoch    8      batch 5\n",
      "RLoss: 610.9698486328125\n",
      "========================================\n",
      "Epoch 9/1000 - partial_train_loss: 896.9616 \n",
      "Epoch: [9/1000], TrainLoss: 463.3793825422014\n",
      "training Loss has not improved for 4 epochs.\n",
      "current in epoch    9      batch 0\n",
      "RLoss: 463.6466064453125\n",
      "current in epoch    9      batch 1\n",
      "RLoss: 756.2715454101562\n",
      "current in epoch    9      batch 2\n",
      "RLoss: 232.40847778320312\n",
      "current in epoch    9      batch 3\n",
      "RLoss: 66.4569320678711\n",
      "current in epoch    9      batch 4\n",
      "RLoss: 2133.859130859375\n",
      "current in epoch    9      batch 5\n",
      "RLoss: 13.10791301727295\n",
      "========================================\n",
      "Epoch 10/1000 - partial_train_loss: 632.4098 \n",
      "Epoch: [10/1000], TrainLoss: 8.30840413911002\n",
      "training Loss has not improved for 5 epochs.\n",
      "current in epoch    10      batch 0\n",
      "RLoss: 19.478485107421875\n",
      "current in epoch    10      batch 1\n",
      "RLoss: 493.09228515625\n",
      "current in epoch    10      batch 2\n",
      "RLoss: 905.044189453125\n",
      "current in epoch    10      batch 3\n",
      "RLoss: 273.5836486816406\n",
      "current in epoch    10      batch 4\n",
      "RLoss: 14.243410110473633\n",
      "current in epoch    10      batch 5\n",
      "RLoss: 55.37458419799805\n",
      "========================================\n",
      "Epoch 11/1000 - partial_train_loss: 219.0722 \n",
      "sorting training set\n",
      "Epoch 11/1000 - Training loss: 40.5754 \n",
      "========================================\n",
      "Epoch: [11/1000], TrainLoss: 43.130863606231905\n",
      "training Loss has not improved for 6 epochs.\n",
      "current in epoch    11      batch 0\n",
      "RLoss: 1101.918212890625\n",
      "current in epoch    11      batch 1\n",
      "RLoss: 463.49615478515625\n",
      "current in epoch    11      batch 2\n",
      "RLoss: 57.024314880371094\n",
      "current in epoch    11      batch 3\n",
      "RLoss: 417.94415283203125\n",
      "current in epoch    11      batch 4\n",
      "RLoss: 472.0753479003906\n",
      "current in epoch    11      batch 5\n",
      "RLoss: 507.2534484863281\n",
      "========================================\n",
      "Epoch 12/1000 - partial_train_loss: 344.9521 \n",
      "Epoch: [12/1000], TrainLoss: 364.9051094055176\n",
      "training Loss has not improved for 7 epochs.\n",
      "current in epoch    12      batch 0\n",
      "RLoss: 4280.5205078125\n",
      "current in epoch    12      batch 1\n",
      "RLoss: 2801.1767578125\n",
      "current in epoch    12      batch 2\n",
      "RLoss: 921.7452392578125\n",
      "current in epoch    12      batch 3\n",
      "RLoss: 3373.184326171875\n",
      "current in epoch    12      batch 4\n",
      "RLoss: 577.06689453125\n",
      "current in epoch    12      batch 5\n",
      "RLoss: 205.9541015625\n",
      "========================================\n",
      "Epoch 13/1000 - partial_train_loss: 1706.8730 \n",
      "Epoch: [13/1000], TrainLoss: 148.43532371520996\n",
      "training Loss has not improved for 8 epochs.\n",
      "current in epoch    13      batch 0\n",
      "RLoss: 1194.164794921875\n",
      "current in epoch    13      batch 1\n",
      "RLoss: 121.5853500366211\n",
      "current in epoch    13      batch 2\n",
      "RLoss: 1504.004638671875\n",
      "current in epoch    13      batch 3\n",
      "RLoss: 441.4624328613281\n",
      "current in epoch    13      batch 4\n",
      "RLoss: 259.9405212402344\n",
      "current in epoch    13      batch 5\n",
      "RLoss: 321.3043212890625\n",
      "========================================\n",
      "Epoch 14/1000 - partial_train_loss: 509.8477 \n",
      "Epoch: [14/1000], TrainLoss: 216.56765937805176\n",
      "training Loss has not improved for 9 epochs.\n",
      "current in epoch    14      batch 0\n",
      "RLoss: 13.36685848236084\n",
      "current in epoch    14      batch 1\n",
      "RLoss: 2585.861328125\n",
      "current in epoch    14      batch 2\n",
      "RLoss: 901.8748168945312\n",
      "current in epoch    14      batch 3\n",
      "RLoss: 399.4068298339844\n",
      "current in epoch    14      batch 4\n",
      "RLoss: 1723.56201171875\n",
      "current in epoch    14      batch 5\n",
      "RLoss: 2554.842529296875\n",
      "========================================\n",
      "Epoch 15/1000 - partial_train_loss: 839.4952 \n",
      "Epoch: [15/1000], TrainLoss: 1809.095424107143\n",
      "training Loss has not improved for 10 epochs.\n",
      "current in epoch    15      batch 0\n",
      "RLoss: 4410.95849609375\n",
      "current in epoch    15      batch 1\n",
      "RLoss: 5931.54052734375\n",
      "current in epoch    15      batch 2\n",
      "RLoss: 6420.240234375\n",
      "current in epoch    15      batch 3\n",
      "RLoss: 604.8820190429688\n",
      "current in epoch    15      batch 4\n",
      "RLoss: 297.4763488769531\n",
      "current in epoch    15      batch 5\n",
      "RLoss: 1954.782470703125\n",
      "========================================\n",
      "Epoch 16/1000 - partial_train_loss: 3076.8172 \n",
      "sorting training set\n",
      "Epoch 16/1000 - Training loss: 1391.8884 \n",
      "========================================\n",
      "Epoch: [16/1000], TrainLoss: 1472.2908644949462\n",
      "training Loss has not improved for 11 epochs.\n",
      "current in epoch    16      batch 0\n",
      "RLoss: 5728.89892578125\n",
      "current in epoch    16      batch 1\n",
      "RLoss: 253.44761657714844\n",
      "current in epoch    16      batch 2\n",
      "RLoss: 4059.933837890625\n",
      "current in epoch    16      batch 3\n",
      "RLoss: 474.0997619628906\n",
      "current in epoch    16      batch 4\n",
      "RLoss: 4076.706787109375\n",
      "current in epoch    16      batch 5\n",
      "RLoss: 2268.490966796875\n",
      "========================================\n",
      "Epoch 17/1000 - partial_train_loss: 2512.8820 \n",
      "Epoch: [17/1000], TrainLoss: 1389.2938319614955\n",
      "training Loss has not improved for 12 epochs.\n",
      "current in epoch    17      batch 0\n",
      "RLoss: 5021.36767578125\n",
      "current in epoch    17      batch 1\n",
      "RLoss: 6445.728515625\n",
      "current in epoch    17      batch 2\n",
      "RLoss: 20.927043914794922\n",
      "current in epoch    17      batch 3\n",
      "RLoss: 113.03134155273438\n",
      "current in epoch    17      batch 4\n",
      "RLoss: 929.3113403320312\n",
      "current in epoch    17      batch 5\n",
      "RLoss: 203.54379272460938\n",
      "========================================\n",
      "Epoch 18/1000 - partial_train_loss: 2195.1804 \n",
      "Epoch: [18/1000], TrainLoss: 124.17591163090297\n",
      "training Loss has not improved for 13 epochs.\n",
      "current in epoch    18      batch 0\n",
      "RLoss: 3721.03955078125\n",
      "current in epoch    18      batch 1\n",
      "RLoss: 3163.292724609375\n",
      "current in epoch    18      batch 2\n",
      "RLoss: 88.32777404785156\n",
      "current in epoch    18      batch 3\n",
      "RLoss: 2062.617431640625\n",
      "current in epoch    18      batch 4\n",
      "RLoss: 1765.524169921875\n",
      "current in epoch    18      batch 5\n",
      "RLoss: 355.96612548828125\n",
      "========================================\n",
      "Epoch 19/1000 - partial_train_loss: 1476.1060 \n",
      "Epoch: [19/1000], TrainLoss: 269.65931756155834\n",
      "training Loss has not improved for 14 epochs.\n",
      "current in epoch    19      batch 0\n",
      "RLoss: 1458.935546875\n",
      "current in epoch    19      batch 1\n",
      "RLoss: 2520.7734375\n",
      "current in epoch    19      batch 2\n",
      "RLoss: 10977.5517578125\n",
      "current in epoch    19      batch 3\n",
      "RLoss: 6464.5810546875\n",
      "current in epoch    19      batch 4\n",
      "RLoss: 3449.07763671875\n",
      "current in epoch    19      batch 5\n",
      "RLoss: 857.5408325195312\n",
      "========================================\n",
      "Epoch 20/1000 - partial_train_loss: 3363.9539 \n",
      "Epoch: [20/1000], TrainLoss: 652.3202678135464\n",
      "training Loss has not improved for 15 epochs.\n",
      "current in epoch    20      batch 0\n",
      "RLoss: 4291.95556640625\n",
      "current in epoch    20      batch 1\n",
      "RLoss: 1010.2164306640625\n",
      "current in epoch    20      batch 2\n",
      "RLoss: 4457.01416015625\n",
      "current in epoch    20      batch 3\n",
      "RLoss: 2157.17822265625\n",
      "current in epoch    20      batch 4\n",
      "RLoss: 2446.77490234375\n",
      "current in epoch    20      batch 5\n",
      "RLoss: 1387.5198974609375\n",
      "========================================\n",
      "Epoch 21/1000 - partial_train_loss: 2140.0992 \n",
      "sorting training set\n",
      "Epoch 21/1000 - Training loss: 1106.9000 \n",
      "========================================\n",
      "Epoch: [21/1000], TrainLoss: 1165.161641377285\n",
      "training Loss has not improved for 16 epochs.\n",
      "current in epoch    21      batch 0\n",
      "RLoss: 2138.8515625\n",
      "current in epoch    21      batch 1\n",
      "RLoss: 895.296875\n",
      "current in epoch    21      batch 2\n",
      "RLoss: 615.5377197265625\n",
      "current in epoch    21      batch 3\n",
      "RLoss: 106.43096923828125\n",
      "current in epoch    21      batch 4\n",
      "RLoss: 910.8905639648438\n",
      "current in epoch    21      batch 5\n",
      "RLoss: 403.6839599609375\n",
      "========================================\n",
      "Epoch 22/1000 - partial_train_loss: 1045.8219 \n",
      "Epoch: [22/1000], TrainLoss: 292.89502552577426\n",
      "training Loss has not improved for 17 epochs.\n",
      "current in epoch    22      batch 0\n",
      "RLoss: 68.8862075805664\n",
      "current in epoch    22      batch 1\n",
      "RLoss: 540.2616577148438\n",
      "current in epoch    22      batch 2\n",
      "RLoss: 2.980652332305908\n",
      "current in epoch    22      batch 3\n",
      "RLoss: 318.08953857421875\n",
      "current in epoch    22      batch 4\n",
      "RLoss: 471.719482421875\n",
      "current in epoch    22      batch 5\n",
      "RLoss: 1205.4366455078125\n",
      "========================================\n",
      "Epoch 23/1000 - partial_train_loss: 292.3026 \n",
      "Epoch: [23/1000], TrainLoss: 942.53176007952\n",
      "training Loss has not improved for 18 epochs.\n",
      "current in epoch    23      batch 0\n",
      "RLoss: 7598.09521484375\n",
      "current in epoch    23      batch 1\n",
      "RLoss: 969.6279907226562\n",
      "current in epoch    23      batch 2\n",
      "RLoss: 1231.6593017578125\n",
      "current in epoch    23      batch 3\n",
      "RLoss: 266.95794677734375\n",
      "current in epoch    23      batch 4\n",
      "RLoss: 750.4862060546875\n",
      "current in epoch    23      batch 5\n",
      "RLoss: 420.95745849609375\n",
      "========================================\n",
      "Epoch 24/1000 - partial_train_loss: 1724.3931 \n",
      "Epoch: [24/1000], TrainLoss: 315.9513348170689\n",
      "training Loss has not improved for 19 epochs.\n",
      "current in epoch    24      batch 0\n",
      "RLoss: 910.3220825195312\n",
      "current in epoch    24      batch 1\n",
      "RLoss: 641.87939453125\n",
      "current in epoch    24      batch 2\n",
      "RLoss: 237.88356018066406\n",
      "current in epoch    24      batch 3\n",
      "RLoss: 974.6153564453125\n",
      "current in epoch    24      batch 4\n",
      "RLoss: 1.4197181463241577\n",
      "current in epoch    24      batch 5\n",
      "RLoss: 1371.9891357421875\n",
      "========================================\n",
      "Epoch 25/1000 - partial_train_loss: 456.6686 \n",
      "Epoch: [25/1000], TrainLoss: 1089.3787798200335\n",
      "training Loss has not improved for 20 epochs.\n",
      "current in epoch    25      batch 0\n",
      "RLoss: 4669.4287109375\n",
      "current in epoch    25      batch 1\n",
      "RLoss: 538.7982788085938\n",
      "current in epoch    25      batch 2\n",
      "RLoss: 185.1000518798828\n",
      "current in epoch    25      batch 3\n",
      "RLoss: 1989.829345703125\n",
      "current in epoch    25      batch 4\n",
      "RLoss: 2477.7138671875\n",
      "current in epoch    25      batch 5\n",
      "RLoss: 3806.4853515625\n",
      "========================================\n",
      "Epoch 26/1000 - partial_train_loss: 1709.9791 \n",
      "sorting training set\n",
      "Epoch 26/1000 - Training loss: 3116.7122 \n",
      "========================================\n",
      "Epoch: [26/1000], TrainLoss: 3280.8206151380546\n",
      "training Loss has not improved for 21 epochs.\n",
      "current in epoch    26      batch 0\n",
      "RLoss: 2694.087646484375\n",
      "current in epoch    26      batch 1\n",
      "RLoss: 2991.050537109375\n",
      "current in epoch    26      batch 2\n",
      "RLoss: 191.4615478515625\n",
      "current in epoch    26      batch 3\n",
      "RLoss: 126.9291000366211\n",
      "current in epoch    26      batch 4\n",
      "RLoss: 1407.3580322265625\n",
      "current in epoch    26      batch 5\n",
      "RLoss: 1349.991455078125\n",
      "========================================\n",
      "Epoch 27/1000 - partial_train_loss: 2113.7638 \n",
      "Epoch: [27/1000], TrainLoss: 982.5472074236188\n",
      "training Loss has not improved for 22 epochs.\n",
      "current in epoch    27      batch 0\n",
      "RLoss: 541.1868286132812\n",
      "current in epoch    27      batch 1\n",
      "RLoss: 3531.9169921875\n",
      "current in epoch    27      batch 2\n",
      "RLoss: 137.28140258789062\n",
      "current in epoch    27      batch 3\n",
      "RLoss: 814.1466674804688\n",
      "current in epoch    27      batch 4\n",
      "RLoss: 13.441460609436035\n",
      "current in epoch    27      batch 5\n",
      "RLoss: 72.08934020996094\n",
      "========================================\n",
      "Epoch 28/1000 - partial_train_loss: 1036.1639 \n",
      "Epoch: [28/1000], TrainLoss: 83.37897743497577\n",
      "training Loss has not improved for 23 epochs.\n",
      "current in epoch    28      batch 0\n",
      "RLoss: 92.00646209716797\n",
      "current in epoch    28      batch 1\n",
      "RLoss: 3373.30810546875\n",
      "current in epoch    28      batch 2\n",
      "RLoss: 561.3742065429688\n",
      "current in epoch    28      batch 3\n",
      "RLoss: 120.1600341796875\n",
      "current in epoch    28      batch 4\n",
      "RLoss: 250.39466857910156\n",
      "current in epoch    28      batch 5\n",
      "RLoss: 932.9008178710938\n",
      "========================================\n",
      "Epoch 29/1000 - partial_train_loss: 529.4581 \n",
      "Epoch: [29/1000], TrainLoss: 751.0050866263254\n",
      "training Loss has not improved for 24 epochs.\n",
      "current in epoch    29      batch 0\n",
      "RLoss: 204.52513122558594\n",
      "current in epoch    29      batch 1\n",
      "RLoss: 1113.3089599609375\n",
      "current in epoch    29      batch 2\n",
      "RLoss: 1.078546404838562\n",
      "current in epoch    29      batch 3\n",
      "RLoss: 1381.5799560546875\n",
      "current in epoch    29      batch 4\n",
      "RLoss: 54.5349235534668\n",
      "current in epoch    29      batch 5\n",
      "RLoss: 1277.2908935546875\n",
      "========================================\n",
      "Epoch 30/1000 - partial_train_loss: 638.1205 \n",
      "Epoch: [30/1000], TrainLoss: 1174.7694484165736\n",
      "training Loss has not improved for 25 epochs.\n",
      "current in epoch    30      batch 0\n",
      "RLoss: 324.8753967285156\n",
      "current in epoch    30      batch 1\n",
      "RLoss: 744.7818603515625\n",
      "current in epoch    30      batch 2\n",
      "RLoss: 1834.423828125\n",
      "current in epoch    30      batch 3\n",
      "RLoss: 644.6197509765625\n",
      "current in epoch    30      batch 4\n",
      "RLoss: 2005.3192138671875\n",
      "current in epoch    30      batch 5\n",
      "RLoss: 1099.986328125\n",
      "========================================\n",
      "Epoch 31/1000 - partial_train_loss: 1053.3946 \n",
      "sorting training set\n",
      "Epoch 31/1000 - Training loss: 754.2784 \n",
      "========================================\n",
      "Epoch: [31/1000], TrainLoss: 799.25573546942\n",
      "training Loss has not improved for 26 epochs.\n",
      "current in epoch    31      batch 0\n",
      "RLoss: 3565.642578125\n",
      "current in epoch    31      batch 1\n",
      "RLoss: 2424.2412109375\n",
      "current in epoch    31      batch 2\n",
      "RLoss: 1491.02392578125\n",
      "current in epoch    31      batch 3\n",
      "RLoss: 3890.41845703125\n",
      "current in epoch    31      batch 4\n",
      "RLoss: 47.25646209716797\n",
      "current in epoch    31      batch 5\n",
      "RLoss: 5.077549457550049\n",
      "========================================\n",
      "Epoch 32/1000 - partial_train_loss: 1860.5440 \n",
      "Epoch: [32/1000], TrainLoss: 3.2032091447285245\n",
      "training Loss has not improved for 27 epochs.\n",
      "current in epoch    32      batch 0\n",
      "RLoss: 339.7196960449219\n",
      "current in epoch    32      batch 1\n",
      "RLoss: 2521.21240234375\n",
      "current in epoch    32      batch 2\n",
      "RLoss: 155.81874084472656\n",
      "current in epoch    32      batch 3\n",
      "RLoss: 84.46247100830078\n",
      "current in epoch    32      batch 4\n",
      "RLoss: 571.64794921875\n",
      "current in epoch    32      batch 5\n",
      "RLoss: 89.13987731933594\n",
      "========================================\n",
      "Epoch 33/1000 - partial_train_loss: 552.2979 \n",
      "Epoch: [33/1000], TrainLoss: 54.744924443108694\n",
      "training Loss has not improved for 28 epochs.\n",
      "current in epoch    33      batch 0\n",
      "RLoss: 250.24655151367188\n",
      "current in epoch    33      batch 1\n",
      "RLoss: 17.690385818481445\n",
      "current in epoch    33      batch 2\n",
      "RLoss: 2622.5048828125\n",
      "current in epoch    33      batch 3\n",
      "RLoss: 36.913612365722656\n",
      "current in epoch    33      batch 4\n",
      "RLoss: 1521.39453125\n",
      "current in epoch    33      batch 5\n",
      "RLoss: 228.5088348388672\n",
      "========================================\n",
      "Epoch 34/1000 - partial_train_loss: 619.0205 \n",
      "Epoch: [34/1000], TrainLoss: 204.98946271623885\n",
      "training Loss has not improved for 29 epochs.\n",
      "current in epoch    34      batch 0\n",
      "RLoss: 1.0705480575561523\n",
      "current in epoch    34      batch 1\n",
      "RLoss: 1163.875244140625\n",
      "current in epoch    34      batch 2\n",
      "RLoss: 933.5452880859375\n",
      "current in epoch    34      batch 3\n",
      "RLoss: 147.19229125976562\n",
      "current in epoch    34      batch 4\n",
      "RLoss: 5.225458145141602\n",
      "current in epoch    34      batch 5\n",
      "RLoss: 68.38648223876953\n",
      "========================================\n",
      "Epoch 35/1000 - partial_train_loss: 393.7654 \n",
      "Epoch: [35/1000], TrainLoss: 68.99863651820591\n",
      "training Loss has not improved for 30 epochs.\n",
      "current in epoch    35      batch 0\n",
      "RLoss: 500.5545654296875\n",
      "current in epoch    35      batch 1\n",
      "RLoss: 405.3131408691406\n",
      "current in epoch    35      batch 2\n",
      "RLoss: 84.85272216796875\n",
      "current in epoch    35      batch 3\n",
      "RLoss: 2601.772705078125\n",
      "current in epoch    35      batch 4\n",
      "RLoss: 5471.64501953125\n",
      "current in epoch    35      batch 5\n",
      "RLoss: 273.7126770019531\n",
      "========================================\n",
      "Epoch 36/1000 - partial_train_loss: 1291.1357 \n",
      "sorting training set\n",
      "Epoch 36/1000 - Training loss: 210.4893 \n",
      "========================================\n",
      "Epoch: [36/1000], TrainLoss: 222.09295030353087\n",
      "training Loss has not improved for 31 epochs.\n",
      "current in epoch    36      batch 0\n",
      "RLoss: 390.7591552734375\n",
      "current in epoch    36      batch 1\n",
      "RLoss: 33.44424819946289\n",
      "current in epoch    36      batch 2\n",
      "RLoss: 457.3509826660156\n",
      "current in epoch    36      batch 3\n",
      "RLoss: 1331.3372802734375\n",
      "current in epoch    36      batch 4\n",
      "RLoss: 539.5764770507812\n",
      "current in epoch    36      batch 5\n",
      "RLoss: 21.700021743774414\n",
      "========================================\n",
      "Epoch 37/1000 - partial_train_loss: 537.5618 \n",
      "Epoch: [37/1000], TrainLoss: 15.293680770056588\n",
      "training Loss has not improved for 32 epochs.\n",
      "current in epoch    37      batch 0\n",
      "RLoss: 1205.4361572265625\n",
      "current in epoch    37      batch 1\n",
      "RLoss: 2355.790283203125\n",
      "current in epoch    37      batch 2\n",
      "RLoss: 177.12799072265625\n",
      "current in epoch    37      batch 3\n",
      "RLoss: 2405.8935546875\n",
      "current in epoch    37      batch 4\n",
      "RLoss: 61.25981903076172\n",
      "current in epoch    37      batch 5\n",
      "RLoss: 1264.1435546875\n",
      "========================================\n",
      "Epoch 38/1000 - partial_train_loss: 860.8110 \n",
      "Epoch: [38/1000], TrainLoss: 894.6388179234096\n",
      "training Loss has not improved for 33 epochs.\n",
      "current in epoch    38      batch 0\n",
      "RLoss: 231.94161987304688\n",
      "current in epoch    38      batch 1\n",
      "RLoss: 37.38508605957031\n",
      "current in epoch    38      batch 2\n",
      "RLoss: 1329.7744140625\n",
      "current in epoch    38      batch 3\n",
      "RLoss: 454.47052001953125\n",
      "current in epoch    38      batch 4\n",
      "RLoss: 5.420793056488037\n",
      "current in epoch    38      batch 5\n",
      "RLoss: 876.9185180664062\n",
      "========================================\n",
      "Epoch 39/1000 - partial_train_loss: 663.6168 \n",
      "Epoch: [39/1000], TrainLoss: 645.6159286499023\n",
      "training Loss has not improved for 34 epochs.\n",
      "current in epoch    39      batch 0\n",
      "RLoss: 872.5641479492188\n",
      "current in epoch    39      batch 1\n",
      "RLoss: 1228.1302490234375\n",
      "current in epoch    39      batch 2\n",
      "RLoss: 539.99609375\n",
      "current in epoch    39      batch 3\n",
      "RLoss: 732.2282104492188\n",
      "current in epoch    39      batch 4\n",
      "RLoss: 412.9562072753906\n",
      "current in epoch    39      batch 5\n",
      "RLoss: 372.2585144042969\n",
      "========================================\n",
      "Epoch 40/1000 - partial_train_loss: 738.3394 \n",
      "Epoch: [40/1000], TrainLoss: 291.88666098458424\n",
      "training Loss has not improved for 35 epochs.\n",
      "current in epoch    40      batch 0\n",
      "RLoss: 1001.7395629882812\n",
      "current in epoch    40      batch 1\n",
      "RLoss: 10.043010711669922\n",
      "current in epoch    40      batch 2\n",
      "RLoss: 66.23224639892578\n",
      "current in epoch    40      batch 3\n",
      "RLoss: 693.580810546875\n",
      "current in epoch    40      batch 4\n",
      "RLoss: 395.1972351074219\n",
      "current in epoch    40      batch 5\n",
      "RLoss: 814.11328125\n",
      "========================================\n",
      "Epoch 41/1000 - partial_train_loss: 393.9394 \n",
      "sorting training set\n",
      "Epoch 41/1000 - Training loss: 654.1864 \n",
      "========================================\n",
      "Epoch: [41/1000], TrainLoss: 686.7789993893441\n",
      "training Loss has not improved for 36 epochs.\n",
      "current in epoch    41      batch 0\n",
      "RLoss: 1288.580322265625\n",
      "current in epoch    41      batch 1\n",
      "RLoss: 1532.9737548828125\n",
      "current in epoch    41      batch 2\n",
      "RLoss: 224.93341064453125\n",
      "current in epoch    41      batch 3\n",
      "RLoss: 1027.943603515625\n",
      "current in epoch    41      batch 4\n",
      "RLoss: 404.8788757324219\n",
      "current in epoch    41      batch 5\n",
      "RLoss: 5303.83251953125\n",
      "========================================\n",
      "Epoch 42/1000 - partial_train_loss: 828.1688 \n",
      "Epoch: [42/1000], TrainLoss: 4007.107491629464\n",
      "training Loss has not improved for 37 epochs.\n",
      "current in epoch    42      batch 0\n",
      "RLoss: 1781.006591796875\n",
      "current in epoch    42      batch 1\n",
      "RLoss: 2463.84375\n",
      "current in epoch    42      batch 2\n",
      "RLoss: 399.3833312988281\n",
      "current in epoch    42      batch 3\n",
      "RLoss: 1135.270751953125\n",
      "current in epoch    42      batch 4\n",
      "RLoss: 399.1215515136719\n",
      "current in epoch    42      batch 5\n",
      "RLoss: 372.0289001464844\n",
      "========================================\n",
      "Epoch 43/1000 - partial_train_loss: 2562.2844 \n",
      "Epoch: [43/1000], TrainLoss: 290.7979943411691\n",
      "training Loss has not improved for 38 epochs.\n",
      "current in epoch    43      batch 0\n",
      "RLoss: 1095.6475830078125\n",
      "current in epoch    43      batch 1\n",
      "RLoss: 2592.850341796875\n",
      "current in epoch    43      batch 2\n",
      "RLoss: 2876.173828125\n",
      "current in epoch    43      batch 3\n",
      "RLoss: 360.3648986816406\n",
      "current in epoch    43      batch 4\n",
      "RLoss: 2470.52099609375\n",
      "current in epoch    43      batch 5\n",
      "RLoss: 3557.469482421875\n",
      "========================================\n",
      "Epoch 44/1000 - partial_train_loss: 1376.8296 \n",
      "Epoch: [44/1000], TrainLoss: 2683.5866350446427\n",
      "training Loss has not improved for 39 epochs.\n",
      "current in epoch    44      batch 0\n",
      "RLoss: 8962.8251953125\n",
      "current in epoch    44      batch 1\n",
      "RLoss: 27.344900131225586\n",
      "current in epoch    44      batch 2\n",
      "RLoss: 1155.6202392578125\n",
      "current in epoch    44      batch 3\n",
      "RLoss: 1468.4239501953125\n",
      "current in epoch    44      batch 4\n",
      "RLoss: 708.40625\n",
      "current in epoch    44      batch 5\n",
      "RLoss: 5480.63623046875\n",
      "========================================\n",
      "Epoch 45/1000 - partial_train_loss: 2749.5013 \n",
      "Epoch: [45/1000], TrainLoss: 4165.5606689453125\n",
      "training Loss has not improved for 40 epochs.\n",
      "current in epoch    45      batch 0\n",
      "RLoss: 4086.022705078125\n",
      "current in epoch    45      batch 1\n",
      "RLoss: 85.74337005615234\n",
      "current in epoch    45      batch 2\n",
      "RLoss: 342.54754638671875\n",
      "current in epoch    45      batch 3\n",
      "RLoss: 42.93234634399414\n",
      "current in epoch    45      batch 4\n",
      "RLoss: 701.7904052734375\n",
      "current in epoch    45      batch 5\n",
      "RLoss: 254.83370971679688\n",
      "========================================\n",
      "Epoch 46/1000 - partial_train_loss: 2320.5674 \n",
      "sorting training set\n",
      "Epoch 46/1000 - Training loss: 184.5832 \n",
      "========================================\n",
      "Epoch: [46/1000], TrainLoss: 197.0400755447513\n",
      "training Loss has not improved for 41 epochs.\n",
      "current in epoch    46      batch 0\n",
      "RLoss: 5.779491424560547\n",
      "current in epoch    46      batch 1\n",
      "RLoss: 145.85874938964844\n",
      "current in epoch    46      batch 2\n",
      "RLoss: 211.40895080566406\n",
      "current in epoch    46      batch 3\n",
      "RLoss: 371.1916809082031\n",
      "current in epoch    46      batch 4\n",
      "RLoss: 571.099609375\n",
      "current in epoch    46      batch 5\n",
      "RLoss: 858.094970703125\n",
      "========================================\n",
      "Epoch 47/1000 - partial_train_loss: 288.6511 \n",
      "Epoch: [47/1000], TrainLoss: 634.0826761381967\n",
      "training Loss has not improved for 42 epochs.\n",
      "current in epoch    47      batch 0\n",
      "RLoss: 236.07823181152344\n",
      "current in epoch    47      batch 1\n",
      "RLoss: 10.65886402130127\n",
      "current in epoch    47      batch 2\n",
      "RLoss: 283.10302734375\n",
      "current in epoch    47      batch 3\n",
      "RLoss: 877.3320922851562\n",
      "current in epoch    47      batch 4\n",
      "RLoss: 363.77728271484375\n",
      "current in epoch    47      batch 5\n",
      "RLoss: 1159.65087890625\n",
      "========================================\n",
      "Epoch 48/1000 - partial_train_loss: 514.4268 \n",
      "Epoch: [48/1000], TrainLoss: 892.6600298200335\n",
      "training Loss has not improved for 43 epochs.\n",
      "current in epoch    48      batch 0\n",
      "RLoss: 799.7869873046875\n",
      "current in epoch    48      batch 1\n",
      "RLoss: 45.53660202026367\n",
      "current in epoch    48      batch 2\n",
      "RLoss: 91.74073791503906\n",
      "current in epoch    48      batch 3\n",
      "RLoss: 408.8211364746094\n",
      "current in epoch    48      batch 4\n",
      "RLoss: 19.204450607299805\n",
      "current in epoch    48      batch 5\n",
      "RLoss: 7.121367931365967\n",
      "========================================\n",
      "Epoch 49/1000 - partial_train_loss: 491.5863 \n",
      "Epoch: [49/1000], TrainLoss: 8.824918951307025\n",
      "training Loss has not improved for 44 epochs.\n",
      "current in epoch    49      batch 0\n",
      "RLoss: 495.1693420410156\n",
      "current in epoch    49      batch 1\n",
      "RLoss: 510.6751403808594\n",
      "current in epoch    49      batch 2\n",
      "RLoss: 1620.70458984375\n",
      "current in epoch    49      batch 3\n",
      "RLoss: 6745.39453125\n",
      "current in epoch    49      batch 4\n",
      "RLoss: 198.08567810058594\n",
      "current in epoch    49      batch 5\n",
      "RLoss: 512.0717163085938\n",
      "========================================\n",
      "Epoch 50/1000 - partial_train_loss: 1340.1287 \n",
      "Epoch: [50/1000], TrainLoss: 359.82080078125\n",
      "training Loss has not improved for 45 epochs.\n",
      "current in epoch    50      batch 0\n",
      "RLoss: 144.53387451171875\n",
      "current in epoch    50      batch 1\n",
      "RLoss: 1140.53125\n",
      "current in epoch    50      batch 2\n",
      "RLoss: 342.2563781738281\n",
      "current in epoch    50      batch 3\n",
      "RLoss: 889.6773071289062\n",
      "current in epoch    50      batch 4\n",
      "RLoss: 354.6766662597656\n",
      "current in epoch    50      batch 5\n",
      "RLoss: 63.330448150634766\n",
      "========================================\n",
      "Epoch 51/1000 - partial_train_loss: 543.6431 \n",
      "sorting training set\n",
      "Epoch 51/1000 - Training loss: 37.4097 \n",
      "========================================\n",
      "Epoch: [51/1000], TrainLoss: 40.4376387835934\n",
      "training Loss has not improved for 46 epochs.\n",
      "current in epoch    51      batch 0\n",
      "RLoss: 2635.870849609375\n",
      "current in epoch    51      batch 1\n",
      "RLoss: 57.6192741394043\n",
      "current in epoch    51      batch 2\n",
      "RLoss: 2937.25634765625\n",
      "current in epoch    51      batch 3\n",
      "RLoss: 170.85508728027344\n",
      "current in epoch    51      batch 4\n",
      "RLoss: 317.2901306152344\n",
      "current in epoch    51      batch 5\n",
      "RLoss: 104.4595947265625\n",
      "========================================\n",
      "Epoch 52/1000 - partial_train_loss: 1051.6482 \n",
      "Epoch: [52/1000], TrainLoss: 119.84599113464355\n",
      "training Loss has not improved for 47 epochs.\n",
      "current in epoch    52      batch 0\n",
      "RLoss: 3142.238037109375\n",
      "current in epoch    52      batch 1\n",
      "RLoss: 24.247447967529297\n",
      "current in epoch    52      batch 2\n",
      "RLoss: 14.337284088134766\n",
      "current in epoch    52      batch 3\n",
      "RLoss: 436.2889404296875\n",
      "current in epoch    52      batch 4\n",
      "RLoss: 371.0077209472656\n",
      "current in epoch    52      batch 5\n",
      "RLoss: 455.8218078613281\n",
      "========================================\n",
      "Epoch 53/1000 - partial_train_loss: 603.1401 \n",
      "Epoch: [53/1000], TrainLoss: 426.6188310895647\n",
      "training Loss has not improved for 48 epochs.\n",
      "current in epoch    53      batch 0\n",
      "RLoss: 747.97119140625\n",
      "current in epoch    53      batch 1\n",
      "RLoss: 20.036367416381836\n",
      "current in epoch    53      batch 2\n",
      "RLoss: 275.3821105957031\n",
      "current in epoch    53      batch 3\n",
      "RLoss: 241.4714813232422\n",
      "current in epoch    53      batch 4\n",
      "RLoss: 416.4749755859375\n",
      "current in epoch    53      batch 5\n",
      "RLoss: 490.5352478027344\n",
      "========================================\n",
      "Epoch 54/1000 - partial_train_loss: 363.2308 \n",
      "Epoch: [54/1000], TrainLoss: 459.3158612932478\n",
      "training Loss has not improved for 49 epochs.\n",
      "current in epoch    54      batch 0\n",
      "RLoss: 305.2283935546875\n",
      "current in epoch    54      batch 1\n",
      "RLoss: 17.085071563720703\n",
      "current in epoch    54      batch 2\n",
      "RLoss: 749.3828125\n",
      "current in epoch    54      batch 3\n",
      "RLoss: 100.22540283203125\n",
      "current in epoch    54      batch 4\n",
      "RLoss: 129.72645568847656\n",
      "current in epoch    54      batch 5\n",
      "RLoss: 245.5203857421875\n",
      "========================================\n",
      "Epoch 55/1000 - partial_train_loss: 363.1764 \n",
      "Epoch: [55/1000], TrainLoss: 166.84459291185652\n",
      "training Loss has not improved for 50 epochs.\n",
      "current in epoch    55      batch 0\n",
      "RLoss: 112.37773132324219\n",
      "current in epoch    55      batch 1\n",
      "RLoss: 698.6722412109375\n",
      "current in epoch    55      batch 2\n",
      "RLoss: 1882.3284912109375\n",
      "current in epoch    55      batch 3\n",
      "RLoss: 350.80810546875\n",
      "current in epoch    55      batch 4\n",
      "RLoss: 108.89306640625\n",
      "current in epoch    55      batch 5\n",
      "RLoss: 2819.87109375\n",
      "========================================\n",
      "Epoch 56/1000 - partial_train_loss: 560.0777 \n",
      "sorting training set\n",
      "Epoch 56/1000 - Training loss: 2275.3315 \n",
      "========================================\n",
      "Epoch: [56/1000], TrainLoss: 2405.5898783436246\n",
      "training Loss has not improved for 51 epochs.\n",
      "current in epoch    56      batch 0\n",
      "RLoss: 3180.724609375\n",
      "current in epoch    56      batch 1\n",
      "RLoss: 748.7529907226562\n",
      "current in epoch    56      batch 2\n",
      "RLoss: 2143.782470703125\n",
      "current in epoch    56      batch 3\n",
      "RLoss: 658.2686157226562\n",
      "current in epoch    56      batch 4\n",
      "RLoss: 21.833251953125\n",
      "current in epoch    56      batch 5\n",
      "RLoss: 102.27867126464844\n",
      "========================================\n",
      "Epoch 57/1000 - partial_train_loss: 1874.5707 \n",
      "Epoch: [57/1000], TrainLoss: 130.31594848632812\n",
      "training Loss has not improved for 52 epochs.\n",
      "current in epoch    57      batch 0\n",
      "RLoss: 163.34100341796875\n",
      "current in epoch    57      batch 1\n",
      "RLoss: 400.4246520996094\n",
      "current in epoch    57      batch 2\n",
      "RLoss: 1031.692626953125\n",
      "current in epoch    57      batch 3\n",
      "RLoss: 216.9023895263672\n",
      "current in epoch    57      batch 4\n",
      "RLoss: 499.2218017578125\n",
      "current in epoch    57      batch 5\n",
      "RLoss: 520.0908203125\n",
      "========================================\n",
      "Epoch 58/1000 - partial_train_loss: 332.2182 \n",
      "Epoch: [58/1000], TrainLoss: 438.3091272626604\n",
      "training Loss has not improved for 53 epochs.\n",
      "current in epoch    58      batch 0\n",
      "RLoss: 552.7916870117188\n",
      "current in epoch    58      batch 1\n",
      "RLoss: 2200.08056640625\n",
      "current in epoch    58      batch 2\n",
      "RLoss: 1145.3056640625\n",
      "current in epoch    58      batch 3\n",
      "RLoss: 410.6981201171875\n",
      "current in epoch    58      batch 4\n",
      "RLoss: 8.338666915893555\n",
      "current in epoch    58      batch 5\n",
      "RLoss: 266.824951171875\n",
      "========================================\n",
      "Epoch 59/1000 - partial_train_loss: 792.1293 \n",
      "Epoch: [59/1000], TrainLoss: 209.73166710989815\n",
      "training Loss has not improved for 54 epochs.\n",
      "current in epoch    59      batch 0\n",
      "RLoss: 308.4398193359375\n",
      "current in epoch    59      batch 1\n",
      "RLoss: 463.25604248046875\n",
      "current in epoch    59      batch 2\n",
      "RLoss: 2563.86181640625\n",
      "current in epoch    59      batch 3\n",
      "RLoss: 323.290283203125\n",
      "current in epoch    59      batch 4\n",
      "RLoss: 186.70257568359375\n",
      "current in epoch    59      batch 5\n",
      "RLoss: 550.3946533203125\n",
      "========================================\n",
      "Epoch 60/1000 - partial_train_loss: 561.3154 \n",
      "Epoch: [60/1000], TrainLoss: 266.0399757112776\n",
      "training Loss has not improved for 55 epochs.\n",
      "current in epoch    60      batch 0\n",
      "RLoss: 396.4476318359375\n",
      "current in epoch    60      batch 1\n",
      "RLoss: 530.8843383789062\n",
      "current in epoch    60      batch 2\n",
      "RLoss: 367.4508972167969\n",
      "current in epoch    60      batch 3\n",
      "RLoss: 3059.285888671875\n",
      "current in epoch    60      batch 4\n",
      "RLoss: 143.2898406982422\n",
      "current in epoch    60      batch 5\n",
      "RLoss: 118.33847045898438\n",
      "========================================\n",
      "Epoch 61/1000 - partial_train_loss: 786.2536 \n",
      "sorting training set\n",
      "Epoch 61/1000 - Training loss: 48.4266 \n",
      "========================================\n",
      "Epoch: [61/1000], TrainLoss: 50.993833505891\n",
      "training Loss has not improved for 56 epochs.\n",
      "current in epoch    61      batch 0\n",
      "RLoss: 853.2792358398438\n",
      "current in epoch    61      batch 1\n",
      "RLoss: 251.2036895751953\n",
      "current in epoch    61      batch 2\n",
      "RLoss: 254.23193359375\n",
      "current in epoch    61      batch 3\n",
      "RLoss: 4328.70947265625\n",
      "current in epoch    61      batch 4\n",
      "RLoss: 1476.3385009765625\n",
      "current in epoch    61      batch 5\n",
      "RLoss: 2774.72412109375\n",
      "========================================\n",
      "Epoch 62/1000 - partial_train_loss: 1043.0741 \n",
      "Epoch: [62/1000], TrainLoss: 2289.2855050223216\n",
      "training Loss has not improved for 57 epochs.\n",
      "current in epoch    62      batch 0\n",
      "RLoss: 3027.11083984375\n",
      "current in epoch    62      batch 1\n",
      "RLoss: 2623.6845703125\n",
      "current in epoch    62      batch 2\n",
      "RLoss: 467.51556396484375\n",
      "current in epoch    62      batch 3\n",
      "RLoss: 1486.1080322265625\n",
      "current in epoch    62      batch 4\n",
      "RLoss: 159.2816925048828\n",
      "current in epoch    62      batch 5\n",
      "RLoss: 36.02656555175781\n",
      "========================================\n",
      "Epoch 63/1000 - partial_train_loss: 1815.0542 \n",
      "Epoch: [63/1000], TrainLoss: 27.830495629991805\n",
      "training Loss has not improved for 58 epochs.\n",
      "current in epoch    63      batch 0\n",
      "RLoss: 1362.3607177734375\n",
      "current in epoch    63      batch 1\n",
      "RLoss: 28.80534553527832\n",
      "current in epoch    63      batch 2\n",
      "RLoss: 589.5402221679688\n",
      "current in epoch    63      batch 3\n",
      "RLoss: 808.82421875\n",
      "current in epoch    63      batch 4\n",
      "RLoss: 83.95804595947266\n",
      "current in epoch    63      batch 5\n",
      "RLoss: 48.865562438964844\n",
      "========================================\n",
      "Epoch 64/1000 - partial_train_loss: 424.4505 \n",
      "Epoch: [64/1000], TrainLoss: 21.697125507252558\n",
      "training Loss has not improved for 59 epochs.\n",
      "current in epoch    64      batch 0\n",
      "RLoss: 5.9114861488342285\n",
      "current in epoch    64      batch 1\n",
      "RLoss: 23.50963592529297\n",
      "current in epoch    64      batch 2\n",
      "RLoss: 1128.9462890625\n",
      "current in epoch    64      batch 3\n",
      "RLoss: 2596.166748046875\n",
      "current in epoch    64      batch 4\n",
      "RLoss: 141.45462036132812\n",
      "current in epoch    64      batch 5\n",
      "RLoss: 90.22048950195312\n",
      "========================================\n",
      "Epoch 65/1000 - partial_train_loss: 581.3218 \n",
      "Epoch: [65/1000], TrainLoss: 98.9201717376709\n",
      "training Loss has not improved for 60 epochs.\n",
      "current in epoch    65      batch 0\n",
      "RLoss: 611.5465087890625\n",
      "current in epoch    65      batch 1\n",
      "RLoss: 1280.9884033203125\n",
      "current in epoch    65      batch 2\n",
      "RLoss: 304.65521240234375\n",
      "current in epoch    65      batch 3\n",
      "RLoss: 205.92942810058594\n",
      "current in epoch    65      batch 4\n",
      "RLoss: 1.4577317237854004\n",
      "current in epoch    65      batch 5\n",
      "RLoss: 28.800708770751953\n",
      "========================================\n",
      "Epoch 66/1000 - partial_train_loss: 349.6515 \n",
      "sorting training set\n",
      "Epoch 66/1000 - Training loss: 84.4152 \n",
      "========================================\n",
      "Epoch: [66/1000], TrainLoss: 91.07649945504329\n",
      "training Loss has not improved for 61 epochs.\n",
      "current in epoch    66      batch 0\n",
      "RLoss: 82.13397216796875\n",
      "current in epoch    66      batch 1\n",
      "RLoss: 81.78816986083984\n",
      "current in epoch    66      batch 2\n",
      "RLoss: 974.9306030273438\n",
      "current in epoch    66      batch 3\n",
      "RLoss: 440.0709228515625\n",
      "current in epoch    66      batch 4\n",
      "RLoss: 270.7706298828125\n",
      "current in epoch    66      batch 5\n",
      "RLoss: 62.47505569458008\n",
      "========================================\n",
      "Epoch 67/1000 - partial_train_loss: 322.9367 \n",
      "Epoch: [67/1000], TrainLoss: 69.68018613542829\n",
      "training Loss has not improved for 62 epochs.\n",
      "current in epoch    67      batch 0\n",
      "RLoss: 2004.7633056640625\n",
      "current in epoch    67      batch 1\n",
      "RLoss: 98.89854431152344\n",
      "current in epoch    67      batch 2\n",
      "RLoss: 847.3028564453125\n",
      "current in epoch    67      batch 3\n",
      "RLoss: 152.1713104248047\n",
      "current in epoch    67      batch 4\n",
      "RLoss: 2411.787841796875\n",
      "current in epoch    67      batch 5\n",
      "RLoss: 1022.0277099609375\n",
      "========================================\n",
      "Epoch 68/1000 - partial_train_loss: 846.2580 \n",
      "Epoch: [68/1000], TrainLoss: 964.5550319126675\n",
      "training Loss has not improved for 63 epochs.\n",
      "current in epoch    68      batch 0\n",
      "RLoss: 3129.69775390625\n",
      "current in epoch    68      batch 1\n",
      "RLoss: 1248.4132080078125\n",
      "current in epoch    68      batch 2\n",
      "RLoss: 28.32103729248047\n",
      "current in epoch    68      batch 3\n",
      "RLoss: 174.02691650390625\n",
      "current in epoch    68      batch 4\n",
      "RLoss: 482.6272888183594\n",
      "current in epoch    68      batch 5\n",
      "RLoss: 53.95790100097656\n",
      "========================================\n",
      "Epoch 69/1000 - partial_train_loss: 909.4282 \n",
      "Epoch: [69/1000], TrainLoss: 58.79033647264753\n",
      "training Loss has not improved for 64 epochs.\n",
      "current in epoch    69      batch 0\n",
      "RLoss: 259.7541198730469\n",
      "current in epoch    69      batch 1\n",
      "RLoss: 341.78106689453125\n",
      "current in epoch    69      batch 2\n",
      "RLoss: 131.0228271484375\n",
      "current in epoch    69      batch 3\n",
      "RLoss: 2011.9302978515625\n",
      "current in epoch    69      batch 4\n",
      "RLoss: 489.2206115722656\n",
      "current in epoch    69      batch 5\n",
      "RLoss: 857.4089965820312\n",
      "========================================\n",
      "Epoch 70/1000 - partial_train_loss: 460.8809 \n",
      "Epoch: [70/1000], TrainLoss: 857.3813934326172\n",
      "training Loss has not improved for 65 epochs.\n",
      "current in epoch    70      batch 0\n",
      "RLoss: 9.29653263092041\n",
      "current in epoch    70      batch 1\n",
      "RLoss: 804.992919921875\n",
      "current in epoch    70      batch 2\n",
      "RLoss: 11.29505443572998\n",
      "current in epoch    70      batch 3\n",
      "RLoss: 833.980712890625\n",
      "current in epoch    70      batch 4\n",
      "RLoss: 19.150985717773438\n",
      "current in epoch    70      batch 5\n",
      "RLoss: 115.82940673828125\n",
      "========================================\n",
      "Epoch 71/1000 - partial_train_loss: 382.2412 \n",
      "sorting training set\n",
      "Epoch 71/1000 - Training loss: 108.2620 \n",
      "========================================\n",
      "Epoch: [71/1000], TrainLoss: 115.22491574692779\n",
      "training Loss has not improved for 66 epochs.\n",
      "current in epoch    71      batch 0\n",
      "RLoss: 6.583112716674805\n",
      "current in epoch    71      batch 1\n",
      "RLoss: 224.15553283691406\n",
      "current in epoch    71      batch 2\n",
      "RLoss: 2394.318359375\n",
      "current in epoch    71      batch 3\n",
      "RLoss: 847.7186889648438\n",
      "current in epoch    71      batch 4\n",
      "RLoss: 119.35689544677734\n",
      "current in epoch    71      batch 5\n",
      "RLoss: 654.9080200195312\n",
      "========================================\n",
      "Epoch 72/1000 - partial_train_loss: 581.1773 \n",
      "Epoch: [72/1000], TrainLoss: 461.22494833809986\n",
      "training Loss has not improved for 67 epochs.\n",
      "current in epoch    72      batch 0\n",
      "RLoss: 1952.845947265625\n",
      "current in epoch    72      batch 1\n",
      "RLoss: 3604.843017578125\n",
      "current in epoch    72      batch 2\n",
      "RLoss: 105.30134582519531\n",
      "current in epoch    72      batch 3\n",
      "RLoss: 36.152034759521484\n",
      "current in epoch    72      batch 4\n",
      "RLoss: 130.40586853027344\n",
      "current in epoch    72      batch 5\n",
      "RLoss: 2364.82568359375\n",
      "========================================\n",
      "Epoch 73/1000 - partial_train_loss: 935.3732 \n",
      "Epoch: [73/1000], TrainLoss: 2078.612021309989\n",
      "training Loss has not improved for 68 epochs.\n",
      "current in epoch    73      batch 0\n",
      "RLoss: 2100.16357421875\n",
      "current in epoch    73      batch 1\n",
      "RLoss: 419.6878662109375\n",
      "current in epoch    73      batch 2\n",
      "RLoss: 3014.5712890625\n",
      "current in epoch    73      batch 3\n",
      "RLoss: 91.32295227050781\n",
      "current in epoch    73      batch 4\n",
      "RLoss: 260.18341064453125\n",
      "current in epoch    73      batch 5\n",
      "RLoss: 1133.1904296875\n",
      "========================================\n",
      "Epoch 74/1000 - partial_train_loss: 1441.9117 \n",
      "Epoch: [74/1000], TrainLoss: 684.0073449271066\n",
      "training Loss has not improved for 69 epochs.\n",
      "current in epoch    74      batch 0\n",
      "RLoss: 594.94189453125\n",
      "current in epoch    74      batch 1\n",
      "RLoss: 2090.104736328125\n",
      "current in epoch    74      batch 2\n",
      "RLoss: 282.07110595703125\n",
      "current in epoch    74      batch 3\n",
      "RLoss: 30.41655731201172\n",
      "current in epoch    74      batch 4\n",
      "RLoss: 16.707365036010742\n",
      "current in epoch    74      batch 5\n",
      "RLoss: 1129.5733642578125\n",
      "========================================\n",
      "Epoch 75/1000 - partial_train_loss: 799.5426 \n",
      "Epoch: [75/1000], TrainLoss: 1162.6968667166573\n",
      "training Loss has not improved for 70 epochs.\n",
      "current in epoch    75      batch 0\n",
      "RLoss: 49.23402786254883\n",
      "current in epoch    75      batch 1\n",
      "RLoss: 85.91604614257812\n",
      "current in epoch    75      batch 2\n",
      "RLoss: 316.03521728515625\n",
      "current in epoch    75      batch 3\n",
      "RLoss: 1369.9583740234375\n",
      "current in epoch    75      batch 4\n",
      "RLoss: 86.85366821289062\n",
      "current in epoch    75      batch 5\n",
      "RLoss: 214.64328002929688\n",
      "========================================\n",
      "Epoch 76/1000 - partial_train_loss: 524.2799 \n",
      "sorting training set\n",
      "Epoch 76/1000 - Training loss: 76.9940 \n",
      "========================================\n",
      "Epoch: [76/1000], TrainLoss: 81.89725928753371\n",
      "training Loss has not improved for 71 epochs.\n",
      "current in epoch    76      batch 0\n",
      "RLoss: 34.23548126220703\n",
      "current in epoch    76      batch 1\n",
      "RLoss: 833.5880126953125\n",
      "current in epoch    76      batch 2\n",
      "RLoss: 15.152002334594727\n",
      "current in epoch    76      batch 3\n",
      "RLoss: 2688.242919921875\n",
      "current in epoch    76      batch 4\n",
      "RLoss: 231.1241455078125\n",
      "current in epoch    76      batch 5\n",
      "RLoss: 72.67007446289062\n",
      "========================================\n",
      "Epoch 77/1000 - partial_train_loss: 691.3156 \n",
      "Epoch: [77/1000], TrainLoss: 38.791297503880095\n",
      "training Loss has not improved for 72 epochs.\n",
      "current in epoch    77      batch 0\n",
      "RLoss: 410.01251220703125\n",
      "current in epoch    77      batch 1\n",
      "RLoss: 1438.9163818359375\n",
      "current in epoch    77      batch 2\n",
      "RLoss: 995.02978515625\n",
      "current in epoch    77      batch 3\n",
      "RLoss: 581.7442016601562\n",
      "current in epoch    77      batch 4\n",
      "RLoss: 2443.512939453125\n",
      "current in epoch    77      batch 5\n",
      "RLoss: 66.91676330566406\n",
      "========================================\n",
      "Epoch 78/1000 - partial_train_loss: 968.3409 \n",
      "Epoch: [78/1000], TrainLoss: 43.11566202981131\n",
      "training Loss has not improved for 73 epochs.\n",
      "current in epoch    78      batch 0\n",
      "RLoss: 508.2866516113281\n",
      "current in epoch    78      batch 1\n",
      "RLoss: 400.5756530761719\n",
      "current in epoch    78      batch 2\n",
      "RLoss: 39.94668197631836\n",
      "current in epoch    78      batch 3\n",
      "RLoss: 846.2058715820312\n",
      "current in epoch    78      batch 4\n",
      "RLoss: 816.642333984375\n",
      "current in epoch    78      batch 5\n",
      "RLoss: 61.122108459472656\n",
      "========================================\n",
      "Epoch 79/1000 - partial_train_loss: 523.3445 \n",
      "Epoch: [79/1000], TrainLoss: 94.8433540889195\n",
      "training Loss has not improved for 74 epochs.\n",
      "current in epoch    79      batch 0\n",
      "RLoss: 317.5265808105469\n",
      "current in epoch    79      batch 1\n",
      "RLoss: 1673.671142578125\n",
      "current in epoch    79      batch 2\n",
      "RLoss: 701.8892211914062\n",
      "current in epoch    79      batch 3\n",
      "RLoss: 105.24485778808594\n",
      "current in epoch    79      batch 4\n",
      "RLoss: 18.463083267211914\n",
      "current in epoch    79      batch 5\n",
      "RLoss: 688.3004150390625\n",
      "========================================\n",
      "Epoch 80/1000 - partial_train_loss: 523.4471 \n",
      "Epoch: [80/1000], TrainLoss: 539.7011827741351\n",
      "training Loss has not improved for 75 epochs.\n",
      "current in epoch    80      batch 0\n",
      "RLoss: 24.122678756713867\n",
      "current in epoch    80      batch 1\n",
      "RLoss: 1770.23828125\n",
      "current in epoch    80      batch 2\n",
      "RLoss: 196.00575256347656\n",
      "current in epoch    80      batch 3\n",
      "RLoss: 49.29703140258789\n",
      "current in epoch    80      batch 4\n",
      "RLoss: 77.014404296875\n",
      "current in epoch    80      batch 5\n",
      "RLoss: 384.5279235839844\n",
      "========================================\n",
      "Epoch 81/1000 - partial_train_loss: 470.0818 \n",
      "sorting training set\n",
      "Epoch 81/1000 - Training loss: 572.0134 \n",
      "========================================\n",
      "Epoch: [81/1000], TrainLoss: 606.5978366257469\n",
      "training Loss has not improved for 76 epochs.\n",
      "current in epoch    81      batch 0\n",
      "RLoss: 940.7472534179688\n",
      "current in epoch    81      batch 1\n",
      "RLoss: 658.6535034179688\n",
      "current in epoch    81      batch 2\n",
      "RLoss: 825.6477661132812\n",
      "current in epoch    81      batch 3\n",
      "RLoss: 851.0352172851562\n",
      "current in epoch    81      batch 4\n",
      "RLoss: 20.385700225830078\n",
      "current in epoch    81      batch 5\n",
      "RLoss: 119.0960464477539\n",
      "========================================\n",
      "Epoch 82/1000 - partial_train_loss: 787.8885 \n",
      "Epoch: [82/1000], TrainLoss: 211.7515949521746\n",
      "training Loss has not improved for 77 epochs.\n",
      "current in epoch    82      batch 0\n",
      "RLoss: 21.118061065673828\n",
      "current in epoch    82      batch 1\n",
      "RLoss: 4.544836521148682\n",
      "current in epoch    82      batch 2\n",
      "RLoss: 733.8717041015625\n",
      "current in epoch    82      batch 3\n",
      "RLoss: 8.849276542663574\n",
      "current in epoch    82      batch 4\n",
      "RLoss: 29.90913200378418\n",
      "current in epoch    82      batch 5\n",
      "RLoss: 155.82916259765625\n",
      "========================================\n",
      "Epoch 83/1000 - partial_train_loss: 111.9273 \n",
      "Epoch: [83/1000], TrainLoss: 116.34790107182094\n",
      "training Loss has not improved for 78 epochs.\n",
      "current in epoch    83      batch 0\n",
      "RLoss: 224.16885375976562\n",
      "current in epoch    83      batch 1\n",
      "RLoss: 4109.86328125\n",
      "current in epoch    83      batch 2\n",
      "RLoss: 5895.40625\n",
      "current in epoch    83      batch 3\n",
      "RLoss: 1715.48486328125\n",
      "current in epoch    83      batch 4\n",
      "RLoss: 413.122802734375\n",
      "current in epoch    83      batch 5\n",
      "RLoss: 1697.0399169921875\n",
      "========================================\n",
      "Epoch 84/1000 - partial_train_loss: 1816.3006 \n",
      "Epoch: [84/1000], TrainLoss: 1463.8419363839287\n",
      "training Loss has not improved for 79 epochs.\n",
      "current in epoch    84      batch 0\n",
      "RLoss: 696.9049072265625\n",
      "current in epoch    84      batch 1\n",
      "RLoss: 10.536056518554688\n",
      "current in epoch    84      batch 2\n",
      "RLoss: 798.3787841796875\n",
      "current in epoch    84      batch 3\n",
      "RLoss: 111.14677429199219\n",
      "current in epoch    84      batch 4\n",
      "RLoss: 72.03463745117188\n",
      "current in epoch    84      batch 5\n",
      "RLoss: 65.79862213134766\n",
      "========================================\n",
      "Epoch 85/1000 - partial_train_loss: 603.0178 \n",
      "Epoch: [85/1000], TrainLoss: 48.245345728737966\n",
      "training Loss has not improved for 80 epochs.\n",
      "current in epoch    85      batch 0\n",
      "RLoss: 1011.635009765625\n",
      "current in epoch    85      batch 1\n",
      "RLoss: 22.58761215209961\n",
      "current in epoch    85      batch 2\n",
      "RLoss: 146.25572204589844\n",
      "current in epoch    85      batch 3\n",
      "RLoss: 106.44544219970703\n",
      "current in epoch    85      batch 4\n",
      "RLoss: 38.233131408691406\n",
      "current in epoch    85      batch 5\n",
      "RLoss: 104.87852478027344\n",
      "========================================\n",
      "Epoch 86/1000 - partial_train_loss: 187.8518 \n",
      "sorting training set\n",
      "Epoch 86/1000 - Training loss: 77.1401 \n",
      "========================================\n",
      "Epoch: [86/1000], TrainLoss: 83.07348230587392\n",
      "training Loss has not improved for 81 epochs.\n",
      "current in epoch    86      batch 0\n",
      "RLoss: 758.33251953125\n",
      "current in epoch    86      batch 1\n",
      "RLoss: 75.38754272460938\n",
      "current in epoch    86      batch 2\n",
      "RLoss: 313.3689270019531\n",
      "current in epoch    86      batch 3\n",
      "RLoss: 407.40313720703125\n",
      "current in epoch    86      batch 4\n",
      "RLoss: 80.3968505859375\n",
      "current in epoch    86      batch 5\n",
      "RLoss: 312.8778381347656\n",
      "========================================\n",
      "Epoch 87/1000 - partial_train_loss: 276.6724 \n",
      "Epoch: [87/1000], TrainLoss: 237.17821339198522\n",
      "training Loss has not improved for 82 epochs.\n",
      "current in epoch    87      batch 0\n",
      "RLoss: 1244.9451904296875\n",
      "current in epoch    87      batch 1\n",
      "RLoss: 161.29986572265625\n",
      "current in epoch    87      batch 2\n",
      "RLoss: 19.250202178955078\n",
      "current in epoch    87      batch 3\n",
      "RLoss: 116.38922119140625\n",
      "current in epoch    87      batch 4\n",
      "RLoss: 805.4080810546875\n",
      "current in epoch    87      batch 5\n",
      "RLoss: 92.69898223876953\n",
      "========================================\n",
      "Epoch 88/1000 - partial_train_loss: 448.2339 \n",
      "Epoch: [88/1000], TrainLoss: 80.56533336639404\n",
      "training Loss has not improved for 83 epochs.\n",
      "current in epoch    88      batch 0\n",
      "RLoss: 298.2733154296875\n",
      "current in epoch    88      batch 1\n",
      "RLoss: 1925.2894287109375\n",
      "current in epoch    88      batch 2\n",
      "RLoss: 114.50698852539062\n",
      "current in epoch    88      batch 3\n",
      "RLoss: 130.4471893310547\n",
      "current in epoch    88      batch 4\n",
      "RLoss: 203.38401794433594\n",
      "current in epoch    88      batch 5\n",
      "RLoss: 442.5624084472656\n",
      "========================================\n",
      "Epoch 89/1000 - partial_train_loss: 391.3143 \n",
      "Epoch: [89/1000], TrainLoss: 561.4488318307059\n",
      "training Loss has not improved for 84 epochs.\n",
      "current in epoch    89      batch 0\n",
      "RLoss: 1125.5303955078125\n",
      "current in epoch    89      batch 1\n",
      "RLoss: 195.66558837890625\n",
      "current in epoch    89      batch 2\n",
      "RLoss: 621.6536254882812\n",
      "current in epoch    89      batch 3\n",
      "RLoss: 955.3448486328125\n",
      "current in epoch    89      batch 4\n",
      "RLoss: 111.8706283569336\n",
      "current in epoch    89      batch 5\n",
      "RLoss: 10.266544342041016\n",
      "========================================\n",
      "Epoch 90/1000 - partial_train_loss: 551.1338 \n",
      "Epoch: [90/1000], TrainLoss: 8.270205020904541\n",
      "training Loss has not improved for 85 epochs.\n",
      "current in epoch    90      batch 0\n",
      "RLoss: 3247.84912109375\n",
      "current in epoch    90      batch 1\n",
      "RLoss: 403.7671813964844\n",
      "current in epoch    90      batch 2\n",
      "RLoss: 74.56688690185547\n",
      "current in epoch    90      batch 3\n",
      "RLoss: 21.53938865661621\n",
      "current in epoch    90      batch 4\n",
      "RLoss: 533.2191162109375\n",
      "current in epoch    90      batch 5\n",
      "RLoss: 200.5476531982422\n",
      "========================================\n",
      "Epoch 91/1000 - partial_train_loss: 584.2861 \n",
      "sorting training set\n",
      "Epoch 91/1000 - Training loss: 302.4704 \n",
      "========================================\n",
      "Epoch: [91/1000], TrainLoss: 322.75274248997226\n",
      "training Loss has not improved for 86 epochs.\n",
      "current in epoch    91      batch 0\n",
      "RLoss: 57.3099479675293\n",
      "current in epoch    91      batch 1\n",
      "RLoss: 27.945350646972656\n",
      "current in epoch    91      batch 2\n",
      "RLoss: 61.03459930419922\n",
      "current in epoch    91      batch 3\n",
      "RLoss: 500.3941650390625\n",
      "current in epoch    91      batch 4\n",
      "RLoss: 924.2763671875\n",
      "current in epoch    91      batch 5\n",
      "RLoss: 56.285972595214844\n",
      "========================================\n",
      "Epoch 92/1000 - partial_train_loss: 482.3826 \n",
      "Epoch: [92/1000], TrainLoss: 29.154266459601267\n",
      "training Loss has not improved for 87 epochs.\n",
      "current in epoch    92      batch 0\n",
      "RLoss: 422.65802001953125\n",
      "current in epoch    92      batch 1\n",
      "RLoss: 1637.0980224609375\n",
      "current in epoch    92      batch 2\n",
      "RLoss: 4.637013912200928\n",
      "current in epoch    92      batch 3\n",
      "RLoss: 5.014848232269287\n",
      "current in epoch    92      batch 4\n",
      "RLoss: 195.2611541748047\n",
      "current in epoch    92      batch 5\n",
      "RLoss: 240.93191528320312\n",
      "========================================\n",
      "Epoch 93/1000 - partial_train_loss: 303.5497 \n",
      "Epoch: [93/1000], TrainLoss: 132.66154520852226\n",
      "training Loss has not improved for 88 epochs.\n",
      "current in epoch    93      batch 0\n",
      "RLoss: 482.7109375\n",
      "current in epoch    93      batch 1\n",
      "RLoss: 570.6068115234375\n",
      "current in epoch    93      batch 2\n",
      "RLoss: 97.97151947021484\n",
      "current in epoch    93      batch 3\n",
      "RLoss: 45.667633056640625\n",
      "current in epoch    93      batch 4\n",
      "RLoss: 21.276426315307617\n",
      "current in epoch    93      batch 5\n",
      "RLoss: 325.7784729003906\n",
      "========================================\n",
      "Epoch 94/1000 - partial_train_loss: 215.2707 \n",
      "Epoch: [94/1000], TrainLoss: 149.43766934531075\n",
      "training Loss has not improved for 89 epochs.\n",
      "current in epoch    94      batch 0\n",
      "RLoss: 57.3413200378418\n",
      "current in epoch    94      batch 1\n",
      "RLoss: 1967.6043701171875\n",
      "current in epoch    94      batch 2\n",
      "RLoss: 37.16670227050781\n",
      "current in epoch    94      batch 3\n",
      "RLoss: 96.5910415649414\n",
      "current in epoch    94      batch 4\n",
      "RLoss: 2175.7294921875\n",
      "current in epoch    94      batch 5\n",
      "RLoss: 62.19635772705078\n",
      "========================================\n",
      "Epoch 95/1000 - partial_train_loss: 657.5233 \n",
      "Epoch: [95/1000], TrainLoss: 63.33544887815203\n",
      "training Loss has not improved for 90 epochs.\n",
      "current in epoch    95      batch 0\n",
      "RLoss: 9.205548286437988\n",
      "current in epoch    95      batch 1\n",
      "RLoss: 57.03506851196289\n",
      "current in epoch    95      batch 2\n",
      "RLoss: 39.726409912109375\n",
      "current in epoch    95      batch 3\n",
      "RLoss: 108.10454559326172\n",
      "current in epoch    95      batch 4\n",
      "RLoss: 113.84465026855469\n",
      "current in epoch    95      batch 5\n",
      "RLoss: 38.09790802001953\n",
      "========================================\n",
      "Epoch 96/1000 - partial_train_loss: 54.1427 \n",
      "sorting training set\n",
      "Epoch 96/1000 - Training loss: 33.8539 \n",
      "========================================\n",
      "Epoch: [96/1000], TrainLoss: 35.842055648865376\n",
      "training Loss has not improved for 91 epochs.\n",
      "current in epoch    96      batch 0\n",
      "RLoss: 1348.715576171875\n",
      "current in epoch    96      batch 1\n",
      "RLoss: 48.38047790527344\n",
      "current in epoch    96      batch 2\n",
      "RLoss: 446.9770812988281\n",
      "current in epoch    96      batch 3\n",
      "RLoss: 926.8126831054688\n",
      "current in epoch    96      batch 4\n",
      "RLoss: 4144.10302734375\n",
      "current in epoch    96      batch 5\n",
      "RLoss: 468.73248291015625\n",
      "========================================\n",
      "Epoch 97/1000 - partial_train_loss: 1177.7214 \n",
      "Epoch: [97/1000], TrainLoss: 321.2075729370117\n",
      "training Loss has not improved for 92 epochs.\n",
      "current in epoch    97      batch 0\n",
      "RLoss: 106.27415466308594\n",
      "current in epoch    97      batch 1\n",
      "RLoss: 143.92384338378906\n",
      "current in epoch    97      batch 2\n",
      "RLoss: 430.43804931640625\n",
      "current in epoch    97      batch 3\n",
      "RLoss: 80.69073486328125\n",
      "current in epoch    97      batch 4\n",
      "RLoss: 31.30887794494629\n",
      "current in epoch    97      batch 5\n",
      "RLoss: 613.3740844726562\n",
      "========================================\n",
      "Epoch 98/1000 - partial_train_loss: 120.1648 \n",
      "Epoch: [98/1000], TrainLoss: 563.5148980276925\n",
      "training Loss has not improved for 93 epochs.\n",
      "current in epoch    98      batch 0\n",
      "RLoss: 267.344970703125\n",
      "current in epoch    98      batch 1\n",
      "RLoss: 50.68180847167969\n",
      "current in epoch    98      batch 2\n",
      "RLoss: 2744.968017578125\n",
      "current in epoch    98      batch 3\n",
      "RLoss: 45.85782241821289\n",
      "current in epoch    98      batch 4\n",
      "RLoss: 165.43838500976562\n",
      "current in epoch    98      batch 5\n",
      "RLoss: 499.41943359375\n",
      "========================================\n",
      "Epoch 99/1000 - partial_train_loss: 613.8156 \n",
      "Epoch: [99/1000], TrainLoss: 361.7343008858817\n",
      "training Loss has not improved for 94 epochs.\n",
      "current in epoch    99      batch 0\n",
      "RLoss: 3.4815726280212402\n",
      "current in epoch    99      batch 1\n",
      "RLoss: 404.1523132324219\n",
      "current in epoch    99      batch 2\n",
      "RLoss: 148.30001831054688\n",
      "current in epoch    99      batch 3\n",
      "RLoss: 58.201255798339844\n",
      "current in epoch    99      batch 4\n",
      "RLoss: 918.0333251953125\n",
      "current in epoch    99      batch 5\n",
      "RLoss: 101.03556060791016\n",
      "========================================\n",
      "Epoch 100/1000 - partial_train_loss: 458.6238 \n",
      "Epoch: [100/1000], TrainLoss: 126.15882423945835\n",
      "training Loss has not improved for 95 epochs.\n",
      "current in epoch    100      batch 0\n",
      "RLoss: 339.6229553222656\n",
      "current in epoch    100      batch 1\n",
      "RLoss: 629.8214721679688\n",
      "current in epoch    100      batch 2\n",
      "RLoss: 247.9352264404297\n",
      "current in epoch    100      batch 3\n",
      "RLoss: 463.4366149902344\n",
      "current in epoch    100      batch 4\n",
      "RLoss: 2.811446189880371\n",
      "current in epoch    100      batch 5\n",
      "RLoss: 159.09280395507812\n",
      "========================================\n",
      "Epoch 101/1000 - partial_train_loss: 318.6410 \n",
      "sorting training set\n",
      "Epoch 101/1000 - Training loss: 183.9250 \n",
      "========================================\n",
      "Epoch: [101/1000], TrainLoss: 196.1020162227215\n",
      "training Loss has not improved for 96 epochs.\n",
      "current in epoch    101      batch 0\n",
      "RLoss: 1240.421875\n",
      "current in epoch    101      batch 1\n",
      "RLoss: 1705.2080078125\n",
      "current in epoch    101      batch 2\n",
      "RLoss: 529.678466796875\n",
      "current in epoch    101      batch 3\n",
      "RLoss: 25.877500534057617\n",
      "current in epoch    101      batch 4\n",
      "RLoss: 142.6742706298828\n",
      "current in epoch    101      batch 5\n",
      "RLoss: 13.832858085632324\n",
      "========================================\n",
      "Epoch 102/1000 - partial_train_loss: 861.5448 \n",
      "Epoch: [102/1000], TrainLoss: 16.403357999665396\n",
      "training Loss has not improved for 97 epochs.\n",
      "current in epoch    102      batch 0\n",
      "RLoss: 50.68016052246094\n",
      "current in epoch    102      batch 1\n",
      "RLoss: 168.40379333496094\n",
      "current in epoch    102      batch 2\n",
      "RLoss: 1.4885045289993286\n",
      "current in epoch    102      batch 3\n",
      "RLoss: 14.171843528747559\n",
      "current in epoch    102      batch 4\n",
      "RLoss: 88.85177612304688\n",
      "current in epoch    102      batch 5\n",
      "RLoss: 2987.922607421875\n",
      "========================================\n",
      "Epoch 103/1000 - partial_train_loss: 142.6475 \n",
      "Epoch: [103/1000], TrainLoss: 3483.2258911132812\n",
      "training Loss has not improved for 98 epochs.\n",
      "current in epoch    103      batch 0\n",
      "RLoss: 2637.859375\n",
      "current in epoch    103      batch 1\n",
      "RLoss: 594.9392700195312\n",
      "current in epoch    103      batch 2\n",
      "RLoss: 49.00753402709961\n",
      "current in epoch    103      batch 3\n",
      "RLoss: 736.099365234375\n",
      "current in epoch    103      batch 4\n",
      "RLoss: 35.36656188964844\n",
      "current in epoch    103      batch 5\n",
      "RLoss: 579.01611328125\n",
      "========================================\n",
      "Epoch 104/1000 - partial_train_loss: 903.1930 \n",
      "Epoch: [104/1000], TrainLoss: 474.8645978655134\n",
      "training Loss has not improved for 99 epochs.\n",
      "current in epoch    104      batch 0\n",
      "RLoss: 8.34698486328125\n",
      "current in epoch    104      batch 1\n",
      "RLoss: 130.4449462890625\n",
      "current in epoch    104      batch 2\n",
      "RLoss: 834.05859375\n",
      "current in epoch    104      batch 3\n",
      "RLoss: 1809.62841796875\n",
      "current in epoch    104      batch 4\n",
      "RLoss: 232.3892059326172\n",
      "current in epoch    104      batch 5\n",
      "RLoss: 1208.2353515625\n",
      "========================================\n",
      "Epoch 105/1000 - partial_train_loss: 562.6868 \n",
      "Epoch: [105/1000], TrainLoss: 1008.9110434395926\n",
      "training Loss has not improved for 100 epochs.\n",
      "current in epoch    105      batch 0\n",
      "RLoss: 214.7359161376953\n",
      "current in epoch    105      batch 1\n",
      "RLoss: 7.103193759918213\n",
      "current in epoch    105      batch 2\n",
      "RLoss: 294.989990234375\n",
      "current in epoch    105      batch 3\n",
      "RLoss: 2202.138427734375\n",
      "current in epoch    105      batch 4\n",
      "RLoss: 620.3469848632812\n",
      "current in epoch    105      batch 5\n",
      "RLoss: 4482.26123046875\n",
      "========================================\n",
      "Epoch 106/1000 - partial_train_loss: 749.1399 \n",
      "sorting training set\n",
      "Epoch 106/1000 - Training loss: 4339.1269 \n",
      "========================================\n",
      "Epoch: [106/1000], TrainLoss: 4585.295494769151\n",
      "training Loss has not improved for 101 epochs.\n",
      "current in epoch    106      batch 0\n",
      "RLoss: 212.8629608154297\n",
      "current in epoch    106      batch 1\n",
      "RLoss: 956.59716796875\n",
      "current in epoch    106      batch 2\n",
      "RLoss: 160.6818084716797\n",
      "current in epoch    106      batch 3\n",
      "RLoss: 70.67747497558594\n",
      "current in epoch    106      batch 4\n",
      "RLoss: 258.0184020996094\n",
      "current in epoch    106      batch 5\n",
      "RLoss: 56.368927001953125\n",
      "========================================\n",
      "Epoch 107/1000 - partial_train_loss: 2095.5210 \n",
      "Epoch: [107/1000], TrainLoss: 43.909730434417725\n",
      "training Loss has not improved for 102 epochs.\n",
      "current in epoch    107      batch 0\n",
      "RLoss: 269.0309753417969\n",
      "current in epoch    107      batch 1\n",
      "RLoss: 6285.9423828125\n",
      "current in epoch    107      batch 2\n",
      "RLoss: 613.2703247070312\n",
      "current in epoch    107      batch 3\n",
      "RLoss: 124.52613067626953\n",
      "current in epoch    107      batch 4\n",
      "RLoss: 475.47003173828125\n",
      "current in epoch    107      batch 5\n",
      "RLoss: 612.1773681640625\n",
      "========================================\n",
      "Epoch 108/1000 - partial_train_loss: 1080.6980 \n",
      "Epoch: [108/1000], TrainLoss: 471.2440610613142\n",
      "training Loss has not improved for 103 epochs.\n",
      "current in epoch    108      batch 0\n",
      "RLoss: 2031.6851806640625\n",
      "current in epoch    108      batch 1\n",
      "RLoss: 94.6764144897461\n",
      "current in epoch    108      batch 2\n",
      "RLoss: 40.72590637207031\n",
      "current in epoch    108      batch 3\n",
      "RLoss: 43.95911407470703\n",
      "current in epoch    108      batch 4\n",
      "RLoss: 850.3524780273438\n",
      "current in epoch    108      batch 5\n",
      "RLoss: 73.87296295166016\n",
      "========================================\n",
      "Epoch 109/1000 - partial_train_loss: 666.5476 \n",
      "Epoch: [109/1000], TrainLoss: 89.86039856501988\n",
      "training Loss has not improved for 104 epochs.\n",
      "current in epoch    109      batch 0\n",
      "RLoss: 74.6156997680664\n",
      "current in epoch    109      batch 1\n",
      "RLoss: 1643.06396484375\n",
      "current in epoch    109      batch 2\n",
      "RLoss: 678.7452392578125\n",
      "current in epoch    109      batch 3\n",
      "RLoss: 7834.9765625\n",
      "current in epoch    109      batch 4\n",
      "RLoss: 277.2342529296875\n",
      "current in epoch    109      batch 5\n",
      "RLoss: 183.4391326904297\n",
      "========================================\n",
      "Epoch 110/1000 - partial_train_loss: 1664.7672 \n",
      "Epoch: [110/1000], TrainLoss: 139.4074821472168\n",
      "training Loss has not improved for 105 epochs.\n",
      "current in epoch    110      batch 0\n",
      "RLoss: 622.0614624023438\n",
      "current in epoch    110      batch 1\n",
      "RLoss: 281.47064208984375\n",
      "current in epoch    110      batch 2\n",
      "RLoss: 579.8529052734375\n",
      "current in epoch    110      batch 3\n",
      "RLoss: 82.38858032226562\n",
      "current in epoch    110      batch 4\n",
      "RLoss: 36.83927917480469\n",
      "current in epoch    110      batch 5\n",
      "RLoss: 575.52392578125\n",
      "========================================\n",
      "Epoch 111/1000 - partial_train_loss: 288.6444 \n",
      "sorting training set\n",
      "Epoch 111/1000 - Training loss: 310.4262 \n",
      "========================================\n",
      "Epoch: [111/1000], TrainLoss: 331.5169212687263\n",
      "training Loss has not improved for 106 epochs.\n",
      "current in epoch    111      batch 0\n",
      "RLoss: 1013.8110961914062\n",
      "current in epoch    111      batch 1\n",
      "RLoss: 664.047119140625\n",
      "current in epoch    111      batch 2\n",
      "RLoss: 243.93527221679688\n",
      "current in epoch    111      batch 3\n",
      "RLoss: 335.0218505859375\n",
      "current in epoch    111      batch 4\n",
      "RLoss: 395.2165222167969\n",
      "current in epoch    111      batch 5\n",
      "RLoss: 76.93663787841797\n",
      "========================================\n",
      "Epoch 112/1000 - partial_train_loss: 629.3799 \n",
      "Epoch: [112/1000], TrainLoss: 63.23572778701782\n",
      "training Loss has not improved for 107 epochs.\n",
      "current in epoch    112      batch 0\n",
      "RLoss: 263.69635009765625\n",
      "current in epoch    112      batch 1\n",
      "RLoss: 274.1461181640625\n",
      "current in epoch    112      batch 2\n",
      "RLoss: 2457.244873046875\n",
      "current in epoch    112      batch 3\n",
      "RLoss: 337.1952209472656\n",
      "current in epoch    112      batch 4\n",
      "RLoss: 503.2192687988281\n",
      "current in epoch    112      batch 5\n",
      "RLoss: 585.52294921875\n",
      "========================================\n",
      "Epoch 113/1000 - partial_train_loss: 612.3923 \n",
      "Epoch: [113/1000], TrainLoss: 429.2046350751604\n",
      "training Loss has not improved for 108 epochs.\n",
      "current in epoch    113      batch 0\n",
      "RLoss: 556.864013671875\n",
      "current in epoch    113      batch 1\n",
      "RLoss: 106.72638702392578\n",
      "current in epoch    113      batch 2\n",
      "RLoss: 108.56063842773438\n",
      "current in epoch    113      batch 3\n",
      "RLoss: 115.01171112060547\n",
      "current in epoch    113      batch 4\n",
      "RLoss: 1090.9180908203125\n",
      "current in epoch    113      batch 5\n",
      "RLoss: 32.966732025146484\n",
      "========================================\n",
      "Epoch 114/1000 - partial_train_loss: 397.6915 \n",
      "Epoch: [114/1000], TrainLoss: 29.46820182459695\n",
      "training Loss has not improved for 109 epochs.\n",
      "current in epoch    114      batch 0\n",
      "RLoss: 1288.697998046875\n",
      "current in epoch    114      batch 1\n",
      "RLoss: 420.1427307128906\n",
      "current in epoch    114      batch 2\n",
      "RLoss: 182.34494018554688\n",
      "current in epoch    114      batch 3\n",
      "RLoss: 1006.0853271484375\n",
      "current in epoch    114      batch 4\n",
      "RLoss: 210.70602416992188\n",
      "current in epoch    114      batch 5\n",
      "RLoss: 56.96662139892578\n",
      "========================================\n",
      "Epoch 115/1000 - partial_train_loss: 454.1170 \n",
      "Epoch: [115/1000], TrainLoss: 134.14823423113143\n",
      "training Loss has not improved for 110 epochs.\n",
      "current in epoch    115      batch 0\n",
      "RLoss: 133.2433624267578\n",
      "current in epoch    115      batch 1\n",
      "RLoss: 462.8259582519531\n",
      "current in epoch    115      batch 2\n",
      "RLoss: 121.35599517822266\n",
      "current in epoch    115      batch 3\n",
      "RLoss: 267.120849609375\n",
      "current in epoch    115      batch 4\n",
      "RLoss: 326.084716796875\n",
      "current in epoch    115      batch 5\n",
      "RLoss: 39.86756896972656\n",
      "========================================\n",
      "Epoch 116/1000 - partial_train_loss: 193.9799 \n",
      "sorting training set\n",
      "Epoch 116/1000 - Training loss: 141.2844 \n",
      "========================================\n",
      "Epoch: [116/1000], TrainLoss: 150.39734031892823\n",
      "training Loss has not improved for 111 epochs.\n",
      "current in epoch    116      batch 0\n",
      "RLoss: 263.7335205078125\n",
      "current in epoch    116      batch 1\n",
      "RLoss: 6.9067511558532715\n",
      "current in epoch    116      batch 2\n",
      "RLoss: 1503.7108154296875\n",
      "current in epoch    116      batch 3\n",
      "RLoss: 273.1539306640625\n",
      "current in epoch    116      batch 4\n",
      "RLoss: 374.2651672363281\n",
      "current in epoch    116      batch 5\n",
      "RLoss: 275.2785339355469\n",
      "========================================\n",
      "Epoch 117/1000 - partial_train_loss: 554.5855 \n",
      "Epoch: [117/1000], TrainLoss: 289.4646432059152\n",
      "training Loss has not improved for 112 epochs.\n",
      "current in epoch    117      batch 0\n",
      "RLoss: 406.56878662109375\n",
      "current in epoch    117      batch 1\n",
      "RLoss: 285.8730773925781\n",
      "current in epoch    117      batch 2\n",
      "RLoss: 484.6917724609375\n",
      "current in epoch    117      batch 3\n",
      "RLoss: 102.50161743164062\n",
      "current in epoch    117      batch 4\n",
      "RLoss: 46.18391036987305\n",
      "current in epoch    117      batch 5\n",
      "RLoss: 175.66941833496094\n",
      "========================================\n",
      "Epoch 118/1000 - partial_train_loss: 275.2664 \n",
      "Epoch: [118/1000], TrainLoss: 155.50600242614746\n",
      "training Loss has not improved for 113 epochs.\n",
      "current in epoch    118      batch 0\n",
      "RLoss: 89.22238159179688\n",
      "current in epoch    118      batch 1\n",
      "RLoss: 845.5718383789062\n",
      "current in epoch    118      batch 2\n",
      "RLoss: 8.17786979675293\n",
      "current in epoch    118      batch 3\n",
      "RLoss: 30.46347999572754\n",
      "current in epoch    118      batch 4\n",
      "RLoss: 29.621376037597656\n",
      "current in epoch    118      batch 5\n",
      "RLoss: 46.594078063964844\n",
      "========================================\n",
      "Epoch 119/1000 - partial_train_loss: 203.1578 \n",
      "Epoch: [119/1000], TrainLoss: 28.27492836543492\n",
      "training Loss has not improved for 114 epochs.\n",
      "current in epoch    119      batch 0\n",
      "RLoss: 310.4424743652344\n",
      "current in epoch    119      batch 1\n",
      "RLoss: 11.563081741333008\n",
      "current in epoch    119      batch 2\n",
      "RLoss: 80.98091125488281\n",
      "current in epoch    119      batch 3\n",
      "RLoss: 15.347787857055664\n",
      "current in epoch    119      batch 4\n",
      "RLoss: 96.2776870727539\n",
      "current in epoch    119      batch 5\n",
      "RLoss: 346.8990783691406\n",
      "========================================\n",
      "Epoch 120/1000 - partial_train_loss: 83.1872 \n",
      "Epoch: [120/1000], TrainLoss: 285.47046388898576\n",
      "training Loss has not improved for 115 epochs.\n",
      "current in epoch    120      batch 0\n",
      "RLoss: 32.96489715576172\n",
      "current in epoch    120      batch 1\n",
      "RLoss: 35.56059265136719\n",
      "current in epoch    120      batch 2\n",
      "RLoss: 157.36758422851562\n",
      "current in epoch    120      batch 3\n",
      "RLoss: 149.16050720214844\n",
      "current in epoch    120      batch 4\n",
      "RLoss: 622.185791015625\n",
      "current in epoch    120      batch 5\n",
      "RLoss: 12.780781745910645\n",
      "========================================\n",
      "Epoch 121/1000 - partial_train_loss: 289.2502 \n",
      "sorting training set\n",
      "Epoch 121/1000 - Training loss: 10.1283 \n",
      "========================================\n",
      "Epoch: [121/1000], TrainLoss: 11.07097224712079\n",
      "training Loss has not improved for 116 epochs.\n",
      "current in epoch    121      batch 0\n",
      "RLoss: 2.687443971633911\n",
      "current in epoch    121      batch 1\n",
      "RLoss: 71.71246337890625\n",
      "current in epoch    121      batch 2\n",
      "RLoss: 334.115966796875\n",
      "current in epoch    121      batch 3\n",
      "RLoss: 9.751422882080078\n",
      "current in epoch    121      batch 4\n",
      "RLoss: 136.62820434570312\n",
      "current in epoch    121      batch 5\n",
      "RLoss: 41.40590286254883\n",
      "========================================\n",
      "Epoch 122/1000 - partial_train_loss: 89.3328 \n",
      "Epoch: [122/1000], TrainLoss: 36.35246072496687\n",
      "training Loss has not improved for 117 epochs.\n",
      "current in epoch    122      batch 0\n",
      "RLoss: 19.539430618286133\n",
      "current in epoch    122      batch 1\n",
      "RLoss: 1180.95556640625\n",
      "current in epoch    122      batch 2\n",
      "RLoss: 73.40775299072266\n",
      "current in epoch    122      batch 3\n",
      "RLoss: 92.80415344238281\n",
      "current in epoch    122      batch 4\n",
      "RLoss: 456.39483642578125\n",
      "current in epoch    122      batch 5\n",
      "RLoss: 11.988349914550781\n",
      "========================================\n",
      "Epoch 123/1000 - partial_train_loss: 285.5991 \n",
      "Epoch: [123/1000], TrainLoss: 10.379106283187866\n",
      "training Loss has not improved for 118 epochs.\n",
      "current in epoch    123      batch 0\n",
      "RLoss: 1146.211181640625\n",
      "current in epoch    123      batch 1\n",
      "RLoss: 85.09002685546875\n",
      "current in epoch    123      batch 2\n",
      "RLoss: 158.2602996826172\n",
      "current in epoch    123      batch 3\n",
      "RLoss: 50.215415954589844\n",
      "current in epoch    123      batch 4\n",
      "RLoss: 184.4431610107422\n",
      "current in epoch    123      batch 5\n",
      "RLoss: 42.4708251953125\n",
      "========================================\n",
      "Epoch 124/1000 - partial_train_loss: 227.1497 \n",
      "Epoch: [124/1000], TrainLoss: 39.81730624607631\n",
      "training Loss has not improved for 119 epochs.\n",
      "current in epoch    124      batch 0\n",
      "RLoss: 257.6075439453125\n",
      "current in epoch    124      batch 1\n",
      "RLoss: 92.8886489868164\n",
      "current in epoch    124      batch 2\n",
      "RLoss: 329.3258361816406\n",
      "current in epoch    124      batch 3\n",
      "RLoss: 10.764477729797363\n",
      "current in epoch    124      batch 4\n",
      "RLoss: 432.92840576171875\n",
      "current in epoch    124      batch 5\n",
      "RLoss: 113.03995513916016\n",
      "========================================\n",
      "Epoch 125/1000 - partial_train_loss: 177.9206 \n",
      "Epoch: [125/1000], TrainLoss: 90.31599003928048\n",
      "training Loss has not improved for 120 epochs.\n",
      "current in epoch    125      batch 0\n",
      "RLoss: 2339.3037109375\n",
      "current in epoch    125      batch 1\n",
      "RLoss: 69.38339233398438\n",
      "current in epoch    125      batch 2\n",
      "RLoss: 63.669063568115234\n",
      "current in epoch    125      batch 3\n",
      "RLoss: 1455.3035888671875\n",
      "current in epoch    125      batch 4\n",
      "RLoss: 82.0157241821289\n",
      "current in epoch    125      batch 5\n",
      "RLoss: 316.958740234375\n",
      "========================================\n",
      "Epoch 126/1000 - partial_train_loss: 567.1903 \n",
      "sorting training set\n",
      "Epoch 126/1000 - Training loss: 257.2345 \n",
      "========================================\n",
      "Epoch: [126/1000], TrainLoss: 272.7040985032909\n",
      "training Loss has not improved for 121 epochs.\n",
      "current in epoch    126      batch 0\n",
      "RLoss: 156.71119689941406\n",
      "current in epoch    126      batch 1\n",
      "RLoss: 79.62904357910156\n",
      "current in epoch    126      batch 2\n",
      "RLoss: 451.8221130371094\n",
      "current in epoch    126      batch 3\n",
      "RLoss: 872.8779296875\n",
      "current in epoch    126      batch 4\n",
      "RLoss: 173.3575897216797\n",
      "current in epoch    126      batch 5\n",
      "RLoss: 346.70721435546875\n",
      "========================================\n",
      "Epoch 127/1000 - partial_train_loss: 381.8643 \n",
      "Epoch: [127/1000], TrainLoss: 401.5028315952846\n",
      "training Loss has not improved for 122 epochs.\n",
      "current in epoch    127      batch 0\n",
      "RLoss: 439.7763977050781\n",
      "current in epoch    127      batch 1\n",
      "RLoss: 109.1083984375\n",
      "current in epoch    127      batch 2\n",
      "RLoss: 25.501026153564453\n",
      "current in epoch    127      batch 3\n",
      "RLoss: 448.83203125\n",
      "current in epoch    127      batch 4\n",
      "RLoss: 926.6233520507812\n",
      "current in epoch    127      batch 5\n",
      "RLoss: 212.4156494140625\n",
      "========================================\n",
      "Epoch 128/1000 - partial_train_loss: 397.9758 \n",
      "Epoch: [128/1000], TrainLoss: 234.1511595589774\n",
      "training Loss has not improved for 123 epochs.\n",
      "current in epoch    128      batch 0\n",
      "RLoss: 1850.751953125\n",
      "current in epoch    128      batch 1\n",
      "RLoss: 323.0567932128906\n",
      "current in epoch    128      batch 2\n",
      "RLoss: 3792.697998046875\n",
      "current in epoch    128      batch 3\n",
      "RLoss: 933.1798095703125\n",
      "current in epoch    128      batch 4\n",
      "RLoss: 101.04012298583984\n",
      "current in epoch    128      batch 5\n",
      "RLoss: 53.063621520996094\n",
      "========================================\n",
      "Epoch 129/1000 - partial_train_loss: 1085.6988 \n",
      "Epoch: [129/1000], TrainLoss: 55.59969466073172\n",
      "training Loss has not improved for 124 epochs.\n",
      "current in epoch    129      batch 0\n",
      "RLoss: 514.2200927734375\n",
      "current in epoch    129      batch 1\n",
      "RLoss: 20.681249618530273\n",
      "current in epoch    129      batch 2\n",
      "RLoss: 815.384033203125\n",
      "current in epoch    129      batch 3\n",
      "RLoss: 119.09599304199219\n",
      "current in epoch    129      batch 4\n",
      "RLoss: 107.73541259765625\n",
      "current in epoch    129      batch 5\n",
      "RLoss: 57.281517028808594\n",
      "========================================\n",
      "Epoch 130/1000 - partial_train_loss: 238.0694 \n",
      "Epoch: [130/1000], TrainLoss: 52.08533797945295\n",
      "training Loss has not improved for 125 epochs.\n",
      "current in epoch    130      batch 0\n",
      "RLoss: 194.19583129882812\n",
      "current in epoch    130      batch 1\n",
      "RLoss: 109.64421081542969\n",
      "current in epoch    130      batch 2\n",
      "RLoss: 14.758851051330566\n",
      "current in epoch    130      batch 3\n",
      "RLoss: 27.772785186767578\n",
      "current in epoch    130      batch 4\n",
      "RLoss: 63.07515335083008\n",
      "current in epoch    130      batch 5\n",
      "RLoss: 185.98744201660156\n",
      "========================================\n",
      "Epoch 131/1000 - partial_train_loss: 74.7678 \n",
      "sorting training set\n",
      "Epoch 131/1000 - Training loss: 151.2070 \n",
      "========================================\n",
      "Epoch: [131/1000], TrainLoss: 160.51002445922595\n",
      "training Loss has not improved for 126 epochs.\n",
      "current in epoch    131      batch 0\n",
      "RLoss: 71.71234130859375\n",
      "current in epoch    131      batch 1\n",
      "RLoss: 119.3965072631836\n",
      "current in epoch    131      batch 2\n",
      "RLoss: 18.549213409423828\n",
      "current in epoch    131      batch 3\n",
      "RLoss: 22.308643341064453\n",
      "current in epoch    131      batch 4\n",
      "RLoss: 494.2773132324219\n",
      "current in epoch    131      batch 5\n",
      "RLoss: 1570.1473388671875\n",
      "========================================\n",
      "Epoch 132/1000 - partial_train_loss: 206.6968 \n",
      "Epoch: [132/1000], TrainLoss: 1465.2784641810827\n",
      "training Loss has not improved for 127 epochs.\n",
      "current in epoch    132      batch 0\n",
      "RLoss: 1559.8013916015625\n",
      "current in epoch    132      batch 1\n",
      "RLoss: 715.429931640625\n",
      "current in epoch    132      batch 2\n",
      "RLoss: 344.63800048828125\n",
      "current in epoch    132      batch 3\n",
      "RLoss: 41.453121185302734\n",
      "current in epoch    132      batch 4\n",
      "RLoss: 381.1494140625\n",
      "current in epoch    132      batch 5\n",
      "RLoss: 704.0993041992188\n",
      "========================================\n",
      "Epoch 133/1000 - partial_train_loss: 864.1357 \n",
      "Epoch: [133/1000], TrainLoss: 589.1730902535575\n",
      "training Loss has not improved for 128 epochs.\n",
      "current in epoch    133      batch 0\n",
      "RLoss: 135.12274169921875\n",
      "current in epoch    133      batch 1\n",
      "RLoss: 621.4295043945312\n",
      "current in epoch    133      batch 2\n",
      "RLoss: 768.75244140625\n",
      "current in epoch    133      batch 3\n",
      "RLoss: 390.4831848144531\n",
      "current in epoch    133      batch 4\n",
      "RLoss: 239.14361572265625\n",
      "current in epoch    133      batch 5\n",
      "RLoss: 80.81480407714844\n",
      "========================================\n",
      "Epoch 134/1000 - partial_train_loss: 513.3402 \n",
      "Epoch: [134/1000], TrainLoss: 73.40781838553292\n",
      "training Loss has not improved for 129 epochs.\n",
      "current in epoch    134      batch 0\n",
      "RLoss: 580.398193359375\n",
      "current in epoch    134      batch 1\n",
      "RLoss: 117.98736572265625\n",
      "current in epoch    134      batch 2\n",
      "RLoss: 119.46845245361328\n",
      "current in epoch    134      batch 3\n",
      "RLoss: 396.32391357421875\n",
      "current in epoch    134      batch 4\n",
      "RLoss: 236.14593505859375\n",
      "current in epoch    134      batch 5\n",
      "RLoss: 145.6804962158203\n",
      "========================================\n",
      "Epoch 135/1000 - partial_train_loss: 187.5856 \n",
      "Epoch: [135/1000], TrainLoss: 122.2295526776995\n",
      "training Loss has not improved for 130 epochs.\n",
      "current in epoch    135      batch 0\n",
      "RLoss: 510.1330261230469\n",
      "current in epoch    135      batch 1\n",
      "RLoss: 1723.1748046875\n",
      "current in epoch    135      batch 2\n",
      "RLoss: 243.79576110839844\n",
      "current in epoch    135      batch 3\n",
      "RLoss: 257.3634338378906\n",
      "current in epoch    135      batch 4\n",
      "RLoss: 1015.9561157226562\n",
      "current in epoch    135      batch 5\n",
      "RLoss: 16.2830810546875\n",
      "========================================\n",
      "Epoch 136/1000 - partial_train_loss: 499.3465 \n",
      "sorting training set\n",
      "Epoch 136/1000 - Training loss: 15.9588 \n",
      "========================================\n",
      "Epoch: [136/1000], TrainLoss: 18.163390941097074\n",
      "training Loss has not improved for 131 epochs.\n",
      "current in epoch    136      batch 0\n",
      "RLoss: 932.1224975585938\n",
      "current in epoch    136      batch 1\n",
      "RLoss: 59.866905212402344\n",
      "current in epoch    136      batch 2\n",
      "RLoss: 98.14019012451172\n",
      "current in epoch    136      batch 3\n",
      "RLoss: 219.3802032470703\n",
      "current in epoch    136      batch 4\n",
      "RLoss: 14.58383846282959\n",
      "current in epoch    136      batch 5\n",
      "RLoss: 331.55535888671875\n",
      "========================================\n",
      "Epoch 137/1000 - partial_train_loss: 181.6977 \n",
      "Epoch: [137/1000], TrainLoss: 314.1001456124442\n",
      "training Loss has not improved for 132 epochs.\n",
      "current in epoch    137      batch 0\n",
      "RLoss: 748.9702758789062\n",
      "current in epoch    137      batch 1\n",
      "RLoss: 80.98562622070312\n",
      "current in epoch    137      batch 2\n",
      "RLoss: 7.527780055999756\n",
      "current in epoch    137      batch 3\n",
      "RLoss: 30.40477752685547\n",
      "current in epoch    137      batch 4\n",
      "RLoss: 702.0154418945312\n",
      "current in epoch    137      batch 5\n",
      "RLoss: 58.37777328491211\n",
      "========================================\n",
      "Epoch 138/1000 - partial_train_loss: 291.7515 \n",
      "Epoch: [138/1000], TrainLoss: 52.03896290915353\n",
      "training Loss has not improved for 133 epochs.\n",
      "current in epoch    138      batch 0\n",
      "RLoss: 19.57884979248047\n",
      "current in epoch    138      batch 1\n",
      "RLoss: 284.1337890625\n",
      "current in epoch    138      batch 2\n",
      "RLoss: 58.00855255126953\n",
      "current in epoch    138      batch 3\n",
      "RLoss: 91.38682556152344\n",
      "current in epoch    138      batch 4\n",
      "RLoss: 5.93799352645874\n",
      "current in epoch    138      batch 5\n",
      "RLoss: 177.82818603515625\n",
      "========================================\n",
      "Epoch 139/1000 - partial_train_loss: 82.2833 \n",
      "Epoch: [139/1000], TrainLoss: 181.91113771711076\n",
      "training Loss has not improved for 134 epochs.\n",
      "current in epoch    139      batch 0\n",
      "RLoss: 64.90495300292969\n",
      "current in epoch    139      batch 1\n",
      "RLoss: 161.41336059570312\n",
      "current in epoch    139      batch 2\n",
      "RLoss: 147.1995849609375\n",
      "current in epoch    139      batch 3\n",
      "RLoss: 15.896331787109375\n",
      "current in epoch    139      batch 4\n",
      "RLoss: 149.28965759277344\n",
      "current in epoch    139      batch 5\n",
      "RLoss: 213.64036560058594\n",
      "========================================\n",
      "Epoch 140/1000 - partial_train_loss: 125.3023 \n",
      "Epoch: [140/1000], TrainLoss: 178.18598828996932\n",
      "training Loss has not improved for 135 epochs.\n",
      "current in epoch    140      batch 0\n",
      "RLoss: 44.922325134277344\n",
      "current in epoch    140      batch 1\n",
      "RLoss: 40.15618133544922\n",
      "current in epoch    140      batch 2\n",
      "RLoss: 93.9391098022461\n",
      "current in epoch    140      batch 3\n",
      "RLoss: 3.2256178855895996\n",
      "current in epoch    140      batch 4\n",
      "RLoss: 60.275753021240234\n",
      "current in epoch    140      batch 5\n",
      "RLoss: 176.55621337890625\n",
      "========================================\n",
      "Epoch 141/1000 - partial_train_loss: 90.7197 \n",
      "sorting training set\n",
      "Epoch 141/1000 - Training loss: 205.1835 \n",
      "========================================\n",
      "Epoch: [141/1000], TrainLoss: 218.67514969386917\n",
      "training Loss has not improved for 136 epochs.\n",
      "current in epoch    141      batch 0\n",
      "RLoss: 595.4676513671875\n",
      "current in epoch    141      batch 1\n",
      "RLoss: 60.95817947387695\n",
      "current in epoch    141      batch 2\n",
      "RLoss: 290.64111328125\n",
      "current in epoch    141      batch 3\n",
      "RLoss: 205.4298553466797\n",
      "current in epoch    141      batch 4\n",
      "RLoss: 146.51190185546875\n",
      "current in epoch    141      batch 5\n",
      "RLoss: 37.12698745727539\n",
      "========================================\n",
      "Epoch 142/1000 - partial_train_loss: 307.3579 \n",
      "Epoch: [142/1000], TrainLoss: 37.88089384351458\n",
      "training Loss has not improved for 137 epochs.\n",
      "current in epoch    142      batch 0\n",
      "RLoss: 537.104248046875\n",
      "current in epoch    142      batch 1\n",
      "RLoss: 60.75950622558594\n",
      "current in epoch    142      batch 2\n",
      "RLoss: 177.34042358398438\n",
      "current in epoch    142      batch 3\n",
      "RLoss: 455.5694885253906\n",
      "current in epoch    142      batch 4\n",
      "RLoss: 1638.7398681640625\n",
      "current in epoch    142      batch 5\n",
      "RLoss: 265.0415954589844\n",
      "========================================\n",
      "Epoch 143/1000 - partial_train_loss: 442.7427 \n",
      "Epoch: [143/1000], TrainLoss: 303.5924208504813\n",
      "training Loss has not improved for 138 epochs.\n",
      "current in epoch    143      batch 0\n",
      "RLoss: 2638.367919921875\n",
      "current in epoch    143      batch 1\n",
      "RLoss: 1141.854248046875\n",
      "current in epoch    143      batch 2\n",
      "RLoss: 266.63238525390625\n",
      "current in epoch    143      batch 3\n",
      "RLoss: 825.7698364257812\n",
      "current in epoch    143      batch 4\n",
      "RLoss: 50.01780319213867\n",
      "current in epoch    143      batch 5\n",
      "RLoss: 6.13230562210083\n",
      "========================================\n",
      "Epoch 144/1000 - partial_train_loss: 762.3893 \n",
      "Epoch: [144/1000], TrainLoss: 17.558014478002274\n",
      "training Loss has not improved for 139 epochs.\n",
      "current in epoch    144      batch 0\n",
      "RLoss: 123.0667495727539\n",
      "current in epoch    144      batch 1\n",
      "RLoss: 40.35084533691406\n",
      "current in epoch    144      batch 2\n",
      "RLoss: 463.8077392578125\n",
      "current in epoch    144      batch 3\n",
      "RLoss: 338.93865966796875\n",
      "current in epoch    144      batch 4\n",
      "RLoss: 133.89599609375\n",
      "current in epoch    144      batch 5\n",
      "RLoss: 483.91192626953125\n",
      "========================================\n",
      "Epoch 145/1000 - partial_train_loss: 153.7836 \n",
      "Epoch: [145/1000], TrainLoss: 433.17227826799666\n",
      "training Loss has not improved for 140 epochs.\n",
      "current in epoch    145      batch 0\n",
      "RLoss: 1139.2550048828125\n",
      "current in epoch    145      batch 1\n",
      "RLoss: 370.8772277832031\n",
      "current in epoch    145      batch 2\n",
      "RLoss: 663.001220703125\n",
      "current in epoch    145      batch 3\n",
      "RLoss: 46.81658935546875\n",
      "current in epoch    145      batch 4\n",
      "RLoss: 2247.931640625\n",
      "current in epoch    145      batch 5\n",
      "RLoss: 1302.4842529296875\n",
      "========================================\n",
      "Epoch 146/1000 - partial_train_loss: 741.6103 \n",
      "sorting training set\n",
      "Epoch 146/1000 - Training loss: 732.9076 \n",
      "========================================\n",
      "Epoch: [146/1000], TrainLoss: 781.8186654284357\n",
      "training Loss has not improved for 141 epochs.\n",
      "current in epoch    146      batch 0\n",
      "RLoss: 526.8639526367188\n",
      "current in epoch    146      batch 1\n",
      "RLoss: 132.18328857421875\n",
      "current in epoch    146      batch 2\n",
      "RLoss: 370.78564453125\n",
      "current in epoch    146      batch 3\n",
      "RLoss: 62.695274353027344\n",
      "current in epoch    146      batch 4\n",
      "RLoss: 689.9566650390625\n",
      "current in epoch    146      batch 5\n",
      "RLoss: 698.5687866210938\n",
      "========================================\n",
      "Epoch 147/1000 - partial_train_loss: 776.2998 \n",
      "Epoch: [147/1000], TrainLoss: 313.4794781548636\n",
      "training Loss has not improved for 142 epochs.\n",
      "current in epoch    147      batch 0\n",
      "RLoss: 1445.7135009765625\n",
      "current in epoch    147      batch 1\n",
      "RLoss: 264.75518798828125\n",
      "current in epoch    147      batch 2\n",
      "RLoss: 522.3956298828125\n",
      "current in epoch    147      batch 3\n",
      "RLoss: 191.79429626464844\n",
      "current in epoch    147      batch 4\n",
      "RLoss: 272.5257873535156\n",
      "current in epoch    147      batch 5\n",
      "RLoss: 213.49078369140625\n",
      "========================================\n",
      "Epoch 148/1000 - partial_train_loss: 514.2785 \n",
      "Epoch: [148/1000], TrainLoss: 178.489134652274\n",
      "training Loss has not improved for 143 epochs.\n",
      "current in epoch    148      batch 0\n",
      "RLoss: 154.19601440429688\n",
      "current in epoch    148      batch 1\n",
      "RLoss: 502.812255859375\n",
      "current in epoch    148      batch 2\n",
      "RLoss: 71.98784637451172\n",
      "current in epoch    148      batch 3\n",
      "RLoss: 11.299012184143066\n",
      "current in epoch    148      batch 4\n",
      "RLoss: 151.9766082763672\n",
      "current in epoch    148      batch 5\n",
      "RLoss: 499.3341369628906\n",
      "========================================\n",
      "Epoch 149/1000 - partial_train_loss: 191.1484 \n",
      "Epoch: [149/1000], TrainLoss: 243.96860333851404\n",
      "training Loss has not improved for 144 epochs.\n",
      "current in epoch    149      batch 0\n",
      "RLoss: 618.8585205078125\n",
      "current in epoch    149      batch 1\n",
      "RLoss: 480.1649169921875\n",
      "current in epoch    149      batch 2\n",
      "RLoss: 1264.7520751953125\n",
      "current in epoch    149      batch 3\n",
      "RLoss: 748.973388671875\n",
      "current in epoch    149      batch 4\n",
      "RLoss: 47.57831954956055\n",
      "current in epoch    149      batch 5\n",
      "RLoss: 442.9143981933594\n",
      "========================================\n",
      "Epoch 150/1000 - partial_train_loss: 653.3312 \n",
      "Epoch: [150/1000], TrainLoss: 276.2037359986986\n",
      "training Loss has not improved for 145 epochs.\n",
      "current in epoch    150      batch 0\n",
      "RLoss: 99.2223892211914\n",
      "current in epoch    150      batch 1\n",
      "RLoss: 674.892822265625\n",
      "current in epoch    150      batch 2\n",
      "RLoss: 355.1666259765625\n",
      "current in epoch    150      batch 3\n",
      "RLoss: 1131.601806640625\n",
      "current in epoch    150      batch 4\n",
      "RLoss: 13.483591079711914\n",
      "current in epoch    150      batch 5\n",
      "RLoss: 480.69085693359375\n",
      "========================================\n",
      "Epoch 151/1000 - partial_train_loss: 605.1957 \n",
      "sorting training set\n",
      "Epoch 151/1000 - Training loss: 280.4503 \n",
      "========================================\n",
      "Epoch: [151/1000], TrainLoss: 297.87558891010497\n",
      "training Loss has not improved for 146 epochs.\n",
      "current in epoch    151      batch 0\n",
      "RLoss: 303.478515625\n",
      "current in epoch    151      batch 1\n",
      "RLoss: 39.19246292114258\n",
      "current in epoch    151      batch 2\n",
      "RLoss: 17.248348236083984\n",
      "current in epoch    151      batch 3\n",
      "RLoss: 137.86474609375\n",
      "current in epoch    151      batch 4\n",
      "RLoss: 581.4006958007812\n",
      "current in epoch    151      batch 5\n",
      "RLoss: 185.6066436767578\n",
      "========================================\n",
      "Epoch 152/1000 - partial_train_loss: 301.9860 \n",
      "Epoch: [152/1000], TrainLoss: 166.06082017081124\n",
      "training Loss has not improved for 147 epochs.\n",
      "current in epoch    152      batch 0\n",
      "RLoss: 952.6874389648438\n",
      "current in epoch    152      batch 1\n",
      "RLoss: 137.37831115722656\n",
      "current in epoch    152      batch 2\n",
      "RLoss: 1052.9677734375\n",
      "current in epoch    152      batch 3\n",
      "RLoss: 336.7408447265625\n",
      "current in epoch    152      batch 4\n",
      "RLoss: 379.4633483886719\n",
      "current in epoch    152      batch 5\n",
      "RLoss: 25.387487411499023\n",
      "========================================\n",
      "Epoch 153/1000 - partial_train_loss: 439.2670 \n",
      "Epoch: [153/1000], TrainLoss: 39.27984053747995\n",
      "training Loss has not improved for 148 epochs.\n",
      "current in epoch    153      batch 0\n",
      "RLoss: 655.545166015625\n",
      "current in epoch    153      batch 1\n",
      "RLoss: 901.8653564453125\n",
      "current in epoch    153      batch 2\n",
      "RLoss: 298.7921447753906\n",
      "current in epoch    153      batch 3\n",
      "RLoss: 1085.470947265625\n",
      "current in epoch    153      batch 4\n",
      "RLoss: 426.62579345703125\n",
      "current in epoch    153      batch 5\n",
      "RLoss: 432.4786376953125\n",
      "========================================\n",
      "Epoch 154/1000 - partial_train_loss: 477.6879 \n",
      "Epoch: [154/1000], TrainLoss: 288.7425106593541\n",
      "training Loss has not improved for 149 epochs.\n",
      "current in epoch    154      batch 0\n",
      "RLoss: 108.86447143554688\n",
      "current in epoch    154      batch 1\n",
      "RLoss: 19.20039176940918\n",
      "current in epoch    154      batch 2\n",
      "RLoss: 131.88172912597656\n",
      "current in epoch    154      batch 3\n",
      "RLoss: 92.56957244873047\n",
      "current in epoch    154      batch 4\n",
      "RLoss: 216.98886108398438\n",
      "current in epoch    154      batch 5\n",
      "RLoss: 958.2113647460938\n",
      "========================================\n",
      "Epoch 155/1000 - partial_train_loss: 183.0935 \n",
      "Epoch: [155/1000], TrainLoss: 1079.3648354666573\n",
      "training Loss has not improved for 150 epochs.\n",
      "current in epoch    155      batch 0\n",
      "RLoss: 990.8773803710938\n",
      "current in epoch    155      batch 1\n",
      "RLoss: 36.82173156738281\n",
      "current in epoch    155      batch 2\n",
      "RLoss: 257.0805358886719\n",
      "current in epoch    155      batch 3\n",
      "RLoss: 89.64472961425781\n",
      "current in epoch    155      batch 4\n",
      "RLoss: 246.87684631347656\n",
      "current in epoch    155      batch 5\n",
      "RLoss: 6.378077983856201\n",
      "========================================\n",
      "Epoch 156/1000 - partial_train_loss: 615.7812 \n",
      "sorting training set\n",
      "Epoch 156/1000 - Training loss: 9.2298 \n",
      "========================================\n",
      "Epoch: [156/1000], TrainLoss: 9.403542580227565\n",
      "training Loss has not improved for 151 epochs.\n",
      "current in epoch    156      batch 0\n",
      "RLoss: 499.456787109375\n",
      "current in epoch    156      batch 1\n",
      "RLoss: 150.7021942138672\n",
      "current in epoch    156      batch 2\n",
      "RLoss: 23.8044376373291\n",
      "current in epoch    156      batch 3\n",
      "RLoss: 42.87525177001953\n",
      "current in epoch    156      batch 4\n",
      "RLoss: 408.0228271484375\n",
      "current in epoch    156      batch 5\n",
      "RLoss: 155.94039916992188\n",
      "========================================\n",
      "Epoch 157/1000 - partial_train_loss: 261.8979 \n",
      "Epoch: [157/1000], TrainLoss: 221.48352786472864\n",
      "training Loss has not improved for 152 epochs.\n",
      "current in epoch    157      batch 0\n",
      "RLoss: 4.6388258934021\n",
      "current in epoch    157      batch 1\n",
      "RLoss: 12.793754577636719\n",
      "current in epoch    157      batch 2\n",
      "RLoss: 179.1856689453125\n",
      "current in epoch    157      batch 3\n",
      "RLoss: 12.800009727478027\n",
      "current in epoch    157      batch 4\n",
      "RLoss: 50.218894958496094\n",
      "current in epoch    157      batch 5\n",
      "RLoss: 53.68423843383789\n",
      "========================================\n",
      "Epoch 158/1000 - partial_train_loss: 91.8623 \n",
      "Epoch: [158/1000], TrainLoss: 44.5116936479296\n",
      "training Loss has not improved for 153 epochs.\n",
      "current in epoch    158      batch 0\n",
      "RLoss: 1011.4343872070312\n",
      "current in epoch    158      batch 1\n",
      "RLoss: 608.5383911132812\n",
      "current in epoch    158      batch 2\n",
      "RLoss: 391.9832763671875\n",
      "current in epoch    158      batch 3\n",
      "RLoss: 48.1490364074707\n",
      "current in epoch    158      batch 4\n",
      "RLoss: 172.96620178222656\n",
      "current in epoch    158      batch 5\n",
      "RLoss: 519.2329711914062\n",
      "========================================\n",
      "Epoch 159/1000 - partial_train_loss: 307.4807 \n",
      "Epoch: [159/1000], TrainLoss: 463.86109924316406\n",
      "training Loss has not improved for 154 epochs.\n",
      "current in epoch    159      batch 0\n",
      "RLoss: 156.59213256835938\n",
      "current in epoch    159      batch 1\n",
      "RLoss: 91.12271881103516\n",
      "current in epoch    159      batch 2\n",
      "RLoss: 1507.979248046875\n",
      "current in epoch    159      batch 3\n",
      "RLoss: 25.534496307373047\n",
      "current in epoch    159      batch 4\n",
      "RLoss: 31.927202224731445\n",
      "current in epoch    159      batch 5\n",
      "RLoss: 403.1081848144531\n",
      "========================================\n",
      "Epoch 160/1000 - partial_train_loss: 289.5052 \n",
      "Epoch: [160/1000], TrainLoss: 474.9587413242885\n",
      "training Loss has not improved for 155 epochs.\n",
      "current in epoch    160      batch 0\n",
      "RLoss: 193.27587890625\n",
      "current in epoch    160      batch 1\n",
      "RLoss: 284.0531921386719\n",
      "current in epoch    160      batch 2\n",
      "RLoss: 47.22355270385742\n",
      "current in epoch    160      batch 3\n",
      "RLoss: 35.65264892578125\n",
      "current in epoch    160      batch 4\n",
      "RLoss: 145.24290466308594\n",
      "current in epoch    160      batch 5\n",
      "RLoss: 746.6293334960938\n",
      "========================================\n",
      "Epoch 161/1000 - partial_train_loss: 223.4477 \n",
      "sorting training set\n",
      "Epoch 161/1000 - Training loss: 853.3133 \n",
      "========================================\n",
      "Epoch: [161/1000], TrainLoss: 909.971964767363\n",
      "training Loss has not improved for 156 epochs.\n",
      "current in epoch    161      batch 0\n",
      "RLoss: 360.649658203125\n",
      "current in epoch    161      batch 1\n",
      "RLoss: 1781.806884765625\n",
      "current in epoch    161      batch 2\n",
      "RLoss: 23.049375534057617\n",
      "current in epoch    161      batch 3\n",
      "RLoss: 817.4859008789062\n",
      "current in epoch    161      batch 4\n",
      "RLoss: 76.1153335571289\n",
      "current in epoch    161      batch 5\n",
      "RLoss: 877.9820556640625\n",
      "========================================\n",
      "Epoch 162/1000 - partial_train_loss: 878.8745 \n",
      "Epoch: [162/1000], TrainLoss: 773.8969715663364\n",
      "training Loss has not improved for 157 epochs.\n",
      "current in epoch    162      batch 0\n",
      "RLoss: 1794.323974609375\n",
      "current in epoch    162      batch 1\n",
      "RLoss: 83.98332214355469\n",
      "current in epoch    162      batch 2\n",
      "RLoss: 1401.07080078125\n",
      "current in epoch    162      batch 3\n",
      "RLoss: 433.27655029296875\n",
      "current in epoch    162      batch 4\n",
      "RLoss: 679.9046020507812\n",
      "current in epoch    162      batch 5\n",
      "RLoss: 90.78053283691406\n",
      "========================================\n",
      "Epoch 163/1000 - partial_train_loss: 869.0608 \n",
      "Epoch: [163/1000], TrainLoss: 59.46385737827846\n",
      "training Loss has not improved for 158 epochs.\n",
      "current in epoch    163      batch 0\n",
      "RLoss: 450.0018310546875\n",
      "current in epoch    163      batch 1\n",
      "RLoss: 12.985553741455078\n",
      "current in epoch    163      batch 2\n",
      "RLoss: 120.41230010986328\n",
      "current in epoch    163      batch 3\n",
      "RLoss: 62.825836181640625\n",
      "current in epoch    163      batch 4\n",
      "RLoss: 405.17120361328125\n",
      "current in epoch    163      batch 5\n",
      "RLoss: 195.7255859375\n",
      "========================================\n",
      "Epoch 164/1000 - partial_train_loss: 171.3948 \n",
      "Epoch: [164/1000], TrainLoss: 175.4176401410784\n",
      "training Loss has not improved for 159 epochs.\n",
      "current in epoch    164      batch 0\n",
      "RLoss: 741.6679077148438\n",
      "current in epoch    164      batch 1\n",
      "RLoss: 158.2393341064453\n",
      "current in epoch    164      batch 2\n",
      "RLoss: 121.23236846923828\n",
      "current in epoch    164      batch 3\n",
      "RLoss: 106.37384796142578\n",
      "current in epoch    164      batch 4\n",
      "RLoss: 54.05440902709961\n",
      "current in epoch    164      batch 5\n",
      "RLoss: 237.835205078125\n",
      "========================================\n",
      "Epoch 165/1000 - partial_train_loss: 209.9930 \n",
      "Epoch: [165/1000], TrainLoss: 531.0465134211948\n",
      "training Loss has not improved for 160 epochs.\n",
      "current in epoch    165      batch 0\n",
      "RLoss: 441.9887390136719\n",
      "current in epoch    165      batch 1\n",
      "RLoss: 193.20550537109375\n",
      "current in epoch    165      batch 2\n",
      "RLoss: 285.4134521484375\n",
      "current in epoch    165      batch 3\n",
      "RLoss: 397.05621337890625\n",
      "current in epoch    165      batch 4\n",
      "RLoss: 143.04786682128906\n",
      "current in epoch    165      batch 5\n",
      "RLoss: 103.24925994873047\n",
      "========================================\n",
      "Epoch 166/1000 - partial_train_loss: 226.0535 \n",
      "sorting training set\n",
      "Epoch 166/1000 - Training loss: 84.7916 \n",
      "========================================\n",
      "Epoch: [166/1000], TrainLoss: 90.73334643311725\n",
      "training Loss has not improved for 161 epochs.\n",
      "current in epoch    166      batch 0\n",
      "RLoss: 723.8038940429688\n",
      "current in epoch    166      batch 1\n",
      "RLoss: 1413.2308349609375\n",
      "current in epoch    166      batch 2\n",
      "RLoss: 57.87873458862305\n",
      "current in epoch    166      batch 3\n",
      "RLoss: 1086.8642578125\n",
      "current in epoch    166      batch 4\n",
      "RLoss: 87.1864242553711\n",
      "current in epoch    166      batch 5\n",
      "RLoss: 306.8060607910156\n",
      "========================================\n",
      "Epoch 167/1000 - partial_train_loss: 673.8933 \n",
      "Epoch: [167/1000], TrainLoss: 200.07180786132812\n",
      "training Loss has not improved for 162 epochs.\n",
      "current in epoch    167      batch 0\n",
      "RLoss: 865.4177856445312\n",
      "current in epoch    167      batch 1\n",
      "RLoss: 310.31243896484375\n",
      "current in epoch    167      batch 2\n",
      "RLoss: 476.4345703125\n",
      "current in epoch    167      batch 3\n",
      "RLoss: 1107.475341796875\n",
      "current in epoch    167      batch 4\n",
      "RLoss: 1464.97900390625\n",
      "current in epoch    167      batch 5\n",
      "RLoss: 196.8576202392578\n",
      "========================================\n",
      "Epoch 168/1000 - partial_train_loss: 763.2837 \n",
      "Epoch: [168/1000], TrainLoss: 88.54080336434501\n",
      "training Loss has not improved for 163 epochs.\n",
      "current in epoch    168      batch 0\n",
      "RLoss: 250.7142791748047\n",
      "current in epoch    168      batch 1\n",
      "RLoss: 527.9270629882812\n",
      "current in epoch    168      batch 2\n",
      "RLoss: 311.52899169921875\n",
      "current in epoch    168      batch 3\n",
      "RLoss: 81.60014343261719\n",
      "current in epoch    168      batch 4\n",
      "RLoss: 62.63435363769531\n",
      "current in epoch    168      batch 5\n",
      "RLoss: 251.27220153808594\n",
      "========================================\n",
      "Epoch 169/1000 - partial_train_loss: 187.5451 \n",
      "Epoch: [169/1000], TrainLoss: 231.88415854317802\n",
      "training Loss has not improved for 164 epochs.\n",
      "current in epoch    169      batch 0\n",
      "RLoss: 65.00302124023438\n",
      "current in epoch    169      batch 1\n",
      "RLoss: 30.729658126831055\n",
      "current in epoch    169      batch 2\n",
      "RLoss: 182.4971160888672\n",
      "current in epoch    169      batch 3\n",
      "RLoss: 105.5261001586914\n",
      "current in epoch    169      batch 4\n",
      "RLoss: 100.97216796875\n",
      "current in epoch    169      batch 5\n",
      "RLoss: 54.80522918701172\n",
      "========================================\n",
      "Epoch 170/1000 - partial_train_loss: 126.6831 \n",
      "Epoch: [170/1000], TrainLoss: 36.38591480255127\n",
      "training Loss has not improved for 165 epochs.\n",
      "current in epoch    170      batch 0\n",
      "RLoss: 359.9912414550781\n",
      "current in epoch    170      batch 1\n",
      "RLoss: 41.69866180419922\n",
      "current in epoch    170      batch 2\n",
      "RLoss: 952.74072265625\n",
      "current in epoch    170      batch 3\n",
      "RLoss: 1526.5718994140625\n",
      "current in epoch    170      batch 4\n",
      "RLoss: 866.11572265625\n",
      "current in epoch    170      batch 5\n",
      "RLoss: 45.34760284423828\n",
      "========================================\n",
      "Epoch 171/1000 - partial_train_loss: 520.9609 \n",
      "sorting training set\n",
      "Epoch 171/1000 - Training loss: 37.1777 \n",
      "========================================\n",
      "Epoch: [171/1000], TrainLoss: 39.648639177420655\n",
      "training Loss has not improved for 166 epochs.\n",
      "current in epoch    171      batch 0\n",
      "RLoss: 96.04572296142578\n",
      "current in epoch    171      batch 1\n",
      "RLoss: 562.639892578125\n",
      "current in epoch    171      batch 2\n",
      "RLoss: 466.227294921875\n",
      "current in epoch    171      batch 3\n",
      "RLoss: 80.44490814208984\n",
      "current in epoch    171      batch 4\n",
      "RLoss: 314.5704040527344\n",
      "current in epoch    171      batch 5\n",
      "RLoss: 686.357177734375\n",
      "========================================\n",
      "Epoch 172/1000 - partial_train_loss: 231.0888 \n",
      "Epoch: [172/1000], TrainLoss: 518.4207507542202\n",
      "training Loss has not improved for 167 epochs.\n",
      "current in epoch    172      batch 0\n",
      "RLoss: 54.563392639160156\n",
      "current in epoch    172      batch 1\n",
      "RLoss: 499.40325927734375\n",
      "current in epoch    172      batch 2\n",
      "RLoss: 2808.4384765625\n",
      "current in epoch    172      batch 3\n",
      "RLoss: 418.8673400878906\n",
      "current in epoch    172      batch 4\n",
      "RLoss: 509.40740966796875\n",
      "current in epoch    172      batch 5\n",
      "RLoss: 799.4691772460938\n",
      "========================================\n",
      "Epoch 173/1000 - partial_train_loss: 906.8962 \n",
      "Epoch: [173/1000], TrainLoss: 474.8517905644008\n",
      "training Loss has not improved for 168 epochs.\n",
      "current in epoch    173      batch 0\n",
      "RLoss: 490.2853088378906\n",
      "current in epoch    173      batch 1\n",
      "RLoss: 284.0788879394531\n",
      "current in epoch    173      batch 2\n",
      "RLoss: 892.4391479492188\n",
      "current in epoch    173      batch 3\n",
      "RLoss: 32.25295639038086\n",
      "current in epoch    173      batch 4\n",
      "RLoss: 370.5295715332031\n",
      "current in epoch    173      batch 5\n",
      "RLoss: 206.68687438964844\n",
      "========================================\n",
      "Epoch 174/1000 - partial_train_loss: 728.0476 \n",
      "Epoch: [174/1000], TrainLoss: 263.5775827680315\n",
      "training Loss has not improved for 169 epochs.\n",
      "current in epoch    174      batch 0\n",
      "RLoss: 39.444679260253906\n",
      "current in epoch    174      batch 1\n",
      "RLoss: 1271.74658203125\n",
      "current in epoch    174      batch 2\n",
      "RLoss: 2771.017333984375\n",
      "current in epoch    174      batch 3\n",
      "RLoss: 457.013427734375\n",
      "current in epoch    174      batch 4\n",
      "RLoss: 143.18028259277344\n",
      "current in epoch    174      batch 5\n",
      "RLoss: 53.825401306152344\n",
      "========================================\n",
      "Epoch 175/1000 - partial_train_loss: 730.1184 \n",
      "Epoch: [175/1000], TrainLoss: 44.86003426143101\n",
      "training Loss has not improved for 170 epochs.\n",
      "current in epoch    175      batch 0\n",
      "RLoss: 256.106689453125\n",
      "current in epoch    175      batch 1\n",
      "RLoss: 31.877492904663086\n",
      "current in epoch    175      batch 2\n",
      "RLoss: 172.42727661132812\n",
      "current in epoch    175      batch 3\n",
      "RLoss: 381.82611083984375\n",
      "current in epoch    175      batch 4\n",
      "RLoss: 10.246183395385742\n",
      "current in epoch    175      batch 5\n",
      "RLoss: 88.70548248291016\n",
      "========================================\n",
      "Epoch 176/1000 - partial_train_loss: 107.0210 \n",
      "sorting training set\n",
      "Epoch 176/1000 - Training loss: 55.1677 \n",
      "========================================\n",
      "Epoch: [176/1000], TrainLoss: 57.609163746786976\n",
      "training Loss has not improved for 171 epochs.\n",
      "current in epoch    176      batch 0\n",
      "RLoss: 180.75662231445312\n",
      "current in epoch    176      batch 1\n",
      "RLoss: 59.538604736328125\n",
      "current in epoch    176      batch 2\n",
      "RLoss: 271.1736755371094\n",
      "current in epoch    176      batch 3\n",
      "RLoss: 23.150156021118164\n",
      "current in epoch    176      batch 4\n",
      "RLoss: 24.865934371948242\n",
      "current in epoch    176      batch 5\n",
      "RLoss: 32.048683166503906\n",
      "========================================\n",
      "Epoch 177/1000 - partial_train_loss: 113.7964 \n",
      "Epoch: [177/1000], TrainLoss: 30.326282364981516\n",
      "training Loss has not improved for 172 epochs.\n",
      "current in epoch    177      batch 0\n",
      "RLoss: 28.36690902709961\n",
      "current in epoch    177      batch 1\n",
      "RLoss: 58.409820556640625\n",
      "current in epoch    177      batch 2\n",
      "RLoss: 769.3068237304688\n",
      "current in epoch    177      batch 3\n",
      "RLoss: 1391.579345703125\n",
      "current in epoch    177      batch 4\n",
      "RLoss: 140.83761596679688\n",
      "current in epoch    177      batch 5\n",
      "RLoss: 291.0799560546875\n",
      "========================================\n",
      "Epoch 178/1000 - partial_train_loss: 395.7340 \n",
      "Epoch: [178/1000], TrainLoss: 338.75933619907926\n",
      "training Loss has not improved for 173 epochs.\n",
      "current in epoch    178      batch 0\n",
      "RLoss: 254.89373779296875\n",
      "current in epoch    178      batch 1\n",
      "RLoss: 885.9049682617188\n",
      "current in epoch    178      batch 2\n",
      "RLoss: 512.9529418945312\n",
      "current in epoch    178      batch 3\n",
      "RLoss: 45.251678466796875\n",
      "current in epoch    178      batch 4\n",
      "RLoss: 172.22193908691406\n",
      "current in epoch    178      batch 5\n",
      "RLoss: 323.9615478515625\n",
      "========================================\n",
      "Epoch 179/1000 - partial_train_loss: 308.5982 \n",
      "Epoch: [179/1000], TrainLoss: 292.13151005336215\n",
      "training Loss has not improved for 174 epochs.\n",
      "current in epoch    179      batch 0\n",
      "RLoss: 70.47053527832031\n",
      "current in epoch    179      batch 1\n",
      "RLoss: 2473.99755859375\n",
      "current in epoch    179      batch 2\n",
      "RLoss: 48.63710021972656\n",
      "current in epoch    179      batch 3\n",
      "RLoss: 12.275430679321289\n",
      "current in epoch    179      batch 4\n",
      "RLoss: 3.1915242671966553\n",
      "current in epoch    179      batch 5\n",
      "RLoss: 31.805376052856445\n",
      "========================================\n",
      "Epoch 180/1000 - partial_train_loss: 456.1536 \n",
      "Epoch: [180/1000], TrainLoss: 49.28141519001552\n",
      "training Loss has not improved for 175 epochs.\n",
      "current in epoch    180      batch 0\n",
      "RLoss: 145.2783966064453\n",
      "current in epoch    180      batch 1\n",
      "RLoss: 146.73281860351562\n",
      "current in epoch    180      batch 2\n",
      "RLoss: 134.45677185058594\n",
      "current in epoch    180      batch 3\n",
      "RLoss: 203.9524688720703\n",
      "current in epoch    180      batch 4\n",
      "RLoss: 186.4677276611328\n",
      "current in epoch    180      batch 5\n",
      "RLoss: 78.11319732666016\n",
      "========================================\n",
      "Epoch 181/1000 - partial_train_loss: 135.9450 \n",
      "sorting training set\n",
      "Epoch 181/1000 - Training loss: 48.5392 \n",
      "========================================\n",
      "Epoch: [181/1000], TrainLoss: 52.15721963529351\n",
      "training Loss has not improved for 176 epochs.\n",
      "current in epoch    181      batch 0\n",
      "RLoss: 463.364990234375\n",
      "current in epoch    181      batch 1\n",
      "RLoss: 22.17470932006836\n",
      "current in epoch    181      batch 2\n",
      "RLoss: 206.77786254882812\n",
      "current in epoch    181      batch 3\n",
      "RLoss: 230.41844177246094\n",
      "current in epoch    181      batch 4\n",
      "RLoss: 266.9787902832031\n",
      "current in epoch    181      batch 5\n",
      "RLoss: 297.6484375\n",
      "========================================\n",
      "Epoch 182/1000 - partial_train_loss: 183.0995 \n",
      "Epoch: [182/1000], TrainLoss: 192.78528485979353\n",
      "training Loss has not improved for 177 epochs.\n",
      "current in epoch    182      batch 0\n",
      "RLoss: 99.97319793701172\n",
      "current in epoch    182      batch 1\n",
      "RLoss: 1090.794677734375\n",
      "current in epoch    182      batch 2\n",
      "RLoss: 572.1233520507812\n",
      "current in epoch    182      batch 3\n",
      "RLoss: 907.6976928710938\n",
      "current in epoch    182      batch 4\n",
      "RLoss: 1459.4302978515625\n",
      "current in epoch    182      batch 5\n",
      "RLoss: 312.4300231933594\n",
      "========================================\n",
      "Epoch 183/1000 - partial_train_loss: 771.2303 \n",
      "Epoch: [183/1000], TrainLoss: 729.2755497523716\n",
      "training Loss has not improved for 178 epochs.\n",
      "current in epoch    183      batch 0\n",
      "RLoss: 455.51727294921875\n",
      "current in epoch    183      batch 1\n",
      "RLoss: 74.41085815429688\n",
      "current in epoch    183      batch 2\n",
      "RLoss: 27.743694305419922\n",
      "current in epoch    183      batch 3\n",
      "RLoss: 54.49541091918945\n",
      "current in epoch    183      batch 4\n",
      "RLoss: 267.66839599609375\n",
      "current in epoch    183      batch 5\n",
      "RLoss: 1084.525634765625\n",
      "========================================\n",
      "Epoch 184/1000 - partial_train_loss: 174.7423 \n",
      "Epoch: [184/1000], TrainLoss: 793.9141551426479\n",
      "training Loss has not improved for 179 epochs.\n",
      "current in epoch    184      batch 0\n",
      "RLoss: 95.51544189453125\n",
      "current in epoch    184      batch 1\n",
      "RLoss: 52.33056640625\n",
      "current in epoch    184      batch 2\n",
      "RLoss: 373.8023376464844\n",
      "current in epoch    184      batch 3\n",
      "RLoss: 313.8034973144531\n",
      "current in epoch    184      batch 4\n",
      "RLoss: 1695.8109130859375\n",
      "current in epoch    184      batch 5\n",
      "RLoss: 34.18870544433594\n",
      "========================================\n",
      "Epoch 185/1000 - partial_train_loss: 706.7546 \n",
      "Epoch: [185/1000], TrainLoss: 417.70797627312794\n",
      "training Loss has not improved for 180 epochs.\n",
      "current in epoch    185      batch 0\n",
      "RLoss: 2755.103759765625\n",
      "current in epoch    185      batch 1\n",
      "RLoss: 545.203857421875\n",
      "current in epoch    185      batch 2\n",
      "RLoss: 2136.98681640625\n",
      "current in epoch    185      batch 3\n",
      "RLoss: 42.28559494018555\n",
      "current in epoch    185      batch 4\n",
      "RLoss: 57.93202590942383\n",
      "current in epoch    185      batch 5\n",
      "RLoss: 161.9407196044922\n",
      "========================================\n",
      "Epoch 186/1000 - partial_train_loss: 824.3447 \n",
      "sorting training set\n",
      "Epoch 186/1000 - Training loss: 150.6748 \n",
      "========================================\n",
      "Epoch: [186/1000], TrainLoss: 160.692018591253\n",
      "training Loss has not improved for 181 epochs.\n",
      "current in epoch    186      batch 0\n",
      "RLoss: 130.26773071289062\n",
      "current in epoch    186      batch 1\n",
      "RLoss: 129.55360412597656\n",
      "current in epoch    186      batch 2\n",
      "RLoss: 137.3227081298828\n",
      "current in epoch    186      batch 3\n",
      "RLoss: 139.75119018554688\n",
      "current in epoch    186      batch 4\n",
      "RLoss: 81.22340393066406\n",
      "current in epoch    186      batch 5\n",
      "RLoss: 57.45956802368164\n",
      "========================================\n",
      "Epoch 187/1000 - partial_train_loss: 164.0966 \n",
      "Epoch: [187/1000], TrainLoss: 58.40629400525774\n",
      "training Loss has not improved for 182 epochs.\n",
      "current in epoch    187      batch 0\n",
      "RLoss: 1687.8238525390625\n",
      "current in epoch    187      batch 1\n",
      "RLoss: 561.867431640625\n",
      "current in epoch    187      batch 2\n",
      "RLoss: 49.89773178100586\n",
      "current in epoch    187      batch 3\n",
      "RLoss: 111.12196350097656\n",
      "current in epoch    187      batch 4\n",
      "RLoss: 156.69482421875\n",
      "current in epoch    187      batch 5\n",
      "RLoss: 93.26644897460938\n",
      "========================================\n",
      "Epoch 188/1000 - partial_train_loss: 344.8214 \n",
      "Epoch: [188/1000], TrainLoss: 99.83449608939034\n",
      "training Loss has not improved for 183 epochs.\n",
      "current in epoch    188      batch 0\n",
      "RLoss: 82.65587615966797\n",
      "current in epoch    188      batch 1\n",
      "RLoss: 172.1659393310547\n",
      "current in epoch    188      batch 2\n",
      "RLoss: 98.42048645019531\n",
      "current in epoch    188      batch 3\n",
      "RLoss: 46.759613037109375\n",
      "current in epoch    188      batch 4\n",
      "RLoss: 137.33779907226562\n",
      "current in epoch    188      batch 5\n",
      "RLoss: 206.7858428955078\n",
      "========================================\n",
      "Epoch 189/1000 - partial_train_loss: 97.6305 \n",
      "Epoch: [189/1000], TrainLoss: 139.93882315499442\n",
      "training Loss has not improved for 184 epochs.\n",
      "current in epoch    189      batch 0\n",
      "RLoss: 217.42649841308594\n",
      "current in epoch    189      batch 1\n",
      "RLoss: 786.3502197265625\n",
      "current in epoch    189      batch 2\n",
      "RLoss: 179.88385009765625\n",
      "current in epoch    189      batch 3\n",
      "RLoss: 132.3775634765625\n",
      "current in epoch    189      batch 4\n",
      "RLoss: 177.3165740966797\n",
      "current in epoch    189      batch 5\n",
      "RLoss: 18.417062759399414\n",
      "========================================\n",
      "Epoch 190/1000 - partial_train_loss: 230.6025 \n",
      "Epoch: [190/1000], TrainLoss: 15.454697166170392\n",
      "training Loss has not improved for 185 epochs.\n",
      "current in epoch    190      batch 0\n",
      "RLoss: 374.593017578125\n",
      "current in epoch    190      batch 1\n",
      "RLoss: 211.48228454589844\n",
      "current in epoch    190      batch 2\n",
      "RLoss: 182.80117797851562\n",
      "current in epoch    190      batch 3\n",
      "RLoss: 119.51948547363281\n",
      "current in epoch    190      batch 4\n",
      "RLoss: 356.2947692871094\n",
      "current in epoch    190      batch 5\n",
      "RLoss: 1050.295166015625\n",
      "========================================\n",
      "Epoch 191/1000 - partial_train_loss: 172.6197 \n",
      "sorting training set\n",
      "Epoch 191/1000 - Training loss: 797.5316 \n",
      "========================================\n",
      "Epoch: [191/1000], TrainLoss: 851.2823528710936\n",
      "training Loss has not improved for 186 epochs.\n",
      "current in epoch    191      batch 0\n",
      "RLoss: 188.6588592529297\n",
      "current in epoch    191      batch 1\n",
      "RLoss: 2.8394365310668945\n",
      "current in epoch    191      batch 2\n",
      "RLoss: 57.6008186340332\n",
      "current in epoch    191      batch 3\n",
      "RLoss: 741.5962524414062\n",
      "current in epoch    191      batch 4\n",
      "RLoss: 22.88107681274414\n",
      "current in epoch    191      batch 5\n",
      "RLoss: 571.7442016601562\n",
      "========================================\n",
      "Epoch 192/1000 - partial_train_loss: 608.6938 \n",
      "Epoch: [192/1000], TrainLoss: 555.5837642124721\n",
      "training Loss has not improved for 187 epochs.\n",
      "current in epoch    192      batch 0\n",
      "RLoss: 46.996097564697266\n",
      "current in epoch    192      batch 1\n",
      "RLoss: 33.20756912231445\n",
      "current in epoch    192      batch 2\n",
      "RLoss: 48.57979965209961\n",
      "current in epoch    192      batch 3\n",
      "RLoss: 28.417173385620117\n",
      "current in epoch    192      batch 4\n",
      "RLoss: 202.25833129882812\n",
      "current in epoch    192      batch 5\n",
      "RLoss: 287.0243225097656\n",
      "========================================\n",
      "Epoch 193/1000 - partial_train_loss: 153.0570 \n",
      "Epoch: [193/1000], TrainLoss: 342.53726741245816\n",
      "training Loss has not improved for 188 epochs.\n",
      "current in epoch    193      batch 0\n",
      "RLoss: 1027.688232421875\n",
      "current in epoch    193      batch 1\n",
      "RLoss: 825.0811157226562\n",
      "current in epoch    193      batch 2\n",
      "RLoss: 791.8490600585938\n",
      "current in epoch    193      batch 3\n",
      "RLoss: 1949.468994140625\n",
      "current in epoch    193      batch 4\n",
      "RLoss: 191.54812622070312\n",
      "current in epoch    193      batch 5\n",
      "RLoss: 346.5659484863281\n",
      "========================================\n",
      "Epoch 194/1000 - partial_train_loss: 691.7294 \n",
      "Epoch: [194/1000], TrainLoss: 387.77815083095004\n",
      "training Loss has not improved for 189 epochs.\n",
      "current in epoch    194      batch 0\n",
      "RLoss: 274.8877258300781\n",
      "current in epoch    194      batch 1\n",
      "RLoss: 21.43793487548828\n",
      "current in epoch    194      batch 2\n",
      "RLoss: 27.06940269470215\n",
      "current in epoch    194      batch 3\n",
      "RLoss: 2953.87255859375\n",
      "current in epoch    194      batch 4\n",
      "RLoss: 22.97641944885254\n",
      "current in epoch    194      batch 5\n",
      "RLoss: 18.60118865966797\n",
      "========================================\n",
      "Epoch 195/1000 - partial_train_loss: 528.0972 \n",
      "Epoch: [195/1000], TrainLoss: 28.85037578855242\n",
      "training Loss has not improved for 190 epochs.\n",
      "current in epoch    195      batch 0\n",
      "RLoss: 8715.4404296875\n",
      "current in epoch    195      batch 1\n",
      "RLoss: 1655.1109619140625\n",
      "current in epoch    195      batch 2\n",
      "RLoss: 258.30511474609375\n",
      "current in epoch    195      batch 3\n",
      "RLoss: 732.92529296875\n",
      "current in epoch    195      batch 4\n",
      "RLoss: 122.52603912353516\n",
      "current in epoch    195      batch 5\n",
      "RLoss: 458.0099182128906\n",
      "========================================\n",
      "Epoch 196/1000 - partial_train_loss: 1590.1650 \n",
      "sorting training set\n",
      "Epoch 196/1000 - Training loss: 257.9179 \n",
      "========================================\n",
      "Epoch: [196/1000], TrainLoss: 272.7242561403249\n",
      "training Loss has not improved for 191 epochs.\n",
      "current in epoch    196      batch 0\n",
      "RLoss: 248.29583740234375\n",
      "current in epoch    196      batch 1\n",
      "RLoss: 246.16656494140625\n",
      "current in epoch    196      batch 2\n",
      "RLoss: 90.25731658935547\n",
      "current in epoch    196      batch 3\n",
      "RLoss: 44.20674133300781\n",
      "current in epoch    196      batch 4\n",
      "RLoss: 530.0394897460938\n",
      "current in epoch    196      batch 5\n",
      "RLoss: 547.8641967773438\n",
      "========================================\n",
      "Epoch 197/1000 - partial_train_loss: 334.8315 \n",
      "Epoch: [197/1000], TrainLoss: 519.0275998796735\n",
      "training Loss has not improved for 192 epochs.\n",
      "current in epoch    197      batch 0\n",
      "RLoss: 820.1004028320312\n",
      "current in epoch    197      batch 1\n",
      "RLoss: 320.90771484375\n",
      "current in epoch    197      batch 2\n",
      "RLoss: 153.14950561523438\n",
      "current in epoch    197      batch 3\n",
      "RLoss: 286.3368225097656\n",
      "current in epoch    197      batch 4\n",
      "RLoss: 181.8313751220703\n",
      "current in epoch    197      batch 5\n",
      "RLoss: 9.985663414001465\n",
      "========================================\n",
      "Epoch 198/1000 - partial_train_loss: 432.1646 \n",
      "Epoch: [198/1000], TrainLoss: 15.5851548739842\n",
      "training Loss has not improved for 193 epochs.\n",
      "current in epoch    198      batch 0\n",
      "RLoss: 360.8743896484375\n",
      "current in epoch    198      batch 1\n",
      "RLoss: 151.9530487060547\n",
      "current in epoch    198      batch 2\n",
      "RLoss: 55.60386276245117\n",
      "current in epoch    198      batch 3\n",
      "RLoss: 54.38840103149414\n",
      "current in epoch    198      batch 4\n",
      "RLoss: 260.51617431640625\n",
      "current in epoch    198      batch 5\n",
      "RLoss: 9.484184265136719\n",
      "========================================\n",
      "Epoch 199/1000 - partial_train_loss: 131.1579 \n",
      "Epoch: [199/1000], TrainLoss: 22.21056580543518\n",
      "training Loss has not improved for 194 epochs.\n",
      "current in epoch    199      batch 0\n",
      "RLoss: 1450.4957275390625\n",
      "current in epoch    199      batch 1\n",
      "RLoss: 27.283710479736328\n",
      "current in epoch    199      batch 2\n",
      "RLoss: 712.1677856445312\n",
      "current in epoch    199      batch 3\n",
      "RLoss: 767.7651977539062\n",
      "current in epoch    199      batch 4\n",
      "RLoss: 10.860962867736816\n",
      "current in epoch    199      batch 5\n",
      "RLoss: 59.75175476074219\n",
      "========================================\n",
      "Epoch 200/1000 - partial_train_loss: 413.0388 \n",
      "Epoch: [200/1000], TrainLoss: 69.17179775238037\n",
      "training Loss has not improved for 195 epochs.\n",
      "current in epoch    200      batch 0\n",
      "RLoss: 53.53376770019531\n",
      "current in epoch    200      batch 1\n",
      "RLoss: 32.81046676635742\n",
      "current in epoch    200      batch 2\n",
      "RLoss: 67.91271209716797\n",
      "current in epoch    200      batch 3\n",
      "RLoss: 186.51422119140625\n",
      "current in epoch    200      batch 4\n",
      "RLoss: 135.2135772705078\n",
      "current in epoch    200      batch 5\n",
      "RLoss: 219.39036560058594\n",
      "========================================\n",
      "Epoch 201/1000 - partial_train_loss: 83.0302 \n",
      "sorting training set\n",
      "Epoch 201/1000 - Training loss: 360.7919 \n",
      "========================================\n",
      "Epoch: [201/1000], TrainLoss: 385.30064583157997\n",
      "training Loss has not improved for 196 epochs.\n",
      "current in epoch    201      batch 0\n",
      "RLoss: 92.8944091796875\n",
      "current in epoch    201      batch 1\n",
      "RLoss: 70.44285583496094\n",
      "current in epoch    201      batch 2\n",
      "RLoss: 10.123946189880371\n",
      "current in epoch    201      batch 3\n",
      "RLoss: 23.49854850769043\n",
      "current in epoch    201      batch 4\n",
      "RLoss: 67.77037048339844\n",
      "current in epoch    201      batch 5\n",
      "RLoss: 1792.6248779296875\n",
      "========================================\n",
      "Epoch 202/1000 - partial_train_loss: 300.1806 \n",
      "Epoch: [202/1000], TrainLoss: 1245.1136954171318\n",
      "training Loss has not improved for 197 epochs.\n",
      "current in epoch    202      batch 0\n",
      "RLoss: 91.92816162109375\n",
      "current in epoch    202      batch 1\n",
      "RLoss: 135.4877471923828\n",
      "current in epoch    202      batch 2\n",
      "RLoss: 227.10633850097656\n",
      "current in epoch    202      batch 3\n",
      "RLoss: 5.71373987197876\n",
      "current in epoch    202      batch 4\n",
      "RLoss: 214.05679321289062\n",
      "current in epoch    202      batch 5\n",
      "RLoss: 78.33834838867188\n",
      "========================================\n",
      "Epoch 203/1000 - partial_train_loss: 493.6211 \n",
      "Epoch: [203/1000], TrainLoss: 65.03308827536446\n",
      "training Loss has not improved for 198 epochs.\n",
      "current in epoch    203      batch 0\n",
      "RLoss: 112.35733795166016\n",
      "current in epoch    203      batch 1\n",
      "RLoss: 11.767049789428711\n",
      "current in epoch    203      batch 2\n",
      "RLoss: 659.5713500976562\n",
      "current in epoch    203      batch 3\n",
      "RLoss: 283.7303771972656\n",
      "current in epoch    203      batch 4\n",
      "RLoss: 63.94867706298828\n",
      "current in epoch    203      batch 5\n",
      "RLoss: 67.90331268310547\n",
      "========================================\n",
      "Epoch 204/1000 - partial_train_loss: 203.5042 \n",
      "Epoch: [204/1000], TrainLoss: 101.27984196799142\n",
      "training Loss has not improved for 199 epochs.\n",
      "current in epoch    204      batch 0\n",
      "RLoss: 168.11204528808594\n",
      "current in epoch    204      batch 1\n",
      "RLoss: 26.136194229125977\n",
      "current in epoch    204      batch 2\n",
      "RLoss: 385.3404235839844\n",
      "current in epoch    204      batch 3\n",
      "RLoss: 37.43766403198242\n",
      "current in epoch    204      batch 4\n",
      "RLoss: 19.994720458984375\n",
      "current in epoch    204      batch 5\n",
      "RLoss: 26.91896629333496\n",
      "========================================\n",
      "Epoch 205/1000 - partial_train_loss: 108.4994 \n",
      "Epoch: [205/1000], TrainLoss: 48.76409544263567\n",
      "training Loss has not improved for 200 epochs.\n",
      "current in epoch    205      batch 0\n",
      "RLoss: 352.5986328125\n",
      "current in epoch    205      batch 1\n",
      "RLoss: 27.080270767211914\n",
      "current in epoch    205      batch 2\n",
      "RLoss: 695.2628173828125\n",
      "current in epoch    205      batch 3\n",
      "RLoss: 284.3013610839844\n",
      "current in epoch    205      batch 4\n",
      "RLoss: 38.20033264160156\n",
      "current in epoch    205      batch 5\n",
      "RLoss: 187.47142028808594\n",
      "========================================\n",
      "Epoch 206/1000 - partial_train_loss: 205.6555 \n",
      "sorting training set\n",
      "Epoch 206/1000 - Training loss: 766.6011 \n",
      "========================================\n",
      "Epoch: [206/1000], TrainLoss: 817.9423485567248\n",
      "training Loss has not improved for 201 epochs.\n",
      "current in epoch    206      batch 0\n",
      "RLoss: 21.59080696105957\n",
      "current in epoch    206      batch 1\n",
      "RLoss: 10.907490730285645\n",
      "current in epoch    206      batch 2\n",
      "RLoss: 545.3477172851562\n",
      "current in epoch    206      batch 3\n",
      "RLoss: 189.41470336914062\n",
      "current in epoch    206      batch 4\n",
      "RLoss: 35.774024963378906\n",
      "current in epoch    206      batch 5\n",
      "RLoss: 4.854257583618164\n",
      "========================================\n",
      "Epoch 207/1000 - partial_train_loss: 622.2018 \n",
      "Epoch: [207/1000], TrainLoss: 16.33063373395375\n",
      "training Loss has not improved for 202 epochs.\n",
      "current in epoch    207      batch 0\n",
      "RLoss: 69.7841796875\n",
      "current in epoch    207      batch 1\n",
      "RLoss: 24.567928314208984\n",
      "current in epoch    207      batch 2\n",
      "RLoss: 36.49444580078125\n",
      "current in epoch    207      batch 3\n",
      "RLoss: 288.7392272949219\n",
      "current in epoch    207      batch 4\n",
      "RLoss: 187.3458709716797\n",
      "current in epoch    207      batch 5\n",
      "RLoss: 40.31814956665039\n",
      "========================================\n",
      "Epoch 208/1000 - partial_train_loss: 106.5686 \n",
      "Epoch: [208/1000], TrainLoss: 397.5841266087123\n",
      "training Loss has not improved for 203 epochs.\n",
      "current in epoch    208      batch 0\n",
      "RLoss: 48.556129455566406\n",
      "current in epoch    208      batch 1\n",
      "RLoss: 92.10120391845703\n",
      "current in epoch    208      batch 2\n",
      "RLoss: 31.070209503173828\n",
      "current in epoch    208      batch 3\n",
      "RLoss: 29.619430541992188\n",
      "current in epoch    208      batch 4\n",
      "RLoss: 37.78767776489258\n",
      "current in epoch    208      batch 5\n",
      "RLoss: 5.514736175537109\n",
      "========================================\n",
      "Epoch 209/1000 - partial_train_loss: 47.9960 \n",
      "Epoch: [209/1000], TrainLoss: 33.54947653838566\n",
      "training Loss has not improved for 204 epochs.\n",
      "current in epoch    209      batch 0\n",
      "RLoss: 163.29388427734375\n",
      "current in epoch    209      batch 1\n",
      "RLoss: 26.36695098876953\n",
      "current in epoch    209      batch 2\n",
      "RLoss: 189.4843292236328\n",
      "current in epoch    209      batch 3\n",
      "RLoss: 149.39053344726562\n",
      "current in epoch    209      batch 4\n",
      "RLoss: 272.64825439453125\n",
      "current in epoch    209      batch 5\n",
      "RLoss: 27.01675796508789\n",
      "========================================\n",
      "Epoch 210/1000 - partial_train_loss: 128.5720 \n",
      "Epoch: [210/1000], TrainLoss: 237.47720302854265\n",
      "training Loss has not improved for 205 epochs.\n",
      "current in epoch    210      batch 0\n",
      "RLoss: 32.68828582763672\n",
      "current in epoch    210      batch 1\n",
      "RLoss: 188.8035125732422\n",
      "current in epoch    210      batch 2\n",
      "RLoss: 180.708984375\n",
      "current in epoch    210      batch 3\n",
      "RLoss: 253.30661010742188\n",
      "current in epoch    210      batch 4\n",
      "RLoss: 261.30810546875\n",
      "current in epoch    210      batch 5\n",
      "RLoss: 72.82498168945312\n",
      "========================================\n",
      "Epoch 211/1000 - partial_train_loss: 165.9636 \n",
      "sorting training set\n",
      "Epoch 211/1000 - Training loss: 218.7564 \n",
      "========================================\n",
      "Epoch: [211/1000], TrainLoss: 233.08510701488393\n",
      "training Loss has not improved for 206 epochs.\n",
      "current in epoch    211      batch 0\n",
      "RLoss: 565.4536743164062\n",
      "current in epoch    211      batch 1\n",
      "RLoss: 11.654191017150879\n",
      "current in epoch    211      batch 2\n",
      "RLoss: 452.0150451660156\n",
      "current in epoch    211      batch 3\n",
      "RLoss: 384.3915710449219\n",
      "current in epoch    211      batch 4\n",
      "RLoss: 28.63011932373047\n",
      "current in epoch    211      batch 5\n",
      "RLoss: 473.0304260253906\n",
      "========================================\n",
      "Epoch 212/1000 - partial_train_loss: 395.6583 \n",
      "Epoch: [212/1000], TrainLoss: 584.8614153180804\n",
      "training Loss has not improved for 207 epochs.\n",
      "current in epoch    212      batch 0\n",
      "RLoss: 12.828153610229492\n",
      "current in epoch    212      batch 1\n",
      "RLoss: 160.6706085205078\n",
      "current in epoch    212      batch 2\n",
      "RLoss: 179.76986694335938\n",
      "current in epoch    212      batch 3\n",
      "RLoss: 69.45572662353516\n",
      "current in epoch    212      batch 4\n",
      "RLoss: 21.951045989990234\n",
      "current in epoch    212      batch 5\n",
      "RLoss: 70.65655517578125\n",
      "========================================\n",
      "Epoch 213/1000 - partial_train_loss: 109.0226 \n",
      "Epoch: [213/1000], TrainLoss: 47.562469244003296\n",
      "training Loss has not improved for 208 epochs.\n",
      "current in epoch    213      batch 0\n",
      "RLoss: 230.14675903320312\n",
      "current in epoch    213      batch 1\n",
      "RLoss: 49.937381744384766\n",
      "current in epoch    213      batch 2\n",
      "RLoss: 389.4751281738281\n",
      "current in epoch    213      batch 3\n",
      "RLoss: 97.27886962890625\n",
      "current in epoch    213      batch 4\n",
      "RLoss: 130.0815887451172\n",
      "current in epoch    213      batch 5\n",
      "RLoss: 212.78176879882812\n",
      "========================================\n",
      "Epoch 214/1000 - partial_train_loss: 171.2626 \n",
      "Epoch: [214/1000], TrainLoss: 437.268214906965\n",
      "training Loss has not improved for 209 epochs.\n",
      "current in epoch    214      batch 0\n",
      "RLoss: 85.6195297241211\n",
      "current in epoch    214      batch 1\n",
      "RLoss: 228.23289489746094\n",
      "current in epoch    214      batch 2\n",
      "RLoss: 70.35486602783203\n",
      "current in epoch    214      batch 3\n",
      "RLoss: 20.222509384155273\n",
      "current in epoch    214      batch 4\n",
      "RLoss: 113.26535034179688\n",
      "current in epoch    214      batch 5\n",
      "RLoss: 20.90974235534668\n",
      "========================================\n",
      "Epoch 215/1000 - partial_train_loss: 81.3529 \n",
      "Epoch: [215/1000], TrainLoss: 20.89360807623182\n",
      "training Loss has not improved for 210 epochs.\n",
      "current in epoch    215      batch 0\n",
      "RLoss: 5.534377574920654\n",
      "current in epoch    215      batch 1\n",
      "RLoss: 206.97384643554688\n",
      "current in epoch    215      batch 2\n",
      "RLoss: 1.2039058208465576\n",
      "current in epoch    215      batch 3\n",
      "RLoss: 19.656984329223633\n",
      "current in epoch    215      batch 4\n",
      "RLoss: 24.335803985595703\n",
      "current in epoch    215      batch 5\n",
      "RLoss: 51.12809753417969\n",
      "========================================\n",
      "Epoch 216/1000 - partial_train_loss: 57.7898 \n",
      "sorting training set\n",
      "Epoch 216/1000 - Training loss: 80.3800 \n",
      "========================================\n",
      "Epoch: [216/1000], TrainLoss: 85.03184323364523\n",
      "training Loss has not improved for 211 epochs.\n",
      "current in epoch    216      batch 0\n",
      "RLoss: 1207.0377197265625\n",
      "current in epoch    216      batch 1\n",
      "RLoss: 1110.2197265625\n",
      "current in epoch    216      batch 2\n",
      "RLoss: 81.5597152709961\n",
      "current in epoch    216      batch 3\n",
      "RLoss: 125.7571029663086\n",
      "current in epoch    216      batch 4\n",
      "RLoss: 145.60971069335938\n",
      "current in epoch    216      batch 5\n",
      "RLoss: 94.9083251953125\n",
      "========================================\n",
      "Epoch 217/1000 - partial_train_loss: 383.3195 \n",
      "Epoch: [217/1000], TrainLoss: 102.19938441685268\n",
      "training Loss has not improved for 212 epochs.\n",
      "current in epoch    217      batch 0\n",
      "RLoss: 135.13177490234375\n",
      "current in epoch    217      batch 1\n",
      "RLoss: 167.7123565673828\n",
      "current in epoch    217      batch 2\n",
      "RLoss: 49.768592834472656\n",
      "current in epoch    217      batch 3\n",
      "RLoss: 142.1593017578125\n",
      "current in epoch    217      batch 4\n",
      "RLoss: 523.8621215820312\n",
      "current in epoch    217      batch 5\n",
      "RLoss: 54.061832427978516\n",
      "========================================\n",
      "Epoch 218/1000 - partial_train_loss: 181.3725 \n",
      "Epoch: [218/1000], TrainLoss: 39.41708619253976\n",
      "training Loss has not improved for 213 epochs.\n",
      "current in epoch    218      batch 0\n",
      "RLoss: 385.6804504394531\n",
      "current in epoch    218      batch 1\n",
      "RLoss: 60.24501419067383\n",
      "current in epoch    218      batch 2\n",
      "RLoss: 45.1327018737793\n",
      "current in epoch    218      batch 3\n",
      "RLoss: 143.2480926513672\n",
      "current in epoch    218      batch 4\n",
      "RLoss: 76.70919036865234\n",
      "current in epoch    218      batch 5\n",
      "RLoss: 44.16276550292969\n",
      "========================================\n",
      "Epoch 219/1000 - partial_train_loss: 99.3403 \n",
      "Epoch: [219/1000], TrainLoss: 51.61952304840088\n",
      "training Loss has not improved for 214 epochs.\n",
      "current in epoch    219      batch 0\n",
      "RLoss: 600.453125\n",
      "current in epoch    219      batch 1\n",
      "RLoss: 41.348419189453125\n",
      "current in epoch    219      batch 2\n",
      "RLoss: 30.903827667236328\n",
      "current in epoch    219      batch 3\n",
      "RLoss: 96.21797943115234\n",
      "current in epoch    219      batch 4\n",
      "RLoss: 45.39421463012695\n",
      "current in epoch    219      batch 5\n",
      "RLoss: 603.265869140625\n",
      "========================================\n",
      "Epoch 220/1000 - partial_train_loss: 99.8426 \n",
      "Epoch: [220/1000], TrainLoss: 706.138177054269\n",
      "training Loss has not improved for 215 epochs.\n",
      "current in epoch    220      batch 0\n",
      "RLoss: 129.2412567138672\n",
      "current in epoch    220      batch 1\n",
      "RLoss: 183.014404296875\n",
      "current in epoch    220      batch 2\n",
      "RLoss: 17.49423599243164\n",
      "current in epoch    220      batch 3\n",
      "RLoss: 39.543800354003906\n",
      "current in epoch    220      batch 4\n",
      "RLoss: 27.817996978759766\n",
      "current in epoch    220      batch 5\n",
      "RLoss: 362.8343200683594\n",
      "========================================\n",
      "Epoch 221/1000 - partial_train_loss: 204.3037 \n",
      "sorting training set\n",
      "Epoch 221/1000 - Training loss: 331.0443 \n",
      "========================================\n",
      "Epoch: [221/1000], TrainLoss: 350.05877055183595\n",
      "training Loss has not improved for 216 epochs.\n",
      "current in epoch    221      batch 0\n",
      "RLoss: 693.8377685546875\n",
      "current in epoch    221      batch 1\n",
      "RLoss: 1665.36669921875\n",
      "current in epoch    221      batch 2\n",
      "RLoss: 155.85415649414062\n",
      "current in epoch    221      batch 3\n",
      "RLoss: 69.85726928710938\n",
      "current in epoch    221      batch 4\n",
      "RLoss: 815.9253540039062\n",
      "current in epoch    221      batch 5\n",
      "RLoss: 370.2189636230469\n",
      "========================================\n",
      "Epoch 222/1000 - partial_train_loss: 668.7817 \n",
      "Epoch: [222/1000], TrainLoss: 274.5643497194563\n",
      "training Loss has not improved for 217 epochs.\n",
      "current in epoch    222      batch 0\n",
      "RLoss: 31.61307144165039\n",
      "current in epoch    222      batch 1\n",
      "RLoss: 486.667724609375\n",
      "current in epoch    222      batch 2\n",
      "RLoss: 1462.6395263671875\n",
      "current in epoch    222      batch 3\n",
      "RLoss: 103.6205825805664\n",
      "current in epoch    222      batch 4\n",
      "RLoss: 29.07199478149414\n",
      "current in epoch    222      batch 5\n",
      "RLoss: 362.05218505859375\n",
      "========================================\n",
      "Epoch 223/1000 - partial_train_loss: 532.4210 \n",
      "Epoch: [223/1000], TrainLoss: 272.84576688494\n",
      "training Loss has not improved for 218 epochs.\n",
      "current in epoch    223      batch 0\n",
      "RLoss: 154.48841857910156\n",
      "current in epoch    223      batch 1\n",
      "RLoss: 91.40748596191406\n",
      "current in epoch    223      batch 2\n",
      "RLoss: 270.1228332519531\n",
      "current in epoch    223      batch 3\n",
      "RLoss: 149.55825805664062\n",
      "current in epoch    223      batch 4\n",
      "RLoss: 486.79241943359375\n",
      "current in epoch    223      batch 5\n",
      "RLoss: 46.85971450805664\n",
      "========================================\n",
      "Epoch 224/1000 - partial_train_loss: 295.4616 \n",
      "Epoch: [224/1000], TrainLoss: 59.235915660858154\n",
      "training Loss has not improved for 219 epochs.\n",
      "current in epoch    224      batch 0\n",
      "RLoss: 952.437255859375\n",
      "current in epoch    224      batch 1\n",
      "RLoss: 32.732460021972656\n",
      "current in epoch    224      batch 2\n",
      "RLoss: 18.708993911743164\n",
      "current in epoch    224      batch 3\n",
      "RLoss: 257.9018249511719\n",
      "current in epoch    224      batch 4\n",
      "RLoss: 275.91033935546875\n",
      "current in epoch    224      batch 5\n",
      "RLoss: 395.0597839355469\n",
      "========================================\n",
      "Epoch 225/1000 - partial_train_loss: 239.6124 \n",
      "Epoch: [225/1000], TrainLoss: 351.9890807015555\n",
      "training Loss has not improved for 220 epochs.\n",
      "current in epoch    225      batch 0\n",
      "RLoss: 2584.815673828125\n",
      "current in epoch    225      batch 1\n",
      "RLoss: 103.29119873046875\n",
      "current in epoch    225      batch 2\n",
      "RLoss: 110.98978424072266\n",
      "current in epoch    225      batch 3\n",
      "RLoss: 11.05637264251709\n",
      "current in epoch    225      batch 4\n",
      "RLoss: 50.73875045776367\n",
      "current in epoch    225      batch 5\n",
      "RLoss: 57.575191497802734\n",
      "========================================\n",
      "Epoch 226/1000 - partial_train_loss: 494.8167 \n",
      "sorting training set\n",
      "Epoch 226/1000 - Training loss: 36.5133 \n",
      "========================================\n",
      "Epoch: [226/1000], TrainLoss: 39.14781750198318\n",
      "training Loss has not improved for 221 epochs.\n",
      "current in epoch    226      batch 0\n",
      "RLoss: 168.65232849121094\n",
      "current in epoch    226      batch 1\n",
      "RLoss: 429.1387023925781\n",
      "current in epoch    226      batch 2\n",
      "RLoss: 312.1152038574219\n",
      "current in epoch    226      batch 3\n",
      "RLoss: 1377.445068359375\n",
      "current in epoch    226      batch 4\n",
      "RLoss: 553.5081787109375\n",
      "current in epoch    226      batch 5\n",
      "RLoss: 473.6269226074219\n",
      "========================================\n",
      "Epoch 227/1000 - partial_train_loss: 509.0906 \n",
      "Epoch: [227/1000], TrainLoss: 272.8254928588867\n",
      "training Loss has not improved for 222 epochs.\n",
      "current in epoch    227      batch 0\n",
      "RLoss: 11.048585891723633\n",
      "current in epoch    227      batch 1\n",
      "RLoss: 55.16692352294922\n",
      "current in epoch    227      batch 2\n",
      "RLoss: 53.811370849609375\n",
      "current in epoch    227      batch 3\n",
      "RLoss: 370.6884765625\n",
      "current in epoch    227      batch 4\n",
      "RLoss: 503.641357421875\n",
      "current in epoch    227      batch 5\n",
      "RLoss: 176.99664306640625\n",
      "========================================\n",
      "Epoch 228/1000 - partial_train_loss: 341.6701 \n",
      "Epoch: [228/1000], TrainLoss: 128.07892499651228\n",
      "training Loss has not improved for 223 epochs.\n",
      "current in epoch    228      batch 0\n",
      "RLoss: 664.6622924804688\n",
      "current in epoch    228      batch 1\n",
      "RLoss: 131.50132751464844\n",
      "current in epoch    228      batch 2\n",
      "RLoss: 9.08941650390625\n",
      "current in epoch    228      batch 3\n",
      "RLoss: 14.73371410369873\n",
      "current in epoch    228      batch 4\n",
      "RLoss: 9.692280769348145\n",
      "current in epoch    228      batch 5\n",
      "RLoss: 51.19839859008789\n",
      "========================================\n",
      "Epoch 229/1000 - partial_train_loss: 194.3094 \n",
      "Epoch: [229/1000], TrainLoss: 30.977263075964792\n",
      "training Loss has not improved for 224 epochs.\n",
      "current in epoch    229      batch 0\n",
      "RLoss: 632.3177490234375\n",
      "current in epoch    229      batch 1\n",
      "RLoss: 48.653297424316406\n",
      "current in epoch    229      batch 2\n",
      "RLoss: 89.61065673828125\n",
      "current in epoch    229      batch 3\n",
      "RLoss: 3050.915283203125\n",
      "current in epoch    229      batch 4\n",
      "RLoss: 685.5318603515625\n",
      "current in epoch    229      batch 5\n",
      "RLoss: 85.98030090332031\n",
      "========================================\n",
      "Epoch 230/1000 - partial_train_loss: 636.2747 \n",
      "Epoch: [230/1000], TrainLoss: 71.44590486798968\n",
      "training Loss has not improved for 225 epochs.\n",
      "current in epoch    230      batch 0\n",
      "RLoss: 486.3778076171875\n",
      "current in epoch    230      batch 1\n",
      "RLoss: 365.9770812988281\n",
      "current in epoch    230      batch 2\n",
      "RLoss: 3060.22998046875\n",
      "current in epoch    230      batch 3\n",
      "RLoss: 399.93701171875\n",
      "current in epoch    230      batch 4\n",
      "RLoss: 944.5322875976562\n",
      "current in epoch    230      batch 5\n",
      "RLoss: 1160.721923828125\n",
      "========================================\n",
      "Epoch 231/1000 - partial_train_loss: 843.0342 \n",
      "sorting training set\n",
      "Epoch 231/1000 - Training loss: 836.1818 \n",
      "========================================\n",
      "Epoch: [231/1000], TrainLoss: 883.3043929555187\n",
      "training Loss has not improved for 226 epochs.\n",
      "current in epoch    231      batch 0\n",
      "RLoss: 1009.648681640625\n",
      "current in epoch    231      batch 1\n",
      "RLoss: 860.3499145507812\n",
      "current in epoch    231      batch 2\n",
      "RLoss: 4807.54248046875\n",
      "current in epoch    231      batch 3\n",
      "RLoss: 42.9642448425293\n",
      "current in epoch    231      batch 4\n",
      "RLoss: 113.82083892822266\n",
      "current in epoch    231      batch 5\n",
      "RLoss: 96.80776977539062\n",
      "========================================\n",
      "Epoch 232/1000 - partial_train_loss: 1610.3250 \n",
      "Epoch: [232/1000], TrainLoss: 56.06029994147165\n",
      "training Loss has not improved for 227 epochs.\n",
      "current in epoch    232      batch 0\n",
      "RLoss: 253.9833984375\n",
      "current in epoch    232      batch 1\n",
      "RLoss: 360.2311096191406\n",
      "current in epoch    232      batch 2\n",
      "RLoss: 226.1960906982422\n",
      "current in epoch    232      batch 3\n",
      "RLoss: 363.347900390625\n",
      "current in epoch    232      batch 4\n",
      "RLoss: 7.222262859344482\n",
      "current in epoch    232      batch 5\n",
      "RLoss: 180.37518310546875\n",
      "========================================\n",
      "Epoch 233/1000 - partial_train_loss: 220.8486 \n",
      "Epoch: [233/1000], TrainLoss: 159.97395215715682\n",
      "training Loss has not improved for 228 epochs.\n",
      "current in epoch    233      batch 0\n",
      "RLoss: 623.5252685546875\n",
      "current in epoch    233      batch 1\n",
      "RLoss: 2040.105224609375\n",
      "current in epoch    233      batch 2\n",
      "RLoss: 82.10255432128906\n",
      "current in epoch    233      batch 3\n",
      "RLoss: 23.623180389404297\n",
      "current in epoch    233      batch 4\n",
      "RLoss: 1033.775634765625\n",
      "current in epoch    233      batch 5\n",
      "RLoss: 35.373435974121094\n",
      "========================================\n",
      "Epoch 234/1000 - partial_train_loss: 557.2725 \n",
      "Epoch: [234/1000], TrainLoss: 25.974138924053737\n",
      "training Loss has not improved for 229 epochs.\n",
      "current in epoch    234      batch 0\n",
      "RLoss: 142.35171508789062\n",
      "current in epoch    234      batch 1\n",
      "RLoss: 72.96888732910156\n",
      "current in epoch    234      batch 2\n",
      "RLoss: 43.79899978637695\n",
      "current in epoch    234      batch 3\n",
      "RLoss: 516.8263549804688\n",
      "current in epoch    234      batch 4\n",
      "RLoss: 82.13914489746094\n",
      "current in epoch    234      batch 5\n",
      "RLoss: 8.759804725646973\n",
      "========================================\n",
      "Epoch 235/1000 - partial_train_loss: 135.6654 \n",
      "Epoch: [235/1000], TrainLoss: 5.545819612486022\n",
      "training Loss has not improved for 230 epochs.\n",
      "current in epoch    235      batch 0\n",
      "RLoss: 52.581787109375\n",
      "current in epoch    235      batch 1\n",
      "RLoss: 790.600341796875\n",
      "current in epoch    235      batch 2\n",
      "RLoss: 235.23838806152344\n",
      "current in epoch    235      batch 3\n",
      "RLoss: 39.8968620300293\n",
      "current in epoch    235      batch 4\n",
      "RLoss: 1858.9598388671875\n",
      "current in epoch    235      batch 5\n",
      "RLoss: 1707.3890380859375\n",
      "========================================\n",
      "Epoch 236/1000 - partial_train_loss: 452.3651 \n",
      "sorting training set\n",
      "Epoch 236/1000 - Training loss: 1261.0974 \n",
      "========================================\n",
      "Epoch: [236/1000], TrainLoss: 1340.8845304559206\n",
      "training Loss has not improved for 231 epochs.\n",
      "current in epoch    236      batch 0\n",
      "RLoss: 430.99884033203125\n",
      "current in epoch    236      batch 1\n",
      "RLoss: 1140.49072265625\n",
      "current in epoch    236      batch 2\n",
      "RLoss: 168.137939453125\n",
      "current in epoch    236      batch 3\n",
      "RLoss: 85.8858413696289\n",
      "current in epoch    236      batch 4\n",
      "RLoss: 147.57110595703125\n",
      "current in epoch    236      batch 5\n",
      "RLoss: 66.32052612304688\n",
      "========================================\n",
      "Epoch 237/1000 - partial_train_loss: 936.7485 \n",
      "Epoch: [237/1000], TrainLoss: 45.78608250617981\n",
      "training Loss has not improved for 232 epochs.\n",
      "current in epoch    237      batch 0\n",
      "RLoss: 89.1856689453125\n",
      "current in epoch    237      batch 1\n",
      "RLoss: 173.87026977539062\n",
      "current in epoch    237      batch 2\n",
      "RLoss: 47.94022750854492\n",
      "current in epoch    237      batch 3\n",
      "RLoss: 61.850223541259766\n",
      "current in epoch    237      batch 4\n",
      "RLoss: 48.88138198852539\n",
      "current in epoch    237      batch 5\n",
      "RLoss: 39.34102249145508\n",
      "========================================\n",
      "Epoch 238/1000 - partial_train_loss: 78.0890 \n",
      "Epoch: [238/1000], TrainLoss: 35.54813766479492\n",
      "training Loss has not improved for 233 epochs.\n",
      "current in epoch    238      batch 0\n",
      "RLoss: 381.4512939453125\n",
      "current in epoch    238      batch 1\n",
      "RLoss: 63.97294235229492\n",
      "current in epoch    238      batch 2\n",
      "RLoss: 34.85979461669922\n",
      "current in epoch    238      batch 3\n",
      "RLoss: 356.9173583984375\n",
      "current in epoch    238      batch 4\n",
      "RLoss: 76.51177978515625\n",
      "current in epoch    238      batch 5\n",
      "RLoss: 64.86356353759766\n",
      "========================================\n",
      "Epoch 239/1000 - partial_train_loss: 146.2704 \n",
      "Epoch: [239/1000], TrainLoss: 49.339792251586914\n",
      "training Loss has not improved for 234 epochs.\n",
      "current in epoch    239      batch 0\n",
      "RLoss: 12.643888473510742\n",
      "current in epoch    239      batch 1\n",
      "RLoss: 241.18301391601562\n",
      "current in epoch    239      batch 2\n",
      "RLoss: 12.253490447998047\n",
      "current in epoch    239      batch 3\n",
      "RLoss: 60.69070053100586\n",
      "current in epoch    239      batch 4\n",
      "RLoss: 364.936767578125\n",
      "current in epoch    239      batch 5\n",
      "RLoss: 21.38386344909668\n",
      "========================================\n",
      "Epoch 240/1000 - partial_train_loss: 146.1921 \n",
      "Epoch: [240/1000], TrainLoss: 17.3423604794911\n",
      "training Loss has not improved for 235 epochs.\n",
      "current in epoch    240      batch 0\n",
      "RLoss: 220.78518676757812\n",
      "current in epoch    240      batch 1\n",
      "RLoss: 110.94478607177734\n",
      "current in epoch    240      batch 2\n",
      "RLoss: 91.93917846679688\n",
      "current in epoch    240      batch 3\n",
      "RLoss: 80.0478286743164\n",
      "current in epoch    240      batch 4\n",
      "RLoss: 208.67044067382812\n",
      "current in epoch    240      batch 5\n",
      "RLoss: 6.377176284790039\n",
      "========================================\n",
      "Epoch 241/1000 - partial_train_loss: 91.5285 \n",
      "sorting training set\n",
      "Epoch 241/1000 - Training loss: 9.0632 \n",
      "========================================\n",
      "Epoch: [241/1000], TrainLoss: 9.410642697000407\n",
      "training Loss has not improved for 236 epochs.\n",
      "current in epoch    241      batch 0\n",
      "RLoss: 223.36924743652344\n",
      "current in epoch    241      batch 1\n",
      "RLoss: 192.88677978515625\n",
      "current in epoch    241      batch 2\n",
      "RLoss: 83.06503295898438\n",
      "current in epoch    241      batch 3\n",
      "RLoss: 232.3225555419922\n",
      "current in epoch    241      batch 4\n",
      "RLoss: 134.43350219726562\n",
      "current in epoch    241      batch 5\n",
      "RLoss: 25.112791061401367\n",
      "========================================\n",
      "Epoch 242/1000 - partial_train_loss: 131.5503 \n",
      "Epoch: [242/1000], TrainLoss: 30.23796568598066\n",
      "training Loss has not improved for 237 epochs.\n",
      "current in epoch    242      batch 0\n",
      "RLoss: 65.08731842041016\n",
      "current in epoch    242      batch 1\n",
      "RLoss: 42.39189529418945\n",
      "current in epoch    242      batch 2\n",
      "RLoss: 174.64134216308594\n",
      "current in epoch    242      batch 3\n",
      "RLoss: 32.184959411621094\n",
      "current in epoch    242      batch 4\n",
      "RLoss: 18.209444046020508\n",
      "current in epoch    242      batch 5\n",
      "RLoss: 21.100351333618164\n",
      "========================================\n",
      "Epoch 243/1000 - partial_train_loss: 61.1645 \n",
      "Epoch: [243/1000], TrainLoss: 19.846294982092722\n",
      "training Loss has not improved for 238 epochs.\n",
      "current in epoch    243      batch 0\n",
      "RLoss: 11.39634895324707\n",
      "current in epoch    243      batch 1\n",
      "RLoss: 53.46131134033203\n",
      "current in epoch    243      batch 2\n",
      "RLoss: 408.7857666015625\n",
      "current in epoch    243      batch 3\n",
      "RLoss: 226.0749053955078\n",
      "current in epoch    243      batch 4\n",
      "RLoss: 153.67987060546875\n",
      "current in epoch    243      batch 5\n",
      "RLoss: 111.25304412841797\n",
      "========================================\n",
      "Epoch 244/1000 - partial_train_loss: 140.1490 \n",
      "Epoch: [244/1000], TrainLoss: 107.24429512023926\n",
      "training Loss has not improved for 239 epochs.\n",
      "current in epoch    244      batch 0\n",
      "RLoss: 289.03973388671875\n",
      "current in epoch    244      batch 1\n",
      "RLoss: 47.635223388671875\n",
      "current in epoch    244      batch 2\n",
      "RLoss: 968.4352416992188\n",
      "current in epoch    244      batch 3\n",
      "RLoss: 101.2249526977539\n",
      "current in epoch    244      batch 4\n",
      "RLoss: 104.27870178222656\n",
      "current in epoch    244      batch 5\n",
      "RLoss: 52.73695755004883\n",
      "========================================\n",
      "Epoch 245/1000 - partial_train_loss: 238.6347 \n",
      "Epoch: [245/1000], TrainLoss: 43.45532315117972\n",
      "training Loss has not improved for 240 epochs.\n",
      "current in epoch    245      batch 0\n",
      "RLoss: 330.6098937988281\n",
      "current in epoch    245      batch 1\n",
      "RLoss: 153.92843627929688\n",
      "current in epoch    245      batch 2\n",
      "RLoss: 125.74940490722656\n",
      "current in epoch    245      batch 3\n",
      "RLoss: 484.4581298828125\n",
      "current in epoch    245      batch 4\n",
      "RLoss: 24.319042205810547\n",
      "current in epoch    245      batch 5\n",
      "RLoss: 27.222204208374023\n",
      "========================================\n",
      "Epoch 246/1000 - partial_train_loss: 173.9539 \n",
      "sorting training set\n",
      "Epoch 246/1000 - Training loss: 20.6108 \n",
      "========================================\n",
      "Epoch: [246/1000], TrainLoss: 22.37163324010029\n",
      "training Loss has not improved for 241 epochs.\n",
      "current in epoch    246      batch 0\n",
      "RLoss: 414.40325927734375\n",
      "current in epoch    246      batch 1\n",
      "RLoss: 53.08854675292969\n",
      "current in epoch    246      batch 2\n",
      "RLoss: 218.53421020507812\n",
      "current in epoch    246      batch 3\n",
      "RLoss: 40.49204635620117\n",
      "current in epoch    246      batch 4\n",
      "RLoss: 72.2707290649414\n",
      "current in epoch    246      batch 5\n",
      "RLoss: 76.3405532836914\n",
      "========================================\n",
      "Epoch 247/1000 - partial_train_loss: 152.9223 \n",
      "Epoch: [247/1000], TrainLoss: 57.1740768296378\n",
      "training Loss has not improved for 242 epochs.\n",
      "current in epoch    247      batch 0\n",
      "RLoss: 58.8558464050293\n",
      "current in epoch    247      batch 1\n",
      "RLoss: 70.9002914428711\n",
      "current in epoch    247      batch 2\n",
      "RLoss: 220.11502075195312\n",
      "current in epoch    247      batch 3\n",
      "RLoss: 11.813287734985352\n",
      "current in epoch    247      batch 4\n",
      "RLoss: 491.5085754394531\n",
      "current in epoch    247      batch 5\n",
      "RLoss: 29.05626106262207\n",
      "========================================\n",
      "Epoch 248/1000 - partial_train_loss: 155.8077 \n",
      "Epoch: [248/1000], TrainLoss: 25.570638724735804\n",
      "training Loss has not improved for 243 epochs.\n",
      "current in epoch    248      batch 0\n",
      "RLoss: 87.11051940917969\n",
      "current in epoch    248      batch 1\n",
      "RLoss: 36.288475036621094\n",
      "current in epoch    248      batch 2\n",
      "RLoss: 20.28459930419922\n",
      "current in epoch    248      batch 3\n",
      "RLoss: 4.535639762878418\n",
      "current in epoch    248      batch 4\n",
      "RLoss: 33.96897888183594\n",
      "current in epoch    248      batch 5\n",
      "RLoss: 73.8687515258789\n",
      "========================================\n",
      "Epoch 249/1000 - partial_train_loss: 45.8339 \n",
      "Epoch: [249/1000], TrainLoss: 100.0077976499285\n",
      "training Loss has not improved for 244 epochs.\n",
      "current in epoch    249      batch 0\n",
      "RLoss: 79.60009765625\n",
      "current in epoch    249      batch 1\n",
      "RLoss: 101.52440643310547\n",
      "current in epoch    249      batch 2\n",
      "RLoss: 46.41506576538086\n",
      "current in epoch    249      batch 3\n",
      "RLoss: 44.8516731262207\n",
      "current in epoch    249      batch 4\n",
      "RLoss: 58.101280212402344\n",
      "current in epoch    249      batch 5\n",
      "RLoss: 29.914703369140625\n",
      "========================================\n",
      "Epoch 250/1000 - partial_train_loss: 58.4806 \n",
      "Epoch: [250/1000], TrainLoss: 51.532372747148784\n",
      "training Loss has not improved for 245 epochs.\n",
      "current in epoch    250      batch 0\n",
      "RLoss: 268.2452392578125\n",
      "current in epoch    250      batch 1\n",
      "RLoss: 1034.4215087890625\n",
      "current in epoch    250      batch 2\n",
      "RLoss: 340.44146728515625\n",
      "current in epoch    250      batch 3\n",
      "RLoss: 1633.9510498046875\n",
      "current in epoch    250      batch 4\n",
      "RLoss: 18.195098876953125\n",
      "current in epoch    250      batch 5\n",
      "RLoss: 323.5438232421875\n",
      "========================================\n",
      "Epoch 251/1000 - partial_train_loss: 496.3401 \n",
      "sorting training set\n",
      "Epoch 251/1000 - Training loss: 261.6403 \n",
      "========================================\n",
      "Epoch: [251/1000], TrainLoss: 276.91128103594593\n",
      "training Loss has not improved for 246 epochs.\n",
      "current in epoch    251      batch 0\n",
      "RLoss: 1496.6102294921875\n",
      "current in epoch    251      batch 1\n",
      "RLoss: 116.72785186767578\n",
      "current in epoch    251      batch 2\n",
      "RLoss: 1595.644775390625\n",
      "current in epoch    251      batch 3\n",
      "RLoss: 226.75413513183594\n",
      "current in epoch    251      batch 4\n",
      "RLoss: 172.08798217773438\n",
      "current in epoch    251      batch 5\n",
      "RLoss: 95.6986312866211\n",
      "========================================\n",
      "Epoch 252/1000 - partial_train_loss: 634.1568 \n",
      "Epoch: [252/1000], TrainLoss: 76.00450502123151\n",
      "training Loss has not improved for 247 epochs.\n",
      "current in epoch    252      batch 0\n",
      "RLoss: 16.558286666870117\n",
      "current in epoch    252      batch 1\n",
      "RLoss: 212.49789428710938\n",
      "current in epoch    252      batch 2\n",
      "RLoss: 620.9259033203125\n",
      "current in epoch    252      batch 3\n",
      "RLoss: 32.55939865112305\n",
      "current in epoch    252      batch 4\n",
      "RLoss: 89.0875473022461\n",
      "current in epoch    252      batch 5\n",
      "RLoss: 88.91606140136719\n",
      "========================================\n",
      "Epoch 253/1000 - partial_train_loss: 148.1348 \n",
      "Epoch: [253/1000], TrainLoss: 61.43096474238804\n",
      "training Loss has not improved for 248 epochs.\n",
      "current in epoch    253      batch 0\n",
      "RLoss: 81.81244659423828\n",
      "current in epoch    253      batch 1\n",
      "RLoss: 159.45643615722656\n",
      "current in epoch    253      batch 2\n",
      "RLoss: 24.426565170288086\n",
      "current in epoch    253      batch 3\n",
      "RLoss: 100.42015838623047\n",
      "current in epoch    253      batch 4\n",
      "RLoss: 4.779127597808838\n",
      "current in epoch    253      batch 5\n",
      "RLoss: 15.272466659545898\n",
      "========================================\n",
      "Epoch 254/1000 - partial_train_loss: 69.8950 \n",
      "Epoch: [254/1000], TrainLoss: 24.78133259500776\n",
      "training Loss has not improved for 249 epochs.\n",
      "current in epoch    254      batch 0\n",
      "RLoss: 215.4754180908203\n",
      "current in epoch    254      batch 1\n",
      "RLoss: 337.3554992675781\n",
      "current in epoch    254      batch 2\n",
      "RLoss: 16.650562286376953\n",
      "current in epoch    254      batch 3\n",
      "RLoss: 88.79713439941406\n",
      "current in epoch    254      batch 4\n",
      "RLoss: 469.9621276855469\n",
      "current in epoch    254      batch 5\n",
      "RLoss: 40.754764556884766\n",
      "========================================\n",
      "Epoch 255/1000 - partial_train_loss: 195.0531 \n",
      "Epoch: [255/1000], TrainLoss: 28.296269757407053\n",
      "training Loss has not improved for 250 epochs.\n",
      "current in epoch    255      batch 0\n",
      "RLoss: 173.98223876953125\n",
      "current in epoch    255      batch 1\n",
      "RLoss: 60.75688934326172\n",
      "current in epoch    255      batch 2\n",
      "RLoss: 162.12989807128906\n",
      "current in epoch    255      batch 3\n",
      "RLoss: 352.697998046875\n",
      "current in epoch    255      batch 4\n",
      "RLoss: 2421.87744140625\n",
      "current in epoch    255      batch 5\n",
      "RLoss: 243.03758239746094\n",
      "========================================\n",
      "Epoch 256/1000 - partial_train_loss: 496.3736 \n",
      "sorting training set\n",
      "Epoch 256/1000 - Training loss: 252.0535 \n",
      "========================================\n",
      "Epoch: [256/1000], TrainLoss: 266.1398123079718\n",
      "training Loss has not improved for 251 epochs.\n",
      "current in epoch    256      batch 0\n",
      "RLoss: 328.7008056640625\n",
      "current in epoch    256      batch 1\n",
      "RLoss: 126.3333740234375\n",
      "current in epoch    256      batch 2\n",
      "RLoss: 202.34173583984375\n",
      "current in epoch    256      batch 3\n",
      "RLoss: 151.83975219726562\n",
      "current in epoch    256      batch 4\n",
      "RLoss: 193.32742309570312\n",
      "current in epoch    256      batch 5\n",
      "RLoss: 400.45086669921875\n",
      "========================================\n",
      "Epoch 257/1000 - partial_train_loss: 306.4560 \n",
      "Epoch: [257/1000], TrainLoss: 556.3023267473493\n",
      "training Loss has not improved for 252 epochs.\n",
      "current in epoch    257      batch 0\n",
      "RLoss: 86.5662841796875\n",
      "current in epoch    257      batch 1\n",
      "RLoss: 18.893354415893555\n",
      "current in epoch    257      batch 2\n",
      "RLoss: 3450.152587890625\n",
      "current in epoch    257      batch 3\n",
      "RLoss: 517.607177734375\n",
      "current in epoch    257      batch 4\n",
      "RLoss: 569.3043212890625\n",
      "current in epoch    257      batch 5\n",
      "RLoss: 49.405059814453125\n",
      "========================================\n",
      "Epoch 258/1000 - partial_train_loss: 784.9508 \n",
      "Epoch: [258/1000], TrainLoss: 251.15798255375452\n",
      "training Loss has not improved for 253 epochs.\n",
      "current in epoch    258      batch 0\n",
      "RLoss: 2466.45361328125\n",
      "current in epoch    258      batch 1\n",
      "RLoss: 565.0990600585938\n",
      "current in epoch    258      batch 2\n",
      "RLoss: 227.62066650390625\n",
      "current in epoch    258      batch 3\n",
      "RLoss: 59.48210525512695\n",
      "current in epoch    258      batch 4\n",
      "RLoss: 392.43841552734375\n",
      "current in epoch    258      batch 5\n",
      "RLoss: 247.5103759765625\n",
      "========================================\n",
      "Epoch 259/1000 - partial_train_loss: 547.8833 \n",
      "Epoch: [259/1000], TrainLoss: 281.5257590157645\n",
      "training Loss has not improved for 254 epochs.\n",
      "current in epoch    259      batch 0\n",
      "RLoss: 456.0872497558594\n",
      "current in epoch    259      batch 1\n",
      "RLoss: 595.4349975585938\n",
      "current in epoch    259      batch 2\n",
      "RLoss: 245.22109985351562\n",
      "current in epoch    259      batch 3\n",
      "RLoss: 63.018348693847656\n",
      "current in epoch    259      batch 4\n",
      "RLoss: 18.419952392578125\n",
      "current in epoch    259      batch 5\n",
      "RLoss: 162.56967163085938\n",
      "========================================\n",
      "Epoch 260/1000 - partial_train_loss: 225.5084 \n",
      "Epoch: [260/1000], TrainLoss: 147.93173027038574\n",
      "training Loss has not improved for 255 epochs.\n",
      "current in epoch    260      batch 0\n",
      "RLoss: 341.0519714355469\n",
      "current in epoch    260      batch 1\n",
      "RLoss: 159.1763153076172\n",
      "current in epoch    260      batch 2\n",
      "RLoss: 39.139373779296875\n",
      "current in epoch    260      batch 3\n",
      "RLoss: 97.61702728271484\n",
      "current in epoch    260      batch 4\n",
      "RLoss: 285.7590637207031\n",
      "current in epoch    260      batch 5\n",
      "RLoss: 447.2604064941406\n",
      "========================================\n",
      "Epoch 261/1000 - partial_train_loss: 174.5256 \n",
      "sorting training set\n",
      "Epoch 261/1000 - Training loss: 467.6780 \n",
      "========================================\n",
      "Epoch: [261/1000], TrainLoss: 497.74119639245345\n",
      "training Loss has not improved for 256 epochs.\n",
      "current in epoch    261      batch 0\n",
      "RLoss: 80.09130096435547\n",
      "current in epoch    261      batch 1\n",
      "RLoss: 552.6831665039062\n",
      "current in epoch    261      batch 2\n",
      "RLoss: 148.7185821533203\n",
      "current in epoch    261      batch 3\n",
      "RLoss: 887.7847900390625\n",
      "current in epoch    261      batch 4\n",
      "RLoss: 250.47288513183594\n",
      "current in epoch    261      batch 5\n",
      "RLoss: 608.8324584960938\n",
      "========================================\n",
      "Epoch 262/1000 - partial_train_loss: 512.3858 \n",
      "Epoch: [262/1000], TrainLoss: 718.8629586356027\n",
      "training Loss has not improved for 257 epochs.\n",
      "current in epoch    262      batch 0\n",
      "RLoss: 1328.401611328125\n",
      "current in epoch    262      batch 1\n",
      "RLoss: 398.34014892578125\n",
      "current in epoch    262      batch 2\n",
      "RLoss: 262.230712890625\n",
      "current in epoch    262      batch 3\n",
      "RLoss: 143.2360076904297\n",
      "current in epoch    262      batch 4\n",
      "RLoss: 66.45667266845703\n",
      "current in epoch    262      batch 5\n",
      "RLoss: 353.87225341796875\n",
      "========================================\n",
      "Epoch 263/1000 - partial_train_loss: 512.8767 \n",
      "Epoch: [263/1000], TrainLoss: 401.36941882542203\n",
      "training Loss has not improved for 258 epochs.\n",
      "current in epoch    263      batch 0\n",
      "RLoss: 99.62651824951172\n",
      "current in epoch    263      batch 1\n",
      "RLoss: 475.4064025878906\n",
      "current in epoch    263      batch 2\n",
      "RLoss: 71.55696868896484\n",
      "current in epoch    263      batch 3\n",
      "RLoss: 337.2412109375\n",
      "current in epoch    263      batch 4\n",
      "RLoss: 85.87029266357422\n",
      "current in epoch    263      batch 5\n",
      "RLoss: 80.75642395019531\n",
      "========================================\n",
      "Epoch 264/1000 - partial_train_loss: 189.0908 \n",
      "Epoch: [264/1000], TrainLoss: 198.37773295811243\n",
      "training Loss has not improved for 259 epochs.\n",
      "current in epoch    264      batch 0\n",
      "RLoss: 199.3179931640625\n",
      "current in epoch    264      batch 1\n",
      "RLoss: 132.60415649414062\n",
      "current in epoch    264      batch 2\n",
      "RLoss: 197.11529541015625\n",
      "current in epoch    264      batch 3\n",
      "RLoss: 502.17218017578125\n",
      "current in epoch    264      batch 4\n",
      "RLoss: 1164.0064697265625\n",
      "current in epoch    264      batch 5\n",
      "RLoss: 880.3782348632812\n",
      "========================================\n",
      "Epoch 265/1000 - partial_train_loss: 331.2664 \n",
      "Epoch: [265/1000], TrainLoss: 959.9023328508649\n",
      "training Loss has not improved for 260 epochs.\n",
      "current in epoch    265      batch 0\n",
      "RLoss: 624.796630859375\n",
      "current in epoch    265      batch 1\n",
      "RLoss: 469.5249328613281\n",
      "current in epoch    265      batch 2\n",
      "RLoss: 169.8410186767578\n",
      "current in epoch    265      batch 3\n",
      "RLoss: 511.99700927734375\n",
      "current in epoch    265      batch 4\n",
      "RLoss: 99.80035400390625\n",
      "current in epoch    265      batch 5\n",
      "RLoss: 420.4012451171875\n",
      "========================================\n",
      "Epoch 266/1000 - partial_train_loss: 456.2876 \n",
      "sorting training set\n",
      "Epoch 266/1000 - Training loss: 436.4989 \n",
      "========================================\n",
      "Epoch: [266/1000], TrainLoss: 463.712789808222\n",
      "training Loss has not improved for 261 epochs.\n",
      "current in epoch    266      batch 0\n",
      "RLoss: 526.086181640625\n",
      "current in epoch    266      batch 1\n",
      "RLoss: 1040.100830078125\n",
      "current in epoch    266      batch 2\n",
      "RLoss: 60.61110305786133\n",
      "current in epoch    266      batch 3\n",
      "RLoss: 39.42523193359375\n",
      "current in epoch    266      batch 4\n",
      "RLoss: 50.489864349365234\n",
      "current in epoch    266      batch 5\n",
      "RLoss: 57.644691467285156\n",
      "========================================\n",
      "Epoch 267/1000 - partial_train_loss: 492.5850 \n",
      "Epoch: [267/1000], TrainLoss: 38.03766598020281\n",
      "training Loss has not improved for 262 epochs.\n",
      "current in epoch    267      batch 0\n",
      "RLoss: 465.48248291015625\n",
      "current in epoch    267      batch 1\n",
      "RLoss: 612.1243286132812\n",
      "current in epoch    267      batch 2\n",
      "RLoss: 367.2126159667969\n",
      "current in epoch    267      batch 3\n",
      "RLoss: 1125.0189208984375\n",
      "current in epoch    267      batch 4\n",
      "RLoss: 20.409337997436523\n",
      "current in epoch    267      batch 5\n",
      "RLoss: 41.8821907043457\n",
      "========================================\n",
      "Epoch 268/1000 - partial_train_loss: 382.2094 \n",
      "Epoch: [268/1000], TrainLoss: 102.4281554222107\n",
      "training Loss has not improved for 263 epochs.\n",
      "current in epoch    268      batch 0\n",
      "RLoss: 1878.574951171875\n",
      "current in epoch    268      batch 1\n",
      "RLoss: 415.5686950683594\n",
      "current in epoch    268      batch 2\n",
      "RLoss: 586.647216796875\n",
      "current in epoch    268      batch 3\n",
      "RLoss: 621.9156494140625\n",
      "current in epoch    268      batch 4\n",
      "RLoss: 1148.15771484375\n",
      "current in epoch    268      batch 5\n",
      "RLoss: 58.41208267211914\n",
      "========================================\n",
      "Epoch 269/1000 - partial_train_loss: 695.1019 \n",
      "Epoch: [269/1000], TrainLoss: 56.05415194375174\n",
      "training Loss has not improved for 264 epochs.\n",
      "current in epoch    269      batch 0\n",
      "RLoss: 77.58802032470703\n",
      "current in epoch    269      batch 1\n",
      "RLoss: 178.19073486328125\n",
      "current in epoch    269      batch 2\n",
      "RLoss: 85.67868041992188\n",
      "current in epoch    269      batch 3\n",
      "RLoss: 14.926487922668457\n",
      "current in epoch    269      batch 4\n",
      "RLoss: 130.65231323242188\n",
      "current in epoch    269      batch 5\n",
      "RLoss: 360.4702453613281\n",
      "========================================\n",
      "Epoch 270/1000 - partial_train_loss: 90.9579 \n",
      "Epoch: [270/1000], TrainLoss: 246.12861388070243\n",
      "training Loss has not improved for 265 epochs.\n",
      "current in epoch    270      batch 0\n",
      "RLoss: 56.991180419921875\n",
      "current in epoch    270      batch 1\n",
      "RLoss: 215.19337463378906\n",
      "current in epoch    270      batch 2\n",
      "RLoss: 755.063232421875\n",
      "current in epoch    270      batch 3\n",
      "RLoss: 326.2276611328125\n",
      "current in epoch    270      batch 4\n",
      "RLoss: 2036.539794921875\n",
      "current in epoch    270      batch 5\n",
      "RLoss: 88.79106140136719\n",
      "========================================\n",
      "Epoch 271/1000 - partial_train_loss: 673.0558 \n",
      "sorting training set\n",
      "Epoch 271/1000 - Training loss: 213.0213 \n",
      "========================================\n",
      "Epoch: [271/1000], TrainLoss: 227.50945101679434\n",
      "training Loss has not improved for 266 epochs.\n",
      "current in epoch    271      batch 0\n",
      "RLoss: 2587.86572265625\n",
      "current in epoch    271      batch 1\n",
      "RLoss: 572.3372802734375\n",
      "current in epoch    271      batch 2\n",
      "RLoss: 165.0721435546875\n",
      "current in epoch    271      batch 3\n",
      "RLoss: 3634.297119140625\n",
      "current in epoch    271      batch 4\n",
      "RLoss: 38.32102584838867\n",
      "current in epoch    271      batch 5\n",
      "RLoss: 206.99085998535156\n",
      "========================================\n",
      "Epoch 272/1000 - partial_train_loss: 1158.3144 \n",
      "Epoch: [272/1000], TrainLoss: 349.6960007803781\n",
      "training Loss has not improved for 267 epochs.\n",
      "current in epoch    272      batch 0\n",
      "RLoss: 147.1783447265625\n",
      "current in epoch    272      batch 1\n",
      "RLoss: 236.732177734375\n",
      "current in epoch    272      batch 2\n",
      "RLoss: 97.77632904052734\n",
      "current in epoch    272      batch 3\n",
      "RLoss: 191.57435607910156\n",
      "current in epoch    272      batch 4\n",
      "RLoss: 17.62568473815918\n",
      "current in epoch    272      batch 5\n",
      "RLoss: 407.27545166015625\n",
      "========================================\n",
      "Epoch 273/1000 - partial_train_loss: 157.7334 \n",
      "Epoch: [273/1000], TrainLoss: 1821.4378280639648\n",
      "training Loss has not improved for 268 epochs.\n",
      "current in epoch    273      batch 0\n",
      "RLoss: 1578.8341064453125\n",
      "current in epoch    273      batch 1\n",
      "RLoss: 28.228483200073242\n",
      "current in epoch    273      batch 2\n",
      "RLoss: 73.1336898803711\n",
      "current in epoch    273      batch 3\n",
      "RLoss: 193.36878967285156\n",
      "current in epoch    273      batch 4\n",
      "RLoss: 153.83444213867188\n",
      "current in epoch    273      batch 5\n",
      "RLoss: 20.138614654541016\n",
      "========================================\n",
      "Epoch 274/1000 - partial_train_loss: 332.8791 \n",
      "Epoch: [274/1000], TrainLoss: 86.82156447001866\n",
      "training Loss has not improved for 269 epochs.\n",
      "current in epoch    274      batch 0\n",
      "RLoss: 270.2374267578125\n",
      "current in epoch    274      batch 1\n",
      "RLoss: 80.45845794677734\n",
      "current in epoch    274      batch 2\n",
      "RLoss: 49.69581985473633\n",
      "current in epoch    274      batch 3\n",
      "RLoss: 40.23902893066406\n",
      "current in epoch    274      batch 4\n",
      "RLoss: 89.28163146972656\n",
      "current in epoch    274      batch 5\n",
      "RLoss: 39.1983528137207\n",
      "========================================\n",
      "Epoch 275/1000 - partial_train_loss: 78.2090 \n",
      "Epoch: [275/1000], TrainLoss: 116.86049393245152\n",
      "training Loss has not improved for 270 epochs.\n",
      "current in epoch    275      batch 0\n",
      "RLoss: 250.96392822265625\n",
      "current in epoch    275      batch 1\n",
      "RLoss: 430.7374572753906\n",
      "current in epoch    275      batch 2\n",
      "RLoss: 433.90234375\n",
      "current in epoch    275      batch 3\n",
      "RLoss: 31.047828674316406\n",
      "current in epoch    275      batch 4\n",
      "RLoss: 34.744266510009766\n",
      "current in epoch    275      batch 5\n",
      "RLoss: 228.83616638183594\n",
      "========================================\n",
      "Epoch 276/1000 - partial_train_loss: 177.9918 \n",
      "sorting training set\n",
      "Epoch 276/1000 - Training loss: 151.8336 \n",
      "========================================\n",
      "Epoch: [276/1000], TrainLoss: 162.17321006851836\n",
      "training Loss has not improved for 271 epochs.\n",
      "current in epoch    276      batch 0\n",
      "RLoss: 64.83346557617188\n",
      "current in epoch    276      batch 1\n",
      "RLoss: 162.98268127441406\n",
      "current in epoch    276      batch 2\n",
      "RLoss: 16.730018615722656\n",
      "current in epoch    276      batch 3\n",
      "RLoss: 53.422508239746094\n",
      "current in epoch    276      batch 4\n",
      "RLoss: 59.84185791015625\n",
      "current in epoch    276      batch 5\n",
      "RLoss: 1.56747567653656\n",
      "========================================\n",
      "Epoch 277/1000 - partial_train_loss: 210.1617 \n",
      "Epoch: [277/1000], TrainLoss: 1.6058245024510793\n",
      "current in epoch    277      batch 0\n",
      "RLoss: 462.39056396484375\n",
      "current in epoch    277      batch 1\n",
      "RLoss: 59.938228607177734\n",
      "current in epoch    277      batch 2\n",
      "RLoss: 211.2884063720703\n",
      "current in epoch    277      batch 3\n",
      "RLoss: 119.76716613769531\n",
      "current in epoch    277      batch 4\n",
      "RLoss: 272.518310546875\n",
      "current in epoch    277      batch 5\n",
      "RLoss: 145.70314025878906\n",
      "========================================\n",
      "Epoch 278/1000 - partial_train_loss: 147.5013 \n",
      "Epoch: [278/1000], TrainLoss: 91.88502284458706\n",
      "training Loss has not improved for 1 epochs.\n",
      "current in epoch    278      batch 0\n",
      "RLoss: 202.15614318847656\n",
      "current in epoch    278      batch 1\n",
      "RLoss: 170.80636596679688\n",
      "current in epoch    278      batch 2\n",
      "RLoss: 735.2982177734375\n",
      "current in epoch    278      batch 3\n",
      "RLoss: 1575.9735107421875\n",
      "current in epoch    278      batch 4\n",
      "RLoss: 1616.6375732421875\n",
      "current in epoch    278      batch 5\n",
      "RLoss: 115.41117095947266\n",
      "========================================\n",
      "Epoch 279/1000 - partial_train_loss: 712.6718 \n",
      "Epoch: [279/1000], TrainLoss: 90.25316878727504\n",
      "training Loss has not improved for 2 epochs.\n",
      "current in epoch    279      batch 0\n",
      "RLoss: 145.44017028808594\n",
      "current in epoch    279      batch 1\n",
      "RLoss: 97.90318298339844\n",
      "current in epoch    279      batch 2\n",
      "RLoss: 41.71532440185547\n",
      "current in epoch    279      batch 3\n",
      "RLoss: 59.81764221191406\n",
      "current in epoch    279      batch 4\n",
      "RLoss: 114.29122161865234\n",
      "current in epoch    279      batch 5\n",
      "RLoss: 640.7106323242188\n",
      "========================================\n",
      "Epoch 280/1000 - partial_train_loss: 109.3536 \n",
      "Epoch: [280/1000], TrainLoss: 503.442747933524\n",
      "training Loss has not improved for 3 epochs.\n",
      "current in epoch    280      batch 0\n",
      "RLoss: 179.04042053222656\n",
      "current in epoch    280      batch 1\n",
      "RLoss: 19.920650482177734\n",
      "current in epoch    280      batch 2\n",
      "RLoss: 537.92919921875\n",
      "current in epoch    280      batch 3\n",
      "RLoss: 13.311434745788574\n",
      "current in epoch    280      batch 4\n",
      "RLoss: 58.76565933227539\n",
      "current in epoch    280      batch 5\n",
      "RLoss: 12.956567764282227\n",
      "========================================\n",
      "Epoch 281/1000 - partial_train_loss: 294.1147 \n",
      "sorting training set\n",
      "Epoch 281/1000 - Training loss: 13.0170 \n",
      "========================================\n",
      "Epoch: [281/1000], TrainLoss: 13.43059628205127\n",
      "training Loss has not improved for 4 epochs.\n",
      "current in epoch    281      batch 0\n",
      "RLoss: 135.27305603027344\n",
      "current in epoch    281      batch 1\n",
      "RLoss: 64.2558364868164\n",
      "current in epoch    281      batch 2\n",
      "RLoss: 7.2891845703125\n",
      "current in epoch    281      batch 3\n",
      "RLoss: 127.96736907958984\n",
      "current in epoch    281      batch 4\n",
      "RLoss: 70.8646240234375\n",
      "current in epoch    281      batch 5\n",
      "RLoss: 117.60539245605469\n",
      "========================================\n",
      "Epoch 282/1000 - partial_train_loss: 77.2348 \n",
      "Epoch: [282/1000], TrainLoss: 159.5789966583252\n",
      "training Loss has not improved for 5 epochs.\n",
      "current in epoch    282      batch 0\n",
      "RLoss: 166.209716796875\n",
      "current in epoch    282      batch 1\n",
      "RLoss: 220.61392211914062\n",
      "current in epoch    282      batch 2\n",
      "RLoss: 133.5012969970703\n",
      "current in epoch    282      batch 3\n",
      "RLoss: 111.35177612304688\n",
      "current in epoch    282      batch 4\n",
      "RLoss: 20.453283309936523\n",
      "current in epoch    282      batch 5\n",
      "RLoss: 7.805166721343994\n",
      "========================================\n",
      "Epoch 283/1000 - partial_train_loss: 105.4466 \n",
      "Epoch: [283/1000], TrainLoss: 16.27293120111738\n",
      "training Loss has not improved for 6 epochs.\n",
      "current in epoch    283      batch 0\n",
      "RLoss: 207.15054321289062\n",
      "current in epoch    283      batch 1\n",
      "RLoss: 480.76959228515625\n",
      "current in epoch    283      batch 2\n",
      "RLoss: 106.02542114257812\n",
      "current in epoch    283      batch 3\n",
      "RLoss: 21.02053451538086\n",
      "current in epoch    283      batch 4\n",
      "RLoss: 79.23129272460938\n",
      "current in epoch    283      batch 5\n",
      "RLoss: 58.181190490722656\n",
      "========================================\n",
      "Epoch 284/1000 - partial_train_loss: 124.5975 \n",
      "Epoch: [284/1000], TrainLoss: 160.3405692236764\n",
      "training Loss has not improved for 7 epochs.\n",
      "current in epoch    284      batch 0\n",
      "RLoss: 46.97291946411133\n",
      "current in epoch    284      batch 1\n",
      "RLoss: 127.72834777832031\n",
      "current in epoch    284      batch 2\n",
      "RLoss: 288.2623596191406\n",
      "current in epoch    284      batch 3\n",
      "RLoss: 17.798503875732422\n",
      "current in epoch    284      batch 4\n",
      "RLoss: 215.25653076171875\n",
      "current in epoch    284      batch 5\n",
      "RLoss: 100.49604034423828\n",
      "========================================\n",
      "Epoch 285/1000 - partial_train_loss: 135.7254 \n",
      "Epoch: [285/1000], TrainLoss: 94.95351437159947\n",
      "training Loss has not improved for 8 epochs.\n",
      "current in epoch    285      batch 0\n",
      "RLoss: 248.8292694091797\n",
      "current in epoch    285      batch 1\n",
      "RLoss: 15.006887435913086\n",
      "current in epoch    285      batch 2\n",
      "RLoss: 7.712979793548584\n",
      "current in epoch    285      batch 3\n",
      "RLoss: 31.077768325805664\n",
      "current in epoch    285      batch 4\n",
      "RLoss: 45.11204528808594\n",
      "current in epoch    285      batch 5\n",
      "RLoss: 16.500795364379883\n",
      "========================================\n",
      "Epoch 286/1000 - partial_train_loss: 82.4554 \n",
      "sorting training set\n",
      "Epoch 286/1000 - Training loss: 35.2065 \n",
      "========================================\n",
      "Epoch: [286/1000], TrainLoss: 37.818938168158205\n",
      "training Loss has not improved for 9 epochs.\n",
      "current in epoch    286      batch 0\n",
      "RLoss: 817.4561157226562\n",
      "current in epoch    286      batch 1\n",
      "RLoss: 825.4227905273438\n",
      "current in epoch    286      batch 2\n",
      "RLoss: 196.7018585205078\n",
      "current in epoch    286      batch 3\n",
      "RLoss: 35.55925369262695\n",
      "current in epoch    286      batch 4\n",
      "RLoss: 117.34352111816406\n",
      "current in epoch    286      batch 5\n",
      "RLoss: 212.5380859375\n",
      "========================================\n",
      "Epoch 287/1000 - partial_train_loss: 360.3949 \n",
      "Epoch: [287/1000], TrainLoss: 175.59443501063757\n",
      "training Loss has not improved for 10 epochs.\n",
      "current in epoch    287      batch 0\n",
      "RLoss: 4.8965325355529785\n",
      "current in epoch    287      batch 1\n",
      "RLoss: 250.59503173828125\n",
      "current in epoch    287      batch 2\n",
      "RLoss: 18.327322006225586\n",
      "current in epoch    287      batch 3\n",
      "RLoss: 50.08970642089844\n",
      "current in epoch    287      batch 4\n",
      "RLoss: 44.8087158203125\n",
      "current in epoch    287      batch 5\n",
      "RLoss: 101.07793426513672\n",
      "========================================\n",
      "Epoch 288/1000 - partial_train_loss: 118.1989 \n",
      "Epoch: [288/1000], TrainLoss: 92.09940474373954\n",
      "training Loss has not improved for 11 epochs.\n",
      "current in epoch    288      batch 0\n",
      "RLoss: 297.2292175292969\n",
      "current in epoch    288      batch 1\n",
      "RLoss: 77.28459167480469\n",
      "current in epoch    288      batch 2\n",
      "RLoss: 1060.5999755859375\n",
      "current in epoch    288      batch 3\n",
      "RLoss: 21.24712562561035\n",
      "current in epoch    288      batch 4\n",
      "RLoss: 581.6083374023438\n",
      "current in epoch    288      batch 5\n",
      "RLoss: 356.1957092285156\n",
      "========================================\n",
      "Epoch 289/1000 - partial_train_loss: 374.1555 \n",
      "Epoch: [289/1000], TrainLoss: 347.7100067138672\n",
      "training Loss has not improved for 12 epochs.\n",
      "current in epoch    289      batch 0\n",
      "RLoss: 91.4975357055664\n",
      "current in epoch    289      batch 1\n",
      "RLoss: 808.306884765625\n",
      "current in epoch    289      batch 2\n",
      "RLoss: 86.44953155517578\n",
      "current in epoch    289      batch 3\n",
      "RLoss: 220.6178741455078\n",
      "current in epoch    289      batch 4\n",
      "RLoss: 15.117959022521973\n",
      "current in epoch    289      batch 5\n",
      "RLoss: 141.75177001953125\n",
      "========================================\n",
      "Epoch 290/1000 - partial_train_loss: 279.6141 \n",
      "Epoch: [290/1000], TrainLoss: 111.91500418526786\n",
      "training Loss has not improved for 13 epochs.\n",
      "current in epoch    290      batch 0\n",
      "RLoss: 781.8588256835938\n",
      "current in epoch    290      batch 1\n",
      "RLoss: 595.6974487304688\n",
      "current in epoch    290      batch 2\n",
      "RLoss: 241.70413208007812\n",
      "current in epoch    290      batch 3\n",
      "RLoss: 74.80851745605469\n",
      "current in epoch    290      batch 4\n",
      "RLoss: 46.269962310791016\n",
      "current in epoch    290      batch 5\n",
      "RLoss: 207.55859375\n",
      "========================================\n",
      "Epoch 291/1000 - partial_train_loss: 292.8284 \n",
      "sorting training set\n",
      "Epoch 291/1000 - Training loss: 197.3730 \n",
      "========================================\n",
      "Epoch: [291/1000], TrainLoss: 210.0665978380718\n",
      "training Loss has not improved for 14 epochs.\n",
      "current in epoch    291      batch 0\n",
      "RLoss: 1104.3409423828125\n",
      "current in epoch    291      batch 1\n",
      "RLoss: 73.56494903564453\n",
      "current in epoch    291      batch 2\n",
      "RLoss: 351.22332763671875\n",
      "current in epoch    291      batch 3\n",
      "RLoss: 7.509844779968262\n",
      "current in epoch    291      batch 4\n",
      "RLoss: 8.162528991699219\n",
      "current in epoch    291      batch 5\n",
      "RLoss: 198.66500854492188\n",
      "========================================\n",
      "Epoch 292/1000 - partial_train_loss: 316.2577 \n",
      "Epoch: [292/1000], TrainLoss: 133.05692802156722\n",
      "training Loss has not improved for 15 epochs.\n",
      "current in epoch    292      batch 0\n",
      "RLoss: 30.090370178222656\n",
      "current in epoch    292      batch 1\n",
      "RLoss: 40.05709457397461\n",
      "current in epoch    292      batch 2\n",
      "RLoss: 124.07563018798828\n",
      "current in epoch    292      batch 3\n",
      "RLoss: 31.2266845703125\n",
      "current in epoch    292      batch 4\n",
      "RLoss: 69.30855560302734\n",
      "current in epoch    292      batch 5\n",
      "RLoss: 119.3305892944336\n",
      "========================================\n",
      "Epoch 293/1000 - partial_train_loss: 112.7020 \n",
      "Epoch: [293/1000], TrainLoss: 116.55867644718715\n",
      "training Loss has not improved for 16 epochs.\n",
      "current in epoch    293      batch 0\n",
      "RLoss: 419.1996765136719\n",
      "current in epoch    293      batch 1\n",
      "RLoss: 938.6988525390625\n",
      "current in epoch    293      batch 2\n",
      "RLoss: 297.4534606933594\n",
      "current in epoch    293      batch 3\n",
      "RLoss: 327.7749938964844\n",
      "current in epoch    293      batch 4\n",
      "RLoss: 254.41696166992188\n",
      "current in epoch    293      batch 5\n",
      "RLoss: 133.34605407714844\n",
      "========================================\n",
      "Epoch 294/1000 - partial_train_loss: 368.6842 \n",
      "Epoch: [294/1000], TrainLoss: 82.1112220287323\n",
      "training Loss has not improved for 17 epochs.\n",
      "current in epoch    294      batch 0\n",
      "RLoss: 722.2210693359375\n",
      "current in epoch    294      batch 1\n",
      "RLoss: 210.3642578125\n",
      "current in epoch    294      batch 2\n",
      "RLoss: 217.0719451904297\n",
      "current in epoch    294      batch 3\n",
      "RLoss: 650.9906616210938\n",
      "current in epoch    294      batch 4\n",
      "RLoss: 817.1521606445312\n",
      "current in epoch    294      batch 5\n",
      "RLoss: 515.015869140625\n",
      "========================================\n",
      "Epoch 295/1000 - partial_train_loss: 437.3526 \n",
      "Epoch: [295/1000], TrainLoss: 437.58451264245167\n",
      "training Loss has not improved for 18 epochs.\n",
      "current in epoch    295      batch 0\n",
      "RLoss: 32.296566009521484\n",
      "current in epoch    295      batch 1\n",
      "RLoss: 34.84629821777344\n",
      "current in epoch    295      batch 2\n",
      "RLoss: 36.7502555847168\n",
      "current in epoch    295      batch 3\n",
      "RLoss: 187.3556671142578\n",
      "current in epoch    295      batch 4\n",
      "RLoss: 659.8970336914062\n",
      "current in epoch    295      batch 5\n",
      "RLoss: 40.475730895996094\n",
      "========================================\n",
      "Epoch 296/1000 - partial_train_loss: 581.2257 \n",
      "sorting training set\n",
      "Epoch 296/1000 - Training loss: 33.4283 \n",
      "========================================\n",
      "Epoch: [296/1000], TrainLoss: 35.81371132687262\n",
      "training Loss has not improved for 19 epochs.\n",
      "current in epoch    296      batch 0\n",
      "RLoss: 634.3050537109375\n",
      "current in epoch    296      batch 1\n",
      "RLoss: 23.94415283203125\n",
      "current in epoch    296      batch 2\n",
      "RLoss: 17.48536491394043\n",
      "current in epoch    296      batch 3\n",
      "RLoss: 41.51355743408203\n",
      "current in epoch    296      batch 4\n",
      "RLoss: 398.39691162109375\n",
      "current in epoch    296      batch 5\n",
      "RLoss: 31.704227447509766\n",
      "========================================\n",
      "Epoch 297/1000 - partial_train_loss: 176.5808 \n",
      "Epoch: [297/1000], TrainLoss: 33.17199100766863\n",
      "training Loss has not improved for 20 epochs.\n",
      "current in epoch    297      batch 0\n",
      "RLoss: 1580.6263427734375\n",
      "current in epoch    297      batch 1\n",
      "RLoss: 22.91453742980957\n",
      "current in epoch    297      batch 2\n",
      "RLoss: 6.612605094909668\n",
      "current in epoch    297      batch 3\n",
      "RLoss: 79.19099426269531\n",
      "current in epoch    297      batch 4\n",
      "RLoss: 75.70219421386719\n",
      "current in epoch    297      batch 5\n",
      "RLoss: 2.8827292919158936\n",
      "========================================\n",
      "Epoch 298/1000 - partial_train_loss: 264.8051 \n",
      "Epoch: [298/1000], TrainLoss: 2.786984460694449\n",
      "training Loss has not improved for 21 epochs.\n",
      "current in epoch    298      batch 0\n",
      "RLoss: 13.450380325317383\n",
      "current in epoch    298      batch 1\n",
      "RLoss: 114.41963195800781\n",
      "current in epoch    298      batch 2\n",
      "RLoss: 466.36065673828125\n",
      "current in epoch    298      batch 3\n",
      "RLoss: 135.2279052734375\n",
      "current in epoch    298      batch 4\n",
      "RLoss: 762.8380737304688\n",
      "current in epoch    298      batch 5\n",
      "RLoss: 46.559173583984375\n",
      "========================================\n",
      "Epoch 299/1000 - partial_train_loss: 244.1652 \n",
      "Epoch: [299/1000], TrainLoss: 45.77721105303083\n",
      "training Loss has not improved for 22 epochs.\n",
      "current in epoch    299      batch 0\n",
      "RLoss: 145.00425720214844\n",
      "current in epoch    299      batch 1\n",
      "RLoss: 23.30572509765625\n",
      "current in epoch    299      batch 2\n",
      "RLoss: 49.549530029296875\n",
      "current in epoch    299      batch 3\n",
      "RLoss: 32.13162612915039\n",
      "current in epoch    299      batch 4\n",
      "RLoss: 94.45909118652344\n",
      "current in epoch    299      batch 5\n",
      "RLoss: 1195.3450927734375\n",
      "========================================\n",
      "Epoch 300/1000 - partial_train_loss: 70.3473 \n",
      "Epoch: [300/1000], TrainLoss: 1054.070798601423\n",
      "training Loss has not improved for 23 epochs.\n",
      "current in epoch    300      batch 0\n",
      "RLoss: 13.56153392791748\n",
      "current in epoch    300      batch 1\n",
      "RLoss: 80.71007537841797\n",
      "current in epoch    300      batch 2\n",
      "RLoss: 630.1771240234375\n",
      "current in epoch    300      batch 3\n",
      "RLoss: 753.1456909179688\n",
      "current in epoch    300      batch 4\n",
      "RLoss: 252.4013671875\n",
      "current in epoch    300      batch 5\n",
      "RLoss: 38.51668930053711\n",
      "========================================\n",
      "Epoch 301/1000 - partial_train_loss: 746.4434 \n",
      "sorting training set\n",
      "Epoch 301/1000 - Training loss: 36.0359 \n",
      "========================================\n",
      "Epoch: [301/1000], TrainLoss: 38.45749970266479\n",
      "training Loss has not improved for 24 epochs.\n",
      "current in epoch    301      batch 0\n",
      "RLoss: 46.259159088134766\n",
      "current in epoch    301      batch 1\n",
      "RLoss: 46.51970291137695\n",
      "current in epoch    301      batch 2\n",
      "RLoss: 13.394210815429688\n",
      "current in epoch    301      batch 3\n",
      "RLoss: 42.291988372802734\n",
      "current in epoch    301      batch 4\n",
      "RLoss: 78.00416564941406\n",
      "current in epoch    301      batch 5\n",
      "RLoss: 108.75011444091797\n",
      "========================================\n",
      "Epoch 302/1000 - partial_train_loss: 54.1553 \n",
      "Epoch: [302/1000], TrainLoss: 109.0799059186663\n",
      "training Loss has not improved for 25 epochs.\n",
      "current in epoch    302      batch 0\n",
      "RLoss: 581.2232666015625\n",
      "current in epoch    302      batch 1\n",
      "RLoss: 1233.373046875\n",
      "current in epoch    302      batch 2\n",
      "RLoss: 97.41404724121094\n",
      "current in epoch    302      batch 3\n",
      "RLoss: 61.433265686035156\n",
      "current in epoch    302      batch 4\n",
      "RLoss: 231.3055877685547\n",
      "current in epoch    302      batch 5\n",
      "RLoss: 253.5806121826172\n",
      "========================================\n",
      "Epoch 303/1000 - partial_train_loss: 309.5104 \n",
      "Epoch: [303/1000], TrainLoss: 159.1377579825265\n",
      "training Loss has not improved for 26 epochs.\n",
      "current in epoch    303      batch 0\n",
      "RLoss: 139.8672332763672\n",
      "current in epoch    303      batch 1\n",
      "RLoss: 1757.3314208984375\n",
      "current in epoch    303      batch 2\n",
      "RLoss: 9.785831451416016\n",
      "current in epoch    303      batch 3\n",
      "RLoss: 64.04419708251953\n",
      "current in epoch    303      batch 4\n",
      "RLoss: 110.09532165527344\n",
      "current in epoch    303      batch 5\n",
      "RLoss: 89.9802017211914\n",
      "========================================\n",
      "Epoch 304/1000 - partial_train_loss: 366.3256 \n",
      "Epoch: [304/1000], TrainLoss: 72.22180836541312\n",
      "training Loss has not improved for 27 epochs.\n",
      "current in epoch    304      batch 0\n",
      "RLoss: 111.03436279296875\n",
      "current in epoch    304      batch 1\n",
      "RLoss: 242.5934600830078\n",
      "current in epoch    304      batch 2\n",
      "RLoss: 156.65170288085938\n",
      "current in epoch    304      batch 3\n",
      "RLoss: 447.1578674316406\n",
      "current in epoch    304      batch 4\n",
      "RLoss: 169.25595092773438\n",
      "current in epoch    304      batch 5\n",
      "RLoss: 134.8574676513672\n",
      "========================================\n",
      "Epoch 305/1000 - partial_train_loss: 215.3703 \n",
      "Epoch: [305/1000], TrainLoss: 87.98462629318237\n",
      "training Loss has not improved for 28 epochs.\n",
      "current in epoch    305      batch 0\n",
      "RLoss: 3605.640869140625\n",
      "current in epoch    305      batch 1\n",
      "RLoss: 396.23590087890625\n",
      "current in epoch    305      batch 2\n",
      "RLoss: 122.34634399414062\n",
      "current in epoch    305      batch 3\n",
      "RLoss: 68.89917755126953\n",
      "current in epoch    305      batch 4\n",
      "RLoss: 388.9794921875\n",
      "current in epoch    305      batch 5\n",
      "RLoss: 324.548583984375\n",
      "========================================\n",
      "Epoch 306/1000 - partial_train_loss: 673.2603 \n",
      "sorting training set\n",
      "Epoch 306/1000 - Training loss: 242.0480 \n",
      "========================================\n",
      "Epoch: [306/1000], TrainLoss: 255.50610638759278\n",
      "training Loss has not improved for 29 epochs.\n",
      "current in epoch    306      batch 0\n",
      "RLoss: 111.5169906616211\n",
      "current in epoch    306      batch 1\n",
      "RLoss: 8.48621940612793\n",
      "current in epoch    306      batch 2\n",
      "RLoss: 367.64459228515625\n",
      "current in epoch    306      batch 3\n",
      "RLoss: 285.28936767578125\n",
      "current in epoch    306      batch 4\n",
      "RLoss: 565.6051635742188\n",
      "current in epoch    306      batch 5\n",
      "RLoss: 95.95223236083984\n",
      "========================================\n",
      "Epoch 307/1000 - partial_train_loss: 302.8467 \n",
      "Epoch: [307/1000], TrainLoss: 150.48980903625488\n",
      "training Loss has not improved for 30 epochs.\n",
      "current in epoch    307      batch 0\n",
      "RLoss: 1506.4451904296875\n",
      "current in epoch    307      batch 1\n",
      "RLoss: 384.4071960449219\n",
      "current in epoch    307      batch 2\n",
      "RLoss: 110.91127014160156\n",
      "current in epoch    307      batch 3\n",
      "RLoss: 312.6294860839844\n",
      "current in epoch    307      batch 4\n",
      "RLoss: 45.71833801269531\n",
      "current in epoch    307      batch 5\n",
      "RLoss: 51.511051177978516\n",
      "========================================\n",
      "Epoch 308/1000 - partial_train_loss: 298.4654 \n",
      "Epoch: [308/1000], TrainLoss: 105.14630835396903\n",
      "training Loss has not improved for 31 epochs.\n",
      "current in epoch    308      batch 0\n",
      "RLoss: 1079.1885986328125\n",
      "current in epoch    308      batch 1\n",
      "RLoss: 289.6496887207031\n",
      "current in epoch    308      batch 2\n",
      "RLoss: 29.354764938354492\n",
      "current in epoch    308      batch 3\n",
      "RLoss: 341.16064453125\n",
      "current in epoch    308      batch 4\n",
      "RLoss: 47.30942916870117\n",
      "current in epoch    308      batch 5\n",
      "RLoss: 274.5272521972656\n",
      "========================================\n",
      "Epoch 309/1000 - partial_train_loss: 271.5007 \n",
      "Epoch: [309/1000], TrainLoss: 280.44523075648715\n",
      "training Loss has not improved for 32 epochs.\n",
      "current in epoch    309      batch 0\n",
      "RLoss: 125.42908477783203\n",
      "current in epoch    309      batch 1\n",
      "RLoss: 85.9674301147461\n",
      "current in epoch    309      batch 2\n",
      "RLoss: 801.4013061523438\n",
      "current in epoch    309      batch 3\n",
      "RLoss: 443.6663818359375\n",
      "current in epoch    309      batch 4\n",
      "RLoss: 41.30949020385742\n",
      "current in epoch    309      batch 5\n",
      "RLoss: 4.024281024932861\n",
      "========================================\n",
      "Epoch 310/1000 - partial_train_loss: 321.4861 \n",
      "Epoch: [310/1000], TrainLoss: 4.894617889608655\n",
      "training Loss has not improved for 33 epochs.\n",
      "current in epoch    310      batch 0\n",
      "RLoss: 106.08206939697266\n",
      "current in epoch    310      batch 1\n",
      "RLoss: 591.9625244140625\n",
      "current in epoch    310      batch 2\n",
      "RLoss: 556.0162963867188\n",
      "current in epoch    310      batch 3\n",
      "RLoss: 13.386152267456055\n",
      "current in epoch    310      batch 4\n",
      "RLoss: 45.86629104614258\n",
      "current in epoch    310      batch 5\n",
      "RLoss: 71.55274963378906\n",
      "========================================\n",
      "Epoch 311/1000 - partial_train_loss: 183.6090 \n",
      "sorting training set\n",
      "Epoch 311/1000 - Training loss: 95.9082 \n",
      "========================================\n",
      "Epoch: [311/1000], TrainLoss: 102.57258988075499\n",
      "training Loss has not improved for 34 epochs.\n",
      "current in epoch    311      batch 0\n",
      "RLoss: 39.48325729370117\n",
      "current in epoch    311      batch 1\n",
      "RLoss: 4.443683624267578\n",
      "current in epoch    311      batch 2\n",
      "RLoss: 74.84304809570312\n",
      "current in epoch    311      batch 3\n",
      "RLoss: 10.281378746032715\n",
      "current in epoch    311      batch 4\n",
      "RLoss: 33.555511474609375\n",
      "current in epoch    311      batch 5\n",
      "RLoss: 54.434810638427734\n",
      "========================================\n",
      "Epoch 312/1000 - partial_train_loss: 99.9238 \n",
      "Epoch: [312/1000], TrainLoss: 43.77596255711147\n",
      "training Loss has not improved for 35 epochs.\n",
      "current in epoch    312      batch 0\n",
      "RLoss: 6.7493815422058105\n",
      "current in epoch    312      batch 1\n",
      "RLoss: 82.51300811767578\n",
      "current in epoch    312      batch 2\n",
      "RLoss: 30.83562469482422\n",
      "current in epoch    312      batch 3\n",
      "RLoss: 13.760294914245605\n",
      "current in epoch    312      batch 4\n",
      "RLoss: 24.683612823486328\n",
      "current in epoch    312      batch 5\n",
      "RLoss: 69.5375747680664\n",
      "========================================\n",
      "Epoch 313/1000 - partial_train_loss: 50.0307 \n",
      "Epoch: [313/1000], TrainLoss: 59.45497901099069\n",
      "training Loss has not improved for 36 epochs.\n",
      "current in epoch    313      batch 0\n",
      "RLoss: 22.344945907592773\n",
      "current in epoch    313      batch 1\n",
      "RLoss: 8.258859634399414\n",
      "current in epoch    313      batch 2\n",
      "RLoss: 159.4818572998047\n",
      "current in epoch    313      batch 3\n",
      "RLoss: 432.7696838378906\n",
      "current in epoch    313      batch 4\n",
      "RLoss: 769.2474975585938\n",
      "current in epoch    313      batch 5\n",
      "RLoss: 105.76495361328125\n",
      "========================================\n",
      "Epoch 314/1000 - partial_train_loss: 249.3459 \n",
      "Epoch: [314/1000], TrainLoss: 161.70974050249373\n",
      "training Loss has not improved for 37 epochs.\n",
      "current in epoch    314      batch 0\n",
      "RLoss: 261.03302001953125\n",
      "current in epoch    314      batch 1\n",
      "RLoss: 91.00151824951172\n",
      "current in epoch    314      batch 2\n",
      "RLoss: 47.72361373901367\n",
      "current in epoch    314      batch 3\n",
      "RLoss: 5.437627792358398\n",
      "current in epoch    314      batch 4\n",
      "RLoss: 8.241926193237305\n",
      "current in epoch    314      batch 5\n",
      "RLoss: 30.280517578125\n",
      "========================================\n",
      "Epoch 315/1000 - partial_train_loss: 93.0282 \n",
      "Epoch: [315/1000], TrainLoss: 37.34911836896624\n",
      "training Loss has not improved for 38 epochs.\n",
      "current in epoch    315      batch 0\n",
      "RLoss: 318.08135986328125\n",
      "current in epoch    315      batch 1\n",
      "RLoss: 13.397431373596191\n",
      "current in epoch    315      batch 2\n",
      "RLoss: 198.29702758789062\n",
      "current in epoch    315      batch 3\n",
      "RLoss: 52.282955169677734\n",
      "current in epoch    315      batch 4\n",
      "RLoss: 18.724740982055664\n",
      "current in epoch    315      batch 5\n",
      "RLoss: 5.509038925170898\n",
      "========================================\n",
      "Epoch 316/1000 - partial_train_loss: 105.0775 \n",
      "sorting training set\n",
      "Epoch 316/1000 - Training loss: 5.6140 \n",
      "========================================\n",
      "Epoch: [316/1000], TrainLoss: 6.208246186492542\n",
      "training Loss has not improved for 39 epochs.\n",
      "current in epoch    316      batch 0\n",
      "RLoss: 418.74578857421875\n",
      "current in epoch    316      batch 1\n",
      "RLoss: 610.9360961914062\n",
      "current in epoch    316      batch 2\n",
      "RLoss: 51.781593322753906\n",
      "current in epoch    316      batch 3\n",
      "RLoss: 11.743300437927246\n",
      "current in epoch    316      batch 4\n",
      "RLoss: 307.0943298339844\n",
      "current in epoch    316      batch 5\n",
      "RLoss: 34.30630111694336\n",
      "========================================\n",
      "Epoch 317/1000 - partial_train_loss: 191.2660 \n",
      "Epoch: [317/1000], TrainLoss: 25.9465810911996\n",
      "training Loss has not improved for 40 epochs.\n",
      "current in epoch    317      batch 0\n",
      "RLoss: 17.177621841430664\n",
      "current in epoch    317      batch 1\n",
      "RLoss: 312.948486328125\n",
      "current in epoch    317      batch 2\n",
      "RLoss: 67.79698944091797\n",
      "current in epoch    317      batch 3\n",
      "RLoss: 17.595062255859375\n",
      "current in epoch    317      batch 4\n",
      "RLoss: 22.460081100463867\n",
      "current in epoch    317      batch 5\n",
      "RLoss: 16.781200408935547\n",
      "========================================\n",
      "Epoch 318/1000 - partial_train_loss: 66.9774 \n",
      "Epoch: [318/1000], TrainLoss: 20.96798358644758\n",
      "training Loss has not improved for 41 epochs.\n",
      "current in epoch    318      batch 0\n",
      "RLoss: 270.25555419921875\n",
      "current in epoch    318      batch 1\n",
      "RLoss: 210.50665283203125\n",
      "current in epoch    318      batch 2\n",
      "RLoss: 21.61452865600586\n",
      "current in epoch    318      batch 3\n",
      "RLoss: 70.85233306884766\n",
      "current in epoch    318      batch 4\n",
      "RLoss: 33.48719024658203\n",
      "current in epoch    318      batch 5\n",
      "RLoss: 21.196985244750977\n",
      "========================================\n",
      "Epoch 319/1000 - partial_train_loss: 94.5652 \n",
      "Epoch: [319/1000], TrainLoss: 29.29368553842817\n",
      "training Loss has not improved for 42 epochs.\n",
      "current in epoch    319      batch 0\n",
      "RLoss: 1018.385986328125\n",
      "current in epoch    319      batch 1\n",
      "RLoss: 442.86395263671875\n",
      "current in epoch    319      batch 2\n",
      "RLoss: 651.4757690429688\n",
      "current in epoch    319      batch 3\n",
      "RLoss: 159.45697021484375\n",
      "current in epoch    319      batch 4\n",
      "RLoss: 858.5078735351562\n",
      "current in epoch    319      batch 5\n",
      "RLoss: 122.20951843261719\n",
      "========================================\n",
      "Epoch 320/1000 - partial_train_loss: 469.1506 \n",
      "Epoch: [320/1000], TrainLoss: 150.06579835074288\n",
      "training Loss has not improved for 43 epochs.\n",
      "current in epoch    320      batch 0\n",
      "RLoss: 477.48187255859375\n",
      "current in epoch    320      batch 1\n",
      "RLoss: 215.0460205078125\n",
      "current in epoch    320      batch 2\n",
      "RLoss: 394.2943420410156\n",
      "current in epoch    320      batch 3\n",
      "RLoss: 247.4739990234375\n",
      "current in epoch    320      batch 4\n",
      "RLoss: 273.9412841796875\n",
      "current in epoch    320      batch 5\n",
      "RLoss: 39.607330322265625\n",
      "========================================\n",
      "Epoch 321/1000 - partial_train_loss: 256.6412 \n",
      "sorting training set\n",
      "Epoch 321/1000 - Training loss: 37.4877 \n",
      "========================================\n",
      "Epoch: [321/1000], TrainLoss: 40.09499245315734\n",
      "training Loss has not improved for 44 epochs.\n",
      "current in epoch    321      batch 0\n",
      "RLoss: 81.53822326660156\n",
      "current in epoch    321      batch 1\n",
      "RLoss: 282.848388671875\n",
      "current in epoch    321      batch 2\n",
      "RLoss: 708.9810791015625\n",
      "current in epoch    321      batch 3\n",
      "RLoss: 106.73934173583984\n",
      "current in epoch    321      batch 4\n",
      "RLoss: 676.3433227539062\n",
      "current in epoch    321      batch 5\n",
      "RLoss: 31.40097427368164\n",
      "========================================\n",
      "Epoch 322/1000 - partial_train_loss: 312.7986 \n",
      "Epoch: [322/1000], TrainLoss: 22.462698817253113\n",
      "training Loss has not improved for 45 epochs.\n",
      "current in epoch    322      batch 0\n",
      "RLoss: 287.3702697753906\n",
      "current in epoch    322      batch 1\n",
      "RLoss: 307.16156005859375\n",
      "current in epoch    322      batch 2\n",
      "RLoss: 495.742431640625\n",
      "current in epoch    322      batch 3\n",
      "RLoss: 152.02578735351562\n",
      "current in epoch    322      batch 4\n",
      "RLoss: 19.683042526245117\n",
      "current in epoch    322      batch 5\n",
      "RLoss: 2.3631231784820557\n",
      "========================================\n",
      "Epoch 323/1000 - partial_train_loss: 195.0330 \n",
      "Epoch: [323/1000], TrainLoss: 2.014424749783107\n",
      "training Loss has not improved for 46 epochs.\n",
      "current in epoch    323      batch 0\n",
      "RLoss: 29.634822845458984\n",
      "current in epoch    323      batch 1\n",
      "RLoss: 7.336066722869873\n",
      "current in epoch    323      batch 2\n",
      "RLoss: 620.42822265625\n",
      "current in epoch    323      batch 3\n",
      "RLoss: 1183.1083984375\n",
      "current in epoch    323      batch 4\n",
      "RLoss: 215.69505310058594\n",
      "current in epoch    323      batch 5\n",
      "RLoss: 486.6918029785156\n",
      "========================================\n",
      "Epoch 324/1000 - partial_train_loss: 283.5023 \n",
      "Epoch: [324/1000], TrainLoss: 390.0188871111189\n",
      "training Loss has not improved for 47 epochs.\n",
      "current in epoch    324      batch 0\n",
      "RLoss: 147.30484008789062\n",
      "current in epoch    324      batch 1\n",
      "RLoss: 71.64813232421875\n",
      "current in epoch    324      batch 2\n",
      "RLoss: 34.14502716064453\n",
      "current in epoch    324      batch 3\n",
      "RLoss: 211.5101318359375\n",
      "current in epoch    324      batch 4\n",
      "RLoss: 59.91371154785156\n",
      "current in epoch    324      batch 5\n",
      "RLoss: 15.883844375610352\n",
      "========================================\n",
      "Epoch 325/1000 - partial_train_loss: 237.7650 \n",
      "Epoch: [325/1000], TrainLoss: 19.653055429458618\n",
      "training Loss has not improved for 48 epochs.\n",
      "current in epoch    325      batch 0\n",
      "RLoss: 814.6082153320312\n",
      "current in epoch    325      batch 1\n",
      "RLoss: 229.4375457763672\n",
      "current in epoch    325      batch 2\n",
      "RLoss: 37.58437728881836\n",
      "current in epoch    325      batch 3\n",
      "RLoss: 56.385189056396484\n",
      "current in epoch    325      batch 4\n",
      "RLoss: 22.55010414123535\n",
      "current in epoch    325      batch 5\n",
      "RLoss: 201.15753173828125\n",
      "========================================\n",
      "Epoch 326/1000 - partial_train_loss: 176.8023 \n",
      "sorting training set\n",
      "Epoch 326/1000 - Training loss: 154.5056 \n",
      "========================================\n",
      "Epoch: [326/1000], TrainLoss: 164.66320624243278\n",
      "training Loss has not improved for 49 epochs.\n",
      "current in epoch    326      batch 0\n",
      "RLoss: 34.90741729736328\n",
      "current in epoch    326      batch 1\n",
      "RLoss: 235.46299743652344\n",
      "current in epoch    326      batch 2\n",
      "RLoss: 6.163172245025635\n",
      "current in epoch    326      batch 3\n",
      "RLoss: 833.1275634765625\n",
      "current in epoch    326      batch 4\n",
      "RLoss: 38.59956741333008\n",
      "current in epoch    326      batch 5\n",
      "RLoss: 181.9998779296875\n",
      "========================================\n",
      "Epoch 327/1000 - partial_train_loss: 239.6365 \n",
      "Epoch: [327/1000], TrainLoss: 189.20073427472795\n",
      "training Loss has not improved for 50 epochs.\n",
      "current in epoch    327      batch 0\n",
      "RLoss: 229.58963012695312\n",
      "current in epoch    327      batch 1\n",
      "RLoss: 189.70933532714844\n",
      "current in epoch    327      batch 2\n",
      "RLoss: 269.1212463378906\n",
      "current in epoch    327      batch 3\n",
      "RLoss: 25.24936294555664\n",
      "current in epoch    327      batch 4\n",
      "RLoss: 118.16832733154297\n",
      "current in epoch    327      batch 5\n",
      "RLoss: 1202.054931640625\n",
      "========================================\n",
      "Epoch 328/1000 - partial_train_loss: 190.6814 \n",
      "Epoch: [328/1000], TrainLoss: 1272.5142604282923\n",
      "training Loss has not improved for 51 epochs.\n",
      "current in epoch    328      batch 0\n",
      "RLoss: 95.43834686279297\n",
      "current in epoch    328      batch 1\n",
      "RLoss: 777.584228515625\n",
      "current in epoch    328      batch 2\n",
      "RLoss: 971.477294921875\n",
      "current in epoch    328      batch 3\n",
      "RLoss: 447.0544128417969\n",
      "current in epoch    328      batch 4\n",
      "RLoss: 818.8768310546875\n",
      "current in epoch    328      batch 5\n",
      "RLoss: 14.666770935058594\n",
      "========================================\n",
      "Epoch 329/1000 - partial_train_loss: 720.5411 \n",
      "Epoch: [329/1000], TrainLoss: 16.599249993051803\n",
      "training Loss has not improved for 52 epochs.\n",
      "current in epoch    329      batch 0\n",
      "RLoss: 102.88603973388672\n",
      "current in epoch    329      batch 1\n",
      "RLoss: 151.26724243164062\n",
      "current in epoch    329      batch 2\n",
      "RLoss: 962.336181640625\n",
      "current in epoch    329      batch 3\n",
      "RLoss: 278.77081298828125\n",
      "current in epoch    329      batch 4\n",
      "RLoss: 3297.260009765625\n",
      "current in epoch    329      batch 5\n",
      "RLoss: 456.6479797363281\n",
      "========================================\n",
      "Epoch 330/1000 - partial_train_loss: 743.8996 \n",
      "Epoch: [330/1000], TrainLoss: 429.335438319615\n",
      "training Loss has not improved for 53 epochs.\n",
      "current in epoch    330      batch 0\n",
      "RLoss: 678.1474609375\n",
      "current in epoch    330      batch 1\n",
      "RLoss: 65.81410217285156\n",
      "current in epoch    330      batch 2\n",
      "RLoss: 497.87451171875\n",
      "current in epoch    330      batch 3\n",
      "RLoss: 14.909555435180664\n",
      "current in epoch    330      batch 4\n",
      "RLoss: 21.873165130615234\n",
      "current in epoch    330      batch 5\n",
      "RLoss: 1067.2459716796875\n",
      "========================================\n",
      "Epoch 331/1000 - partial_train_loss: 277.5721 \n",
      "sorting training set\n",
      "Epoch 331/1000 - Training loss: 649.2887 \n",
      "========================================\n",
      "Epoch: [331/1000], TrainLoss: 691.4246123425673\n",
      "training Loss has not improved for 54 epochs.\n",
      "current in epoch    331      batch 0\n",
      "RLoss: 706.7571411132812\n",
      "current in epoch    331      batch 1\n",
      "RLoss: 310.3466491699219\n",
      "current in epoch    331      batch 2\n",
      "RLoss: 306.71234130859375\n",
      "current in epoch    331      batch 3\n",
      "RLoss: 1351.5703125\n",
      "current in epoch    331      batch 4\n",
      "RLoss: 23.27301788330078\n",
      "current in epoch    331      batch 5\n",
      "RLoss: 138.31451416015625\n",
      "========================================\n",
      "Epoch 332/1000 - partial_train_loss: 939.5421 \n",
      "Epoch: [332/1000], TrainLoss: 123.0326110294887\n",
      "training Loss has not improved for 55 epochs.\n",
      "current in epoch    332      batch 0\n",
      "RLoss: 14.795648574829102\n",
      "current in epoch    332      batch 1\n",
      "RLoss: 520.834716796875\n",
      "current in epoch    332      batch 2\n",
      "RLoss: 10.811807632446289\n",
      "current in epoch    332      batch 3\n",
      "RLoss: 276.0904541015625\n",
      "current in epoch    332      batch 4\n",
      "RLoss: 467.440185546875\n",
      "current in epoch    332      batch 5\n",
      "RLoss: 1019.3203125\n",
      "========================================\n",
      "Epoch 333/1000 - partial_train_loss: 187.4735 \n",
      "Epoch: [333/1000], TrainLoss: 503.22800309317455\n",
      "training Loss has not improved for 56 epochs.\n",
      "current in epoch    333      batch 0\n",
      "RLoss: 1289.5875244140625\n",
      "current in epoch    333      batch 1\n",
      "RLoss: 133.96624755859375\n",
      "current in epoch    333      batch 2\n",
      "RLoss: 511.40777587890625\n",
      "current in epoch    333      batch 3\n",
      "RLoss: 254.75816345214844\n",
      "current in epoch    333      batch 4\n",
      "RLoss: 90.61481475830078\n",
      "current in epoch    333      batch 5\n",
      "RLoss: 159.75961303710938\n",
      "========================================\n",
      "Epoch 334/1000 - partial_train_loss: 646.8668 \n",
      "Epoch: [334/1000], TrainLoss: 150.16307640075684\n",
      "training Loss has not improved for 57 epochs.\n",
      "current in epoch    334      batch 0\n",
      "RLoss: 82.07946014404297\n",
      "current in epoch    334      batch 1\n",
      "RLoss: 82.90992736816406\n",
      "current in epoch    334      batch 2\n",
      "RLoss: 59.45204162597656\n",
      "current in epoch    334      batch 3\n",
      "RLoss: 422.4029235839844\n",
      "current in epoch    334      batch 4\n",
      "RLoss: 557.12646484375\n",
      "current in epoch    334      batch 5\n",
      "RLoss: 852.50439453125\n",
      "========================================\n",
      "Epoch 335/1000 - partial_train_loss: 238.9585 \n",
      "Epoch: [335/1000], TrainLoss: 638.8396857125418\n",
      "training Loss has not improved for 58 epochs.\n",
      "current in epoch    335      batch 0\n",
      "RLoss: 134.99609375\n",
      "current in epoch    335      batch 1\n",
      "RLoss: 232.41781616210938\n",
      "current in epoch    335      batch 2\n",
      "RLoss: 810.3416137695312\n",
      "current in epoch    335      batch 3\n",
      "RLoss: 93.94859313964844\n",
      "current in epoch    335      batch 4\n",
      "RLoss: 1699.331787109375\n",
      "current in epoch    335      batch 5\n",
      "RLoss: 297.6584167480469\n",
      "========================================\n",
      "Epoch 336/1000 - partial_train_loss: 694.5555 \n",
      "sorting training set\n",
      "Epoch 336/1000 - Training loss: 237.7658 \n",
      "========================================\n",
      "Epoch: [336/1000], TrainLoss: 254.58487735881786\n",
      "training Loss has not improved for 59 epochs.\n",
      "current in epoch    336      batch 0\n",
      "RLoss: 34.024749755859375\n",
      "current in epoch    336      batch 1\n",
      "RLoss: 2482.20263671875\n",
      "current in epoch    336      batch 2\n",
      "RLoss: 176.355224609375\n",
      "current in epoch    336      batch 3\n",
      "RLoss: 293.9013671875\n",
      "current in epoch    336      batch 4\n",
      "RLoss: 141.13917541503906\n",
      "current in epoch    336      batch 5\n",
      "RLoss: 40.4255485534668\n",
      "========================================\n",
      "Epoch 337/1000 - partial_train_loss: 722.3166 \n",
      "Epoch: [337/1000], TrainLoss: 43.245003836495535\n",
      "training Loss has not improved for 60 epochs.\n",
      "current in epoch    337      batch 0\n",
      "RLoss: 159.67474365234375\n",
      "current in epoch    337      batch 1\n",
      "RLoss: 74.34422302246094\n",
      "current in epoch    337      batch 2\n",
      "RLoss: 130.63540649414062\n",
      "current in epoch    337      batch 3\n",
      "RLoss: 209.13609313964844\n",
      "current in epoch    337      batch 4\n",
      "RLoss: 359.4537048339844\n",
      "current in epoch    337      batch 5\n",
      "RLoss: 1601.9334716796875\n",
      "========================================\n",
      "Epoch 338/1000 - partial_train_loss: 155.0277 \n",
      "Epoch: [338/1000], TrainLoss: 1235.9446432931084\n",
      "training Loss has not improved for 61 epochs.\n",
      "current in epoch    338      batch 0\n",
      "RLoss: 359.5224304199219\n",
      "current in epoch    338      batch 1\n",
      "RLoss: 330.7025451660156\n",
      "current in epoch    338      batch 2\n",
      "RLoss: 9.97547721862793\n",
      "current in epoch    338      batch 3\n",
      "RLoss: 444.8340148925781\n",
      "current in epoch    338      batch 4\n",
      "RLoss: 64.30030059814453\n",
      "current in epoch    338      batch 5\n",
      "RLoss: 6.9876604080200195\n",
      "========================================\n",
      "Epoch 339/1000 - partial_train_loss: 543.5230 \n",
      "Epoch: [339/1000], TrainLoss: 7.8045419454574585\n",
      "training Loss has not improved for 62 epochs.\n",
      "current in epoch    339      batch 0\n",
      "RLoss: 388.15899658203125\n",
      "current in epoch    339      batch 1\n",
      "RLoss: 128.76895141601562\n",
      "current in epoch    339      batch 2\n",
      "RLoss: 636.1084594726562\n",
      "current in epoch    339      batch 3\n",
      "RLoss: 28.437870025634766\n",
      "current in epoch    339      batch 4\n",
      "RLoss: 233.10418701171875\n",
      "current in epoch    339      batch 5\n",
      "RLoss: 42.55311584472656\n",
      "========================================\n",
      "Epoch 340/1000 - partial_train_loss: 211.5602 \n",
      "Epoch: [340/1000], TrainLoss: 36.76636746951512\n",
      "training Loss has not improved for 63 epochs.\n",
      "current in epoch    340      batch 0\n",
      "RLoss: 250.31350708007812\n",
      "current in epoch    340      batch 1\n",
      "RLoss: 22.600004196166992\n",
      "current in epoch    340      batch 2\n",
      "RLoss: 67.3244857788086\n",
      "current in epoch    340      batch 3\n",
      "RLoss: 390.3942565917969\n",
      "current in epoch    340      batch 4\n",
      "RLoss: 34.15128707885742\n",
      "current in epoch    340      batch 5\n",
      "RLoss: 17.719411849975586\n",
      "========================================\n",
      "Epoch 341/1000 - partial_train_loss: 129.7656 \n",
      "sorting training set\n",
      "Epoch 341/1000 - Training loss: 22.2394 \n",
      "========================================\n",
      "Epoch: [341/1000], TrainLoss: 23.703105315358222\n",
      "training Loss has not improved for 64 epochs.\n",
      "current in epoch    341      batch 0\n",
      "RLoss: 626.2407836914062\n",
      "current in epoch    341      batch 1\n",
      "RLoss: 346.3154296875\n",
      "current in epoch    341      batch 2\n",
      "RLoss: 37.5230712890625\n",
      "current in epoch    341      batch 3\n",
      "RLoss: 51.51862716674805\n",
      "current in epoch    341      batch 4\n",
      "RLoss: 42.236454010009766\n",
      "current in epoch    341      batch 5\n",
      "RLoss: 298.77984619140625\n",
      "========================================\n",
      "Epoch 342/1000 - partial_train_loss: 198.9206 \n",
      "Epoch: [342/1000], TrainLoss: 365.6951198577881\n",
      "training Loss has not improved for 65 epochs.\n",
      "current in epoch    342      batch 0\n",
      "RLoss: 706.375244140625\n",
      "current in epoch    342      batch 1\n",
      "RLoss: 147.5768585205078\n",
      "current in epoch    342      batch 2\n",
      "RLoss: 169.9708709716797\n",
      "current in epoch    342      batch 3\n",
      "RLoss: 346.256591796875\n",
      "current in epoch    342      batch 4\n",
      "RLoss: 815.2942504882812\n",
      "current in epoch    342      batch 5\n",
      "RLoss: 24.488523483276367\n",
      "========================================\n",
      "Epoch 343/1000 - partial_train_loss: 333.9806 \n",
      "Epoch: [343/1000], TrainLoss: 18.205691201346262\n",
      "training Loss has not improved for 66 epochs.\n",
      "current in epoch    343      batch 0\n",
      "RLoss: 8.986701011657715\n",
      "current in epoch    343      batch 1\n",
      "RLoss: 270.9264831542969\n",
      "current in epoch    343      batch 2\n",
      "RLoss: 185.84030151367188\n",
      "current in epoch    343      batch 3\n",
      "RLoss: 386.9562072753906\n",
      "current in epoch    343      batch 4\n",
      "RLoss: 116.89846801757812\n",
      "current in epoch    343      batch 5\n",
      "RLoss: 9.035386085510254\n",
      "========================================\n",
      "Epoch 344/1000 - partial_train_loss: 179.4959 \n",
      "Epoch: [344/1000], TrainLoss: 6.637656620570591\n",
      "training Loss has not improved for 67 epochs.\n",
      "current in epoch    344      batch 0\n",
      "RLoss: 209.08282470703125\n",
      "current in epoch    344      batch 1\n",
      "RLoss: 152.54750061035156\n",
      "current in epoch    344      batch 2\n",
      "RLoss: 108.65692138671875\n",
      "current in epoch    344      batch 3\n",
      "RLoss: 98.1988754272461\n",
      "current in epoch    344      batch 4\n",
      "RLoss: 248.57899475097656\n",
      "current in epoch    344      batch 5\n",
      "RLoss: 128.6548614501953\n",
      "========================================\n",
      "Epoch 345/1000 - partial_train_loss: 139.4781 \n",
      "Epoch: [345/1000], TrainLoss: 102.90372358049665\n",
      "training Loss has not improved for 68 epochs.\n",
      "current in epoch    345      batch 0\n",
      "RLoss: 49.91038513183594\n",
      "current in epoch    345      batch 1\n",
      "RLoss: 4232.25390625\n",
      "current in epoch    345      batch 2\n",
      "RLoss: 762.412353515625\n",
      "current in epoch    345      batch 3\n",
      "RLoss: 208.62762451171875\n",
      "current in epoch    345      batch 4\n",
      "RLoss: 27.841068267822266\n",
      "current in epoch    345      batch 5\n",
      "RLoss: 95.47370147705078\n",
      "========================================\n",
      "Epoch 346/1000 - partial_train_loss: 865.0694 \n",
      "sorting training set\n",
      "Epoch 346/1000 - Training loss: 66.2145 \n",
      "========================================\n",
      "Epoch: [346/1000], TrainLoss: 70.91695293135999\n",
      "training Loss has not improved for 69 epochs.\n",
      "current in epoch    346      batch 0\n",
      "RLoss: 173.59857177734375\n",
      "current in epoch    346      batch 1\n",
      "RLoss: 367.0230712890625\n",
      "current in epoch    346      batch 2\n",
      "RLoss: 588.3818359375\n",
      "current in epoch    346      batch 3\n",
      "RLoss: 116.12567138671875\n",
      "current in epoch    346      batch 4\n",
      "RLoss: 17.306682586669922\n",
      "current in epoch    346      batch 5\n",
      "RLoss: 54.222442626953125\n",
      "========================================\n",
      "Epoch 347/1000 - partial_train_loss: 217.9123 \n",
      "Epoch: [347/1000], TrainLoss: 40.739151750292095\n",
      "training Loss has not improved for 70 epochs.\n",
      "current in epoch    347      batch 0\n",
      "RLoss: 386.9403991699219\n",
      "current in epoch    347      batch 1\n",
      "RLoss: 37.7901611328125\n",
      "current in epoch    347      batch 2\n",
      "RLoss: 348.8059387207031\n",
      "current in epoch    347      batch 3\n",
      "RLoss: 143.30593872070312\n",
      "current in epoch    347      batch 4\n",
      "RLoss: 739.56787109375\n",
      "current in epoch    347      batch 5\n",
      "RLoss: 273.63409423828125\n",
      "========================================\n",
      "Epoch 348/1000 - partial_train_loss: 265.3823 \n",
      "Epoch: [348/1000], TrainLoss: 222.530339377267\n",
      "training Loss has not improved for 71 epochs.\n",
      "current in epoch    348      batch 0\n",
      "RLoss: 883.5604858398438\n",
      "current in epoch    348      batch 1\n",
      "RLoss: 3444.179931640625\n",
      "current in epoch    348      batch 2\n",
      "RLoss: 176.5772247314453\n",
      "current in epoch    348      batch 3\n",
      "RLoss: 96.70661926269531\n",
      "current in epoch    348      batch 4\n",
      "RLoss: 271.2864685058594\n",
      "current in epoch    348      batch 5\n",
      "RLoss: 207.8495330810547\n",
      "========================================\n",
      "Epoch 349/1000 - partial_train_loss: 936.3396 \n",
      "Epoch: [349/1000], TrainLoss: 232.49181202479772\n",
      "training Loss has not improved for 72 epochs.\n",
      "current in epoch    349      batch 0\n",
      "RLoss: 2529.3642578125\n",
      "current in epoch    349      batch 1\n",
      "RLoss: 174.99147033691406\n",
      "current in epoch    349      batch 2\n",
      "RLoss: 198.66148376464844\n",
      "current in epoch    349      batch 3\n",
      "RLoss: 18.0681095123291\n",
      "current in epoch    349      batch 4\n",
      "RLoss: 73.51695251464844\n",
      "current in epoch    349      batch 5\n",
      "RLoss: 26.964725494384766\n",
      "========================================\n",
      "Epoch 350/1000 - partial_train_loss: 510.0228 \n",
      "Epoch: [350/1000], TrainLoss: 22.27220640863691\n",
      "training Loss has not improved for 73 epochs.\n",
      "current in epoch    350      batch 0\n",
      "RLoss: 365.7511901855469\n",
      "current in epoch    350      batch 1\n",
      "RLoss: 26.1374454498291\n",
      "current in epoch    350      batch 2\n",
      "RLoss: 22.914384841918945\n",
      "current in epoch    350      batch 3\n",
      "RLoss: 81.84161376953125\n",
      "current in epoch    350      batch 4\n",
      "RLoss: 137.1417694091797\n",
      "current in epoch    350      batch 5\n",
      "RLoss: 55.91178512573242\n",
      "========================================\n",
      "Epoch 351/1000 - partial_train_loss: 115.1617 \n",
      "sorting training set\n",
      "Epoch 351/1000 - Training loss: 57.5675 \n",
      "========================================\n",
      "Epoch: [351/1000], TrainLoss: 61.900619499111485\n",
      "training Loss has not improved for 74 epochs.\n",
      "current in epoch    351      batch 0\n",
      "RLoss: 241.41726684570312\n",
      "current in epoch    351      batch 1\n",
      "RLoss: 110.69625091552734\n",
      "current in epoch    351      batch 2\n",
      "RLoss: 779.8218383789062\n",
      "current in epoch    351      batch 3\n",
      "RLoss: 90.3187484741211\n",
      "current in epoch    351      batch 4\n",
      "RLoss: 584.5770263671875\n",
      "current in epoch    351      batch 5\n",
      "RLoss: 144.90380859375\n",
      "========================================\n",
      "Epoch 352/1000 - partial_train_loss: 332.3162 \n",
      "Epoch: [352/1000], TrainLoss: 154.91428157261439\n",
      "training Loss has not improved for 75 epochs.\n",
      "current in epoch    352      batch 0\n",
      "RLoss: 312.5445251464844\n",
      "current in epoch    352      batch 1\n",
      "RLoss: 27.10264015197754\n",
      "current in epoch    352      batch 2\n",
      "RLoss: 282.2872619628906\n",
      "current in epoch    352      batch 3\n",
      "RLoss: 50.34824752807617\n",
      "current in epoch    352      batch 4\n",
      "RLoss: 77.41492462158203\n",
      "current in epoch    352      batch 5\n",
      "RLoss: 321.2825927734375\n",
      "========================================\n",
      "Epoch 353/1000 - partial_train_loss: 128.5941 \n",
      "Epoch: [353/1000], TrainLoss: 310.70803669520785\n",
      "training Loss has not improved for 76 epochs.\n",
      "current in epoch    353      batch 0\n",
      "RLoss: 606.513671875\n",
      "current in epoch    353      batch 1\n",
      "RLoss: 12.541178703308105\n",
      "current in epoch    353      batch 2\n",
      "RLoss: 707.3868408203125\n",
      "current in epoch    353      batch 3\n",
      "RLoss: 87.4044189453125\n",
      "current in epoch    353      batch 4\n",
      "RLoss: 70.5349349975586\n",
      "current in epoch    353      batch 5\n",
      "RLoss: 470.52362060546875\n",
      "========================================\n",
      "Epoch 354/1000 - partial_train_loss: 267.8346 \n",
      "Epoch: [354/1000], TrainLoss: 565.2346801757812\n",
      "training Loss has not improved for 77 epochs.\n",
      "current in epoch    354      batch 0\n",
      "RLoss: 993.674560546875\n",
      "current in epoch    354      batch 1\n",
      "RLoss: 37.702266693115234\n",
      "current in epoch    354      batch 2\n",
      "RLoss: 232.07273864746094\n",
      "current in epoch    354      batch 3\n",
      "RLoss: 1467.792724609375\n",
      "current in epoch    354      batch 4\n",
      "RLoss: 13.544285774230957\n",
      "current in epoch    354      batch 5\n",
      "RLoss: 257.93377685546875\n",
      "========================================\n",
      "Epoch 355/1000 - partial_train_loss: 488.9335 \n",
      "Epoch: [355/1000], TrainLoss: 283.7238518851144\n",
      "training Loss has not improved for 78 epochs.\n",
      "current in epoch    355      batch 0\n",
      "RLoss: 9.250801086425781\n",
      "current in epoch    355      batch 1\n",
      "RLoss: 320.74615478515625\n",
      "current in epoch    355      batch 2\n",
      "RLoss: 148.4226837158203\n",
      "current in epoch    355      batch 3\n",
      "RLoss: 317.8509826660156\n",
      "current in epoch    355      batch 4\n",
      "RLoss: 167.70176696777344\n",
      "current in epoch    355      batch 5\n",
      "RLoss: 118.84724426269531\n",
      "========================================\n",
      "Epoch 356/1000 - partial_train_loss: 191.8333 \n",
      "sorting training set\n",
      "Epoch 356/1000 - Training loss: 162.1198 \n",
      "========================================\n",
      "Epoch: [356/1000], TrainLoss: 173.24008387885823\n",
      "training Loss has not improved for 79 epochs.\n",
      "current in epoch    356      batch 0\n",
      "RLoss: 36.9012336730957\n",
      "current in epoch    356      batch 1\n",
      "RLoss: 38.34817123413086\n",
      "current in epoch    356      batch 2\n",
      "RLoss: 122.11964416503906\n",
      "current in epoch    356      batch 3\n",
      "RLoss: 171.6986083984375\n",
      "current in epoch    356      batch 4\n",
      "RLoss: 308.8516540527344\n",
      "current in epoch    356      batch 5\n",
      "RLoss: 209.468505859375\n",
      "========================================\n",
      "Epoch 357/1000 - partial_train_loss: 224.3162 \n",
      "Epoch: [357/1000], TrainLoss: 200.27560969761439\n",
      "training Loss has not improved for 80 epochs.\n",
      "current in epoch    357      batch 0\n",
      "RLoss: 505.16583251953125\n",
      "current in epoch    357      batch 1\n",
      "RLoss: 51.17647171020508\n",
      "current in epoch    357      batch 2\n",
      "RLoss: 19.08266258239746\n",
      "current in epoch    357      batch 3\n",
      "RLoss: 147.2463836669922\n",
      "current in epoch    357      batch 4\n",
      "RLoss: 41.14389419555664\n",
      "current in epoch    357      batch 5\n",
      "RLoss: 5.909852981567383\n",
      "========================================\n",
      "Epoch 358/1000 - partial_train_loss: 183.2739 \n",
      "Epoch: [358/1000], TrainLoss: 4.955382853746414\n",
      "training Loss has not improved for 81 epochs.\n",
      "current in epoch    358      batch 0\n",
      "RLoss: 24.680625915527344\n",
      "current in epoch    358      batch 1\n",
      "RLoss: 30.997699737548828\n",
      "current in epoch    358      batch 2\n",
      "RLoss: 43.10205841064453\n",
      "current in epoch    358      batch 3\n",
      "RLoss: 22.632566452026367\n",
      "current in epoch    358      batch 4\n",
      "RLoss: 8.02613639831543\n",
      "current in epoch    358      batch 5\n",
      "RLoss: 9.83220386505127\n",
      "========================================\n",
      "Epoch 359/1000 - partial_train_loss: 18.2431 \n",
      "Epoch: [359/1000], TrainLoss: 16.4897038936615\n",
      "training Loss has not improved for 82 epochs.\n",
      "current in epoch    359      batch 0\n",
      "RLoss: 99.88909149169922\n",
      "current in epoch    359      batch 1\n",
      "RLoss: 110.55099487304688\n",
      "current in epoch    359      batch 2\n",
      "RLoss: 23.514564514160156\n",
      "current in epoch    359      batch 3\n",
      "RLoss: 28.659778594970703\n",
      "current in epoch    359      batch 4\n",
      "RLoss: 71.16104888916016\n",
      "current in epoch    359      batch 5\n",
      "RLoss: 42.63991165161133\n",
      "========================================\n",
      "Epoch 360/1000 - partial_train_loss: 61.0342 \n",
      "Epoch: [360/1000], TrainLoss: 51.737335545676096\n",
      "training Loss has not improved for 83 epochs.\n",
      "current in epoch    360      batch 0\n",
      "RLoss: 21.951908111572266\n",
      "current in epoch    360      batch 1\n",
      "RLoss: 44.75715637207031\n",
      "current in epoch    360      batch 2\n",
      "RLoss: 100.42578887939453\n",
      "current in epoch    360      batch 3\n",
      "RLoss: 364.56298828125\n",
      "current in epoch    360      batch 4\n",
      "RLoss: 115.04025268554688\n",
      "current in epoch    360      batch 5\n",
      "RLoss: 196.73663330078125\n",
      "========================================\n",
      "Epoch 361/1000 - partial_train_loss: 112.1499 \n",
      "sorting training set\n",
      "Epoch 361/1000 - Training loss: 99.2037 \n",
      "========================================\n",
      "Epoch: [361/1000], TrainLoss: 105.84816785571856\n",
      "training Loss has not improved for 84 epochs.\n",
      "current in epoch    361      batch 0\n",
      "RLoss: 187.50997924804688\n",
      "current in epoch    361      batch 1\n",
      "RLoss: 59.65789031982422\n",
      "current in epoch    361      batch 2\n",
      "RLoss: 124.87274932861328\n",
      "current in epoch    361      batch 3\n",
      "RLoss: 122.5611801147461\n",
      "current in epoch    361      batch 4\n",
      "RLoss: 238.31240844726562\n",
      "current in epoch    361      batch 5\n",
      "RLoss: 9.514323234558105\n",
      "========================================\n",
      "Epoch 362/1000 - partial_train_loss: 182.5863 \n",
      "Epoch: [362/1000], TrainLoss: 20.33312657901219\n",
      "training Loss has not improved for 85 epochs.\n",
      "current in epoch    362      batch 0\n",
      "RLoss: 43.06969451904297\n",
      "current in epoch    362      batch 1\n",
      "RLoss: 171.48849487304688\n",
      "current in epoch    362      batch 2\n",
      "RLoss: 22.406450271606445\n",
      "current in epoch    362      batch 3\n",
      "RLoss: 185.7878875732422\n",
      "current in epoch    362      batch 4\n",
      "RLoss: 246.08758544921875\n",
      "current in epoch    362      batch 5\n",
      "RLoss: 325.4729309082031\n",
      "========================================\n",
      "Epoch 363/1000 - partial_train_loss: 137.5865 \n",
      "Epoch: [363/1000], TrainLoss: 261.7590708051409\n",
      "training Loss has not improved for 86 epochs.\n",
      "current in epoch    363      batch 0\n",
      "RLoss: 106.6991958618164\n",
      "current in epoch    363      batch 1\n",
      "RLoss: 52.54634094238281\n",
      "current in epoch    363      batch 2\n",
      "RLoss: 668.428466796875\n",
      "current in epoch    363      batch 3\n",
      "RLoss: 256.7624816894531\n",
      "current in epoch    363      batch 4\n",
      "RLoss: 60.547603607177734\n",
      "current in epoch    363      batch 5\n",
      "RLoss: 7.901304244995117\n",
      "========================================\n",
      "Epoch 364/1000 - partial_train_loss: 244.1991 \n",
      "Epoch: [364/1000], TrainLoss: 19.964189495359147\n",
      "training Loss has not improved for 87 epochs.\n",
      "current in epoch    364      batch 0\n",
      "RLoss: 837.7308959960938\n",
      "current in epoch    364      batch 1\n",
      "RLoss: 31.872352600097656\n",
      "current in epoch    364      batch 2\n",
      "RLoss: 63.27284622192383\n",
      "current in epoch    364      batch 3\n",
      "RLoss: 80.06660461425781\n",
      "current in epoch    364      batch 4\n",
      "RLoss: 181.18777465820312\n",
      "current in epoch    364      batch 5\n",
      "RLoss: 50.43257141113281\n",
      "========================================\n",
      "Epoch 365/1000 - partial_train_loss: 188.4515 \n",
      "Epoch: [365/1000], TrainLoss: 54.01081248692104\n",
      "training Loss has not improved for 88 epochs.\n",
      "current in epoch    365      batch 0\n",
      "RLoss: 409.7232666015625\n",
      "current in epoch    365      batch 1\n",
      "RLoss: 148.2598876953125\n",
      "current in epoch    365      batch 2\n",
      "RLoss: 234.36239624023438\n",
      "current in epoch    365      batch 3\n",
      "RLoss: 90.9045639038086\n",
      "current in epoch    365      batch 4\n",
      "RLoss: 25.945552825927734\n",
      "current in epoch    365      batch 5\n",
      "RLoss: 49.20109558105469\n",
      "========================================\n",
      "Epoch 366/1000 - partial_train_loss: 136.9864 \n",
      "sorting training set\n",
      "Epoch 366/1000 - Training loss: 49.0966 \n",
      "========================================\n",
      "Epoch: [366/1000], TrainLoss: 52.66968602797975\n",
      "training Loss has not improved for 89 epochs.\n",
      "current in epoch    366      batch 0\n",
      "RLoss: 220.95872497558594\n",
      "current in epoch    366      batch 1\n",
      "RLoss: 112.13082885742188\n",
      "current in epoch    366      batch 2\n",
      "RLoss: 16.875720977783203\n",
      "current in epoch    366      batch 3\n",
      "RLoss: 24.170936584472656\n",
      "current in epoch    366      batch 4\n",
      "RLoss: 127.12640380859375\n",
      "current in epoch    366      batch 5\n",
      "RLoss: 57.357337951660156\n",
      "========================================\n",
      "Epoch 367/1000 - partial_train_loss: 112.0055 \n",
      "Epoch: [367/1000], TrainLoss: 61.62439359937395\n",
      "training Loss has not improved for 90 epochs.\n",
      "current in epoch    367      batch 0\n",
      "RLoss: 406.5272521972656\n",
      "current in epoch    367      batch 1\n",
      "RLoss: 123.60565185546875\n",
      "current in epoch    367      batch 2\n",
      "RLoss: 26.367563247680664\n",
      "current in epoch    367      batch 3\n",
      "RLoss: 328.52484130859375\n",
      "current in epoch    367      batch 4\n",
      "RLoss: 43.49933624267578\n",
      "current in epoch    367      batch 5\n",
      "RLoss: 305.6792297363281\n",
      "========================================\n",
      "Epoch 368/1000 - partial_train_loss: 135.9679 \n",
      "Epoch: [368/1000], TrainLoss: 238.78839792524064\n",
      "training Loss has not improved for 91 epochs.\n",
      "current in epoch    368      batch 0\n",
      "RLoss: 1190.7474365234375\n",
      "current in epoch    368      batch 1\n",
      "RLoss: 75.13958740234375\n",
      "current in epoch    368      batch 2\n",
      "RLoss: 1134.8212890625\n",
      "current in epoch    368      batch 3\n",
      "RLoss: 44.92341613769531\n",
      "current in epoch    368      batch 4\n",
      "RLoss: 34.07889175415039\n",
      "current in epoch    368      batch 5\n",
      "RLoss: 38.006439208984375\n",
      "========================================\n",
      "Epoch 369/1000 - partial_train_loss: 422.1718 \n",
      "Epoch: [369/1000], TrainLoss: 40.06778526306152\n",
      "training Loss has not improved for 92 epochs.\n",
      "current in epoch    369      batch 0\n",
      "RLoss: 499.89306640625\n",
      "current in epoch    369      batch 1\n",
      "RLoss: 43.25114059448242\n",
      "current in epoch    369      batch 2\n",
      "RLoss: 20.340084075927734\n",
      "current in epoch    369      batch 3\n",
      "RLoss: 134.781494140625\n",
      "current in epoch    369      batch 4\n",
      "RLoss: 14.789935111999512\n",
      "current in epoch    369      batch 5\n",
      "RLoss: 351.44573974609375\n",
      "========================================\n",
      "Epoch 370/1000 - partial_train_loss: 93.9503 \n",
      "Epoch: [370/1000], TrainLoss: 319.31864329746793\n",
      "training Loss has not improved for 93 epochs.\n",
      "current in epoch    370      batch 0\n",
      "RLoss: 396.5848083496094\n",
      "current in epoch    370      batch 1\n",
      "RLoss: 57.41984558105469\n",
      "current in epoch    370      batch 2\n",
      "RLoss: 31.058162689208984\n",
      "current in epoch    370      batch 3\n",
      "RLoss: 84.84860229492188\n",
      "current in epoch    370      batch 4\n",
      "RLoss: 57.04618835449219\n",
      "current in epoch    370      batch 5\n",
      "RLoss: 43.69111251831055\n",
      "========================================\n",
      "Epoch 371/1000 - partial_train_loss: 186.4197 \n",
      "sorting training set\n",
      "Epoch 371/1000 - Training loss: 40.2613 \n",
      "========================================\n",
      "Epoch: [371/1000], TrainLoss: 43.20609816895597\n",
      "training Loss has not improved for 94 epochs.\n",
      "current in epoch    371      batch 0\n",
      "RLoss: 89.18211364746094\n",
      "current in epoch    371      batch 1\n",
      "RLoss: 179.992431640625\n",
      "current in epoch    371      batch 2\n",
      "RLoss: 17.445602416992188\n",
      "current in epoch    371      batch 3\n",
      "RLoss: 1605.6416015625\n",
      "current in epoch    371      batch 4\n",
      "RLoss: 1996.52392578125\n",
      "current in epoch    371      batch 5\n",
      "RLoss: 182.59677124023438\n",
      "========================================\n",
      "Epoch 372/1000 - partial_train_loss: 620.7529 \n",
      "Epoch: [372/1000], TrainLoss: 207.50621632167272\n",
      "training Loss has not improved for 95 epochs.\n",
      "current in epoch    372      batch 0\n",
      "RLoss: 151.69285583496094\n",
      "current in epoch    372      batch 1\n",
      "RLoss: 131.4563751220703\n",
      "current in epoch    372      batch 2\n",
      "RLoss: 137.2039794921875\n",
      "current in epoch    372      batch 3\n",
      "RLoss: 23.074880599975586\n",
      "current in epoch    372      batch 4\n",
      "RLoss: 373.8709411621094\n",
      "current in epoch    372      batch 5\n",
      "RLoss: 209.92730712890625\n",
      "========================================\n",
      "Epoch 373/1000 - partial_train_loss: 160.5704 \n",
      "Epoch: [373/1000], TrainLoss: 306.1808547973633\n",
      "training Loss has not improved for 96 epochs.\n",
      "current in epoch    373      batch 0\n",
      "RLoss: 697.4373779296875\n",
      "current in epoch    373      batch 1\n",
      "RLoss: 420.00311279296875\n",
      "current in epoch    373      batch 2\n",
      "RLoss: 3361.995849609375\n",
      "current in epoch    373      batch 3\n",
      "RLoss: 183.4859619140625\n",
      "current in epoch    373      batch 4\n",
      "RLoss: 90.04149627685547\n",
      "current in epoch    373      batch 5\n",
      "RLoss: 385.3932800292969\n",
      "========================================\n",
      "Epoch 374/1000 - partial_train_loss: 776.8673 \n",
      "Epoch: [374/1000], TrainLoss: 334.37346322195873\n",
      "training Loss has not improved for 97 epochs.\n",
      "current in epoch    374      batch 0\n",
      "RLoss: 52.53668975830078\n",
      "current in epoch    374      batch 1\n",
      "RLoss: 34.23100280761719\n",
      "current in epoch    374      batch 2\n",
      "RLoss: 63.47917938232422\n",
      "current in epoch    374      batch 3\n",
      "RLoss: 52.1619873046875\n",
      "current in epoch    374      batch 4\n",
      "RLoss: 136.92849731445312\n",
      "current in epoch    374      batch 5\n",
      "RLoss: 31.11578369140625\n",
      "========================================\n",
      "Epoch 375/1000 - partial_train_loss: 123.3555 \n",
      "Epoch: [375/1000], TrainLoss: 45.20440019880022\n",
      "training Loss has not improved for 98 epochs.\n",
      "current in epoch    375      batch 0\n",
      "RLoss: 85.22754669189453\n",
      "current in epoch    375      batch 1\n",
      "RLoss: 188.99691772460938\n",
      "current in epoch    375      batch 2\n",
      "RLoss: 416.8248596191406\n",
      "current in epoch    375      batch 3\n",
      "RLoss: 50.06110763549805\n",
      "current in epoch    375      batch 4\n",
      "RLoss: 47.28022003173828\n",
      "current in epoch    375      batch 5\n",
      "RLoss: 64.4021987915039\n",
      "========================================\n",
      "Epoch 376/1000 - partial_train_loss: 147.1567 \n",
      "sorting training set\n",
      "Epoch 376/1000 - Training loss: 70.7760 \n",
      "========================================\n",
      "Epoch: [376/1000], TrainLoss: 75.89399230931703\n",
      "training Loss has not improved for 99 epochs.\n",
      "current in epoch    376      batch 0\n",
      "RLoss: 50.02043151855469\n",
      "current in epoch    376      batch 1\n",
      "RLoss: 37.718536376953125\n",
      "current in epoch    376      batch 2\n",
      "RLoss: 135.68673706054688\n",
      "current in epoch    376      batch 3\n",
      "RLoss: 211.27357482910156\n",
      "current in epoch    376      batch 4\n",
      "RLoss: 215.5013427734375\n",
      "current in epoch    376      batch 5\n",
      "RLoss: 303.4421691894531\n",
      "========================================\n",
      "Epoch 377/1000 - partial_train_loss: 179.1482 \n",
      "Epoch: [377/1000], TrainLoss: 385.9875150408064\n",
      "training Loss has not improved for 100 epochs.\n",
      "current in epoch    377      batch 0\n",
      "RLoss: 766.28857421875\n",
      "current in epoch    377      batch 1\n",
      "RLoss: 191.24276733398438\n",
      "current in epoch    377      batch 2\n",
      "RLoss: 638.2689819335938\n",
      "current in epoch    377      batch 3\n",
      "RLoss: 131.64187622070312\n",
      "current in epoch    377      batch 4\n",
      "RLoss: 56.99268341064453\n",
      "current in epoch    377      batch 5\n",
      "RLoss: 43.7686767578125\n",
      "========================================\n",
      "Epoch 378/1000 - partial_train_loss: 331.5670 \n",
      "Epoch: [378/1000], TrainLoss: 43.50202253886631\n",
      "training Loss has not improved for 101 epochs.\n",
      "current in epoch    378      batch 0\n",
      "RLoss: 116.51692199707031\n",
      "current in epoch    378      batch 1\n",
      "RLoss: 277.9810791015625\n",
      "current in epoch    378      batch 2\n",
      "RLoss: 218.31353759765625\n",
      "current in epoch    378      batch 3\n",
      "RLoss: 335.0198974609375\n",
      "current in epoch    378      batch 4\n",
      "RLoss: 65.82006072998047\n",
      "current in epoch    378      batch 5\n",
      "RLoss: 374.4857482910156\n",
      "========================================\n",
      "Epoch 379/1000 - partial_train_loss: 165.6932 \n",
      "Epoch: [379/1000], TrainLoss: 584.5005689348493\n",
      "training Loss has not improved for 102 epochs.\n",
      "current in epoch    379      batch 0\n",
      "RLoss: 57.55250549316406\n",
      "current in epoch    379      batch 1\n",
      "RLoss: 68.42103576660156\n",
      "current in epoch    379      batch 2\n",
      "RLoss: 46.00373077392578\n",
      "current in epoch    379      batch 3\n",
      "RLoss: 52.90115737915039\n",
      "current in epoch    379      batch 4\n",
      "RLoss: 511.97698974609375\n",
      "current in epoch    379      batch 5\n",
      "RLoss: 26.727771759033203\n",
      "========================================\n",
      "Epoch 380/1000 - partial_train_loss: 181.8057 \n",
      "Epoch: [380/1000], TrainLoss: 31.637888022831508\n",
      "training Loss has not improved for 103 epochs.\n",
      "current in epoch    380      batch 0\n",
      "RLoss: 410.53564453125\n",
      "current in epoch    380      batch 1\n",
      "RLoss: 38.638572692871094\n",
      "current in epoch    380      batch 2\n",
      "RLoss: 104.27831268310547\n",
      "current in epoch    380      batch 3\n",
      "RLoss: 27.024213790893555\n",
      "current in epoch    380      batch 4\n",
      "RLoss: 614.3070678710938\n",
      "current in epoch    380      batch 5\n",
      "RLoss: 51.15666580200195\n",
      "========================================\n",
      "Epoch 381/1000 - partial_train_loss: 155.6057 \n",
      "sorting training set\n",
      "Epoch 381/1000 - Training loss: 68.3099 \n",
      "========================================\n",
      "Epoch: [381/1000], TrainLoss: 72.89874752544475\n",
      "training Loss has not improved for 104 epochs.\n",
      "current in epoch    381      batch 0\n",
      "RLoss: 38.88157653808594\n",
      "current in epoch    381      batch 1\n",
      "RLoss: 253.97293090820312\n",
      "current in epoch    381      batch 2\n",
      "RLoss: 149.07223510742188\n",
      "current in epoch    381      batch 3\n",
      "RLoss: 12.996045112609863\n",
      "current in epoch    381      batch 4\n",
      "RLoss: 6.974108695983887\n",
      "current in epoch    381      batch 5\n",
      "RLoss: 3.8060150146484375\n",
      "========================================\n",
      "Epoch 382/1000 - partial_train_loss: 152.7004 \n",
      "Epoch: [382/1000], TrainLoss: 7.095905218805585\n",
      "training Loss has not improved for 105 epochs.\n",
      "current in epoch    382      batch 0\n",
      "RLoss: 23.2456111907959\n",
      "current in epoch    382      batch 1\n",
      "RLoss: 85.57373046875\n",
      "current in epoch    382      batch 2\n",
      "RLoss: 45.28403091430664\n",
      "current in epoch    382      batch 3\n",
      "RLoss: 10.080114364624023\n",
      "current in epoch    382      batch 4\n",
      "RLoss: 87.15631103515625\n",
      "current in epoch    382      batch 5\n",
      "RLoss: 11.377799987792969\n",
      "========================================\n",
      "Epoch 383/1000 - partial_train_loss: 48.5580 \n",
      "Epoch: [383/1000], TrainLoss: 9.253037946564811\n",
      "training Loss has not improved for 106 epochs.\n",
      "current in epoch    383      batch 0\n",
      "RLoss: 6.825767993927002\n",
      "current in epoch    383      batch 1\n",
      "RLoss: 50.94035720825195\n",
      "current in epoch    383      batch 2\n",
      "RLoss: 96.60997772216797\n",
      "current in epoch    383      batch 3\n",
      "RLoss: 36.88764572143555\n",
      "current in epoch    383      batch 4\n",
      "RLoss: 11.488556861877441\n",
      "current in epoch    383      batch 5\n",
      "RLoss: 7.573129177093506\n",
      "========================================\n",
      "Epoch 384/1000 - partial_train_loss: 33.0534 \n",
      "Epoch: [384/1000], TrainLoss: 6.218312348638262\n",
      "training Loss has not improved for 107 epochs.\n",
      "current in epoch    384      batch 0\n",
      "RLoss: 812.1279907226562\n",
      "current in epoch    384      batch 1\n",
      "RLoss: 24.605680465698242\n",
      "current in epoch    384      batch 2\n",
      "RLoss: 197.12423706054688\n",
      "current in epoch    384      batch 3\n",
      "RLoss: 373.8016052246094\n",
      "current in epoch    384      batch 4\n",
      "RLoss: 678.0335083007812\n",
      "current in epoch    384      batch 5\n",
      "RLoss: 29.196964263916016\n",
      "========================================\n",
      "Epoch 385/1000 - partial_train_loss: 279.5109 \n",
      "Epoch: [385/1000], TrainLoss: 31.90163360323225\n",
      "training Loss has not improved for 108 epochs.\n",
      "current in epoch    385      batch 0\n",
      "RLoss: 427.6632385253906\n",
      "current in epoch    385      batch 1\n",
      "RLoss: 174.72171020507812\n",
      "current in epoch    385      batch 2\n",
      "RLoss: 138.08738708496094\n",
      "current in epoch    385      batch 3\n",
      "RLoss: 28.32200813293457\n",
      "current in epoch    385      batch 4\n",
      "RLoss: 14.340041160583496\n",
      "current in epoch    385      batch 5\n",
      "RLoss: 44.55832290649414\n",
      "========================================\n",
      "Epoch 386/1000 - partial_train_loss: 107.1209 \n",
      "sorting training set\n",
      "Epoch 386/1000 - Training loss: 31.3176 \n",
      "========================================\n",
      "Epoch: [386/1000], TrainLoss: 33.62786145486645\n",
      "training Loss has not improved for 109 epochs.\n",
      "current in epoch    386      batch 0\n",
      "RLoss: 536.1782836914062\n",
      "current in epoch    386      batch 1\n",
      "RLoss: 550.2543334960938\n",
      "current in epoch    386      batch 2\n",
      "RLoss: 517.36083984375\n",
      "current in epoch    386      batch 3\n",
      "RLoss: 97.38072204589844\n",
      "current in epoch    386      batch 4\n",
      "RLoss: 169.90200805664062\n",
      "current in epoch    386      batch 5\n",
      "RLoss: 100.84364318847656\n",
      "========================================\n",
      "Epoch 387/1000 - partial_train_loss: 290.0532 \n",
      "Epoch: [387/1000], TrainLoss: 80.17764486585345\n",
      "training Loss has not improved for 110 epochs.\n",
      "current in epoch    387      batch 0\n",
      "RLoss: 173.27444458007812\n",
      "current in epoch    387      batch 1\n",
      "RLoss: 114.4400634765625\n",
      "current in epoch    387      batch 2\n",
      "RLoss: 140.9793701171875\n",
      "current in epoch    387      batch 3\n",
      "RLoss: 8.366792678833008\n",
      "current in epoch    387      batch 4\n",
      "RLoss: 68.2535171508789\n",
      "current in epoch    387      batch 5\n",
      "RLoss: 85.62420654296875\n",
      "========================================\n",
      "Epoch 388/1000 - partial_train_loss: 95.6017 \n",
      "Epoch: [388/1000], TrainLoss: 93.51700823647636\n",
      "training Loss has not improved for 111 epochs.\n",
      "current in epoch    388      batch 0\n",
      "RLoss: 38.614234924316406\n",
      "current in epoch    388      batch 1\n",
      "RLoss: 12.198038101196289\n",
      "current in epoch    388      batch 2\n",
      "RLoss: 390.0248718261719\n",
      "current in epoch    388      batch 3\n",
      "RLoss: 412.08782958984375\n",
      "current in epoch    388      batch 4\n",
      "RLoss: 40.78671646118164\n",
      "current in epoch    388      batch 5\n",
      "RLoss: 102.17012023925781\n",
      "========================================\n",
      "Epoch 389/1000 - partial_train_loss: 151.4874 \n",
      "Epoch: [389/1000], TrainLoss: 197.44715717860632\n",
      "training Loss has not improved for 112 epochs.\n",
      "current in epoch    389      batch 0\n",
      "RLoss: 3303.086669921875\n",
      "current in epoch    389      batch 1\n",
      "RLoss: 1021.147216796875\n",
      "current in epoch    389      batch 2\n",
      "RLoss: 215.79107666015625\n",
      "current in epoch    389      batch 3\n",
      "RLoss: 25.852008819580078\n",
      "current in epoch    389      batch 4\n",
      "RLoss: 42.95067596435547\n",
      "current in epoch    389      batch 5\n",
      "RLoss: 119.6480484008789\n",
      "========================================\n",
      "Epoch 390/1000 - partial_train_loss: 695.0974 \n",
      "Epoch: [390/1000], TrainLoss: 97.98492813110352\n",
      "training Loss has not improved for 113 epochs.\n",
      "current in epoch    390      batch 0\n",
      "RLoss: 8.370108604431152\n",
      "current in epoch    390      batch 1\n",
      "RLoss: 166.11473083496094\n",
      "current in epoch    390      batch 2\n",
      "RLoss: 52.819427490234375\n",
      "current in epoch    390      batch 3\n",
      "RLoss: 16.873266220092773\n",
      "current in epoch    390      batch 4\n",
      "RLoss: 64.58380126953125\n",
      "current in epoch    390      batch 5\n",
      "RLoss: 6.280338287353516\n",
      "========================================\n",
      "Epoch 391/1000 - partial_train_loss: 77.7252 \n",
      "sorting training set\n",
      "Epoch 391/1000 - Training loss: 33.0872 \n",
      "========================================\n",
      "Epoch: [391/1000], TrainLoss: 34.65496060656906\n",
      "training Loss has not improved for 114 epochs.\n",
      "current in epoch    391      batch 0\n",
      "RLoss: 102.91951751708984\n",
      "current in epoch    391      batch 1\n",
      "RLoss: 45.78995132446289\n",
      "current in epoch    391      batch 2\n",
      "RLoss: 43.07721710205078\n",
      "current in epoch    391      batch 3\n",
      "RLoss: 49.86913299560547\n",
      "current in epoch    391      batch 4\n",
      "RLoss: 8.569707870483398\n",
      "current in epoch    391      batch 5\n",
      "RLoss: 6.2502970695495605\n",
      "========================================\n",
      "Epoch 392/1000 - partial_train_loss: 76.3370 \n",
      "Epoch: [392/1000], TrainLoss: 8.918115598814827\n",
      "training Loss has not improved for 115 epochs.\n",
      "current in epoch    392      batch 0\n",
      "RLoss: 17.547672271728516\n",
      "current in epoch    392      batch 1\n",
      "RLoss: 40.430633544921875\n",
      "current in epoch    392      batch 2\n",
      "RLoss: 12.676023483276367\n",
      "current in epoch    392      batch 3\n",
      "RLoss: 40.265079498291016\n",
      "current in epoch    392      batch 4\n",
      "RLoss: 31.009363174438477\n",
      "current in epoch    392      batch 5\n",
      "RLoss: 113.17960357666016\n",
      "========================================\n",
      "Epoch 393/1000 - partial_train_loss: 23.0176 \n",
      "Epoch: [393/1000], TrainLoss: 86.07253919328961\n",
      "training Loss has not improved for 116 epochs.\n",
      "current in epoch    393      batch 0\n",
      "RLoss: 172.55677795410156\n",
      "current in epoch    393      batch 1\n",
      "RLoss: 24.3072452545166\n",
      "current in epoch    393      batch 2\n",
      "RLoss: 18.36839485168457\n",
      "current in epoch    393      batch 3\n",
      "RLoss: 22.627830505371094\n",
      "current in epoch    393      batch 4\n",
      "RLoss: 5.571561336517334\n",
      "current in epoch    393      batch 5\n",
      "RLoss: 35.222843170166016\n",
      "========================================\n",
      "Epoch 394/1000 - partial_train_loss: 38.1536 \n",
      "Epoch: [394/1000], TrainLoss: 37.04312719617571\n",
      "training Loss has not improved for 117 epochs.\n",
      "current in epoch    394      batch 0\n",
      "RLoss: 36.94823455810547\n",
      "current in epoch    394      batch 1\n",
      "RLoss: 36.61274337768555\n",
      "current in epoch    394      batch 2\n",
      "RLoss: 28.992874145507812\n",
      "current in epoch    394      batch 3\n",
      "RLoss: 3.4018964767456055\n",
      "current in epoch    394      batch 4\n",
      "RLoss: 217.9020538330078\n",
      "current in epoch    394      batch 5\n",
      "RLoss: 46.97052001953125\n",
      "========================================\n",
      "Epoch 395/1000 - partial_train_loss: 53.9628 \n",
      "Epoch: [395/1000], TrainLoss: 52.43258735111782\n",
      "training Loss has not improved for 118 epochs.\n",
      "current in epoch    395      batch 0\n",
      "RLoss: 347.5309143066406\n",
      "current in epoch    395      batch 1\n",
      "RLoss: 36.432621002197266\n",
      "current in epoch    395      batch 2\n",
      "RLoss: 25.54615592956543\n",
      "current in epoch    395      batch 3\n",
      "RLoss: 13.379504203796387\n",
      "current in epoch    395      batch 4\n",
      "RLoss: 24.042564392089844\n",
      "current in epoch    395      batch 5\n",
      "RLoss: 194.36843872070312\n",
      "========================================\n",
      "Epoch 396/1000 - partial_train_loss: 76.6987 \n",
      "sorting training set\n",
      "Epoch 396/1000 - Training loss: 231.5132 \n",
      "========================================\n",
      "Epoch: [396/1000], TrainLoss: 247.11479831879373\n",
      "training Loss has not improved for 119 epochs.\n",
      "current in epoch    396      batch 0\n",
      "RLoss: 94.24211120605469\n",
      "current in epoch    396      batch 1\n",
      "RLoss: 980.4660034179688\n",
      "current in epoch    396      batch 2\n",
      "RLoss: 57.198787689208984\n",
      "current in epoch    396      batch 3\n",
      "RLoss: 2.1325244903564453\n",
      "current in epoch    396      batch 4\n",
      "RLoss: 77.86929321289062\n",
      "current in epoch    396      batch 5\n",
      "RLoss: 4.122623920440674\n",
      "========================================\n",
      "Epoch 397/1000 - partial_train_loss: 255.2051 \n",
      "Epoch: [397/1000], TrainLoss: 9.681287782532829\n",
      "training Loss has not improved for 120 epochs.\n",
      "current in epoch    397      batch 0\n",
      "RLoss: 494.529052734375\n",
      "current in epoch    397      batch 1\n",
      "RLoss: 12.089900970458984\n",
      "current in epoch    397      batch 2\n",
      "RLoss: 121.81474304199219\n",
      "current in epoch    397      batch 3\n",
      "RLoss: 324.627197265625\n",
      "current in epoch    397      batch 4\n",
      "RLoss: 36.015220642089844\n",
      "current in epoch    397      batch 5\n",
      "RLoss: 65.43433380126953\n",
      "========================================\n",
      "Epoch 398/1000 - partial_train_loss: 142.4957 \n",
      "Epoch: [398/1000], TrainLoss: 72.90838963644845\n",
      "training Loss has not improved for 121 epochs.\n",
      "current in epoch    398      batch 0\n",
      "RLoss: 1580.9949951171875\n",
      "current in epoch    398      batch 1\n",
      "RLoss: 413.1612548828125\n",
      "current in epoch    398      batch 2\n",
      "RLoss: 304.2076110839844\n",
      "current in epoch    398      batch 3\n",
      "RLoss: 32.75056457519531\n",
      "current in epoch    398      batch 4\n",
      "RLoss: 33.326866149902344\n",
      "current in epoch    398      batch 5\n",
      "RLoss: 2.695997714996338\n",
      "========================================\n",
      "Epoch 399/1000 - partial_train_loss: 417.4192 \n",
      "Epoch: [399/1000], TrainLoss: 2.8789889173848286\n",
      "training Loss has not improved for 122 epochs.\n",
      "current in epoch    399      batch 0\n",
      "RLoss: 53.610286712646484\n",
      "current in epoch    399      batch 1\n",
      "RLoss: 188.04539489746094\n",
      "current in epoch    399      batch 2\n",
      "RLoss: 289.7207336425781\n",
      "current in epoch    399      batch 3\n",
      "RLoss: 66.82415008544922\n",
      "current in epoch    399      batch 4\n",
      "RLoss: 68.07698059082031\n",
      "current in epoch    399      batch 5\n",
      "RLoss: 49.750186920166016\n",
      "========================================\n",
      "Epoch 400/1000 - partial_train_loss: 97.5787 \n",
      "Epoch: [400/1000], TrainLoss: 47.004726205553325\n",
      "training Loss has not improved for 123 epochs.\n",
      "current in epoch    400      batch 0\n",
      "RLoss: 223.98599243164062\n",
      "current in epoch    400      batch 1\n",
      "RLoss: 210.89134216308594\n",
      "current in epoch    400      batch 2\n",
      "RLoss: 75.32234191894531\n",
      "current in epoch    400      batch 3\n",
      "RLoss: 198.34388732910156\n",
      "current in epoch    400      batch 4\n",
      "RLoss: 351.5529479980469\n",
      "current in epoch    400      batch 5\n",
      "RLoss: 29.273439407348633\n",
      "========================================\n",
      "Epoch 401/1000 - partial_train_loss: 189.8615 \n",
      "sorting training set\n",
      "Epoch 401/1000 - Training loss: 25.0013 \n",
      "========================================\n",
      "Epoch: [401/1000], TrainLoss: 27.64718811756448\n",
      "training Loss has not improved for 124 epochs.\n",
      "current in epoch    401      batch 0\n",
      "RLoss: 271.66461181640625\n",
      "current in epoch    401      batch 1\n",
      "RLoss: 7.059451580047607\n",
      "current in epoch    401      batch 2\n",
      "RLoss: 68.82811737060547\n",
      "current in epoch    401      batch 3\n",
      "RLoss: 10.568278312683105\n",
      "current in epoch    401      batch 4\n",
      "RLoss: 151.2691192626953\n",
      "current in epoch    401      batch 5\n",
      "RLoss: 232.25643920898438\n",
      "========================================\n",
      "Epoch 402/1000 - partial_train_loss: 102.5275 \n",
      "Epoch: [402/1000], TrainLoss: 194.43162754603796\n",
      "training Loss has not improved for 125 epochs.\n",
      "current in epoch    402      batch 0\n",
      "RLoss: 133.26768493652344\n",
      "current in epoch    402      batch 1\n",
      "RLoss: 51.73843765258789\n",
      "current in epoch    402      batch 2\n",
      "RLoss: 24.002042770385742\n",
      "current in epoch    402      batch 3\n",
      "RLoss: 69.24327850341797\n",
      "current in epoch    402      batch 4\n",
      "RLoss: 107.7918472290039\n",
      "current in epoch    402      batch 5\n",
      "RLoss: 38.2693977355957\n",
      "========================================\n",
      "Epoch 403/1000 - partial_train_loss: 136.0259 \n",
      "Epoch: [403/1000], TrainLoss: 30.33021923473903\n",
      "training Loss has not improved for 126 epochs.\n",
      "current in epoch    403      batch 0\n",
      "RLoss: 172.35935974121094\n",
      "current in epoch    403      batch 1\n",
      "RLoss: 14.449199676513672\n",
      "current in epoch    403      batch 2\n",
      "RLoss: 190.28680419921875\n",
      "current in epoch    403      batch 3\n",
      "RLoss: 26.094463348388672\n",
      "current in epoch    403      batch 4\n",
      "RLoss: 47.34397506713867\n",
      "current in epoch    403      batch 5\n",
      "RLoss: 12.14091968536377\n",
      "========================================\n",
      "Epoch 404/1000 - partial_train_loss: 80.0906 \n",
      "Epoch: [404/1000], TrainLoss: 22.63712998798915\n",
      "training Loss has not improved for 127 epochs.\n",
      "current in epoch    404      batch 0\n",
      "RLoss: 73.50721740722656\n",
      "current in epoch    404      batch 1\n",
      "RLoss: 57.67949295043945\n",
      "current in epoch    404      batch 2\n",
      "RLoss: 578.712158203125\n",
      "current in epoch    404      batch 3\n",
      "RLoss: 1223.3695068359375\n",
      "current in epoch    404      batch 4\n",
      "RLoss: 42.141563415527344\n",
      "current in epoch    404      batch 5\n",
      "RLoss: 40.93834686279297\n",
      "========================================\n",
      "Epoch 405/1000 - partial_train_loss: 317.4268 \n",
      "Epoch: [405/1000], TrainLoss: 33.06407512937273\n",
      "training Loss has not improved for 128 epochs.\n",
      "current in epoch    405      batch 0\n",
      "RLoss: 99.0010986328125\n",
      "current in epoch    405      batch 1\n",
      "RLoss: 87.84947967529297\n",
      "current in epoch    405      batch 2\n",
      "RLoss: 7.781331539154053\n",
      "current in epoch    405      batch 3\n",
      "RLoss: 24.982585906982422\n",
      "current in epoch    405      batch 4\n",
      "RLoss: 44.802947998046875\n",
      "current in epoch    405      batch 5\n",
      "RLoss: 381.6021423339844\n",
      "========================================\n",
      "Epoch 406/1000 - partial_train_loss: 49.6716 \n",
      "sorting training set\n",
      "Epoch 406/1000 - Training loss: 304.9431 \n",
      "========================================\n",
      "Epoch: [406/1000], TrainLoss: 325.58548652790853\n",
      "training Loss has not improved for 129 epochs.\n",
      "current in epoch    406      batch 0\n",
      "RLoss: 52.40418243408203\n",
      "current in epoch    406      batch 1\n",
      "RLoss: 376.7276306152344\n",
      "current in epoch    406      batch 2\n",
      "RLoss: 137.21517944335938\n",
      "current in epoch    406      batch 3\n",
      "RLoss: 20.69933319091797\n",
      "current in epoch    406      batch 4\n",
      "RLoss: 33.645423889160156\n",
      "current in epoch    406      batch 5\n",
      "RLoss: 142.686279296875\n",
      "========================================\n",
      "Epoch 407/1000 - partial_train_loss: 246.7903 \n",
      "Epoch: [407/1000], TrainLoss: 153.99186434064592\n",
      "training Loss has not improved for 130 epochs.\n",
      "current in epoch    407      batch 0\n",
      "RLoss: 129.5548858642578\n",
      "current in epoch    407      batch 1\n",
      "RLoss: 25.21573829650879\n",
      "current in epoch    407      batch 2\n",
      "RLoss: 6.264166831970215\n",
      "current in epoch    407      batch 3\n",
      "RLoss: 91.08818817138672\n",
      "current in epoch    407      batch 4\n",
      "RLoss: 11.495880126953125\n",
      "current in epoch    407      batch 5\n",
      "RLoss: 11.800745964050293\n",
      "========================================\n",
      "Epoch 408/1000 - partial_train_loss: 72.4789 \n",
      "Epoch: [408/1000], TrainLoss: 19.008622441973007\n",
      "training Loss has not improved for 131 epochs.\n",
      "current in epoch    408      batch 0\n",
      "RLoss: 195.4025115966797\n",
      "current in epoch    408      batch 1\n",
      "RLoss: 312.5917053222656\n",
      "current in epoch    408      batch 2\n",
      "RLoss: 43.637081146240234\n",
      "current in epoch    408      batch 3\n",
      "RLoss: 76.78665161132812\n",
      "current in epoch    408      batch 4\n",
      "RLoss: 44.7619743347168\n",
      "current in epoch    408      batch 5\n",
      "RLoss: 68.33122253417969\n",
      "========================================\n",
      "Epoch 409/1000 - partial_train_loss: 94.2154 \n",
      "Epoch: [409/1000], TrainLoss: 194.17665508815222\n",
      "training Loss has not improved for 132 epochs.\n",
      "current in epoch    409      batch 0\n",
      "RLoss: 180.5209197998047\n",
      "current in epoch    409      batch 1\n",
      "RLoss: 621.4367065429688\n",
      "current in epoch    409      batch 2\n",
      "RLoss: 73.74981689453125\n",
      "current in epoch    409      batch 3\n",
      "RLoss: 241.04132080078125\n",
      "current in epoch    409      batch 4\n",
      "RLoss: 511.9570617675781\n",
      "current in epoch    409      batch 5\n",
      "RLoss: 242.86839294433594\n",
      "========================================\n",
      "Epoch 410/1000 - partial_train_loss: 255.2350 \n",
      "Epoch: [410/1000], TrainLoss: 169.50007847377233\n",
      "training Loss has not improved for 133 epochs.\n",
      "current in epoch    410      batch 0\n",
      "RLoss: 51.42466735839844\n",
      "current in epoch    410      batch 1\n",
      "RLoss: 36.60174560546875\n",
      "current in epoch    410      batch 2\n",
      "RLoss: 625.4628295898438\n",
      "current in epoch    410      batch 3\n",
      "RLoss: 671.32275390625\n",
      "current in epoch    410      batch 4\n",
      "RLoss: 402.2090148925781\n",
      "current in epoch    410      batch 5\n",
      "RLoss: 23.8780574798584\n",
      "========================================\n",
      "Epoch 411/1000 - partial_train_loss: 308.2528 \n",
      "sorting training set\n",
      "Epoch 411/1000 - Training loss: 88.6807 \n",
      "========================================\n",
      "Epoch: [411/1000], TrainLoss: 94.73644133772294\n",
      "training Loss has not improved for 134 epochs.\n",
      "current in epoch    411      batch 0\n",
      "RLoss: 56.8874626159668\n",
      "current in epoch    411      batch 1\n",
      "RLoss: 20.86934471130371\n",
      "current in epoch    411      batch 2\n",
      "RLoss: 44.078311920166016\n",
      "current in epoch    411      batch 3\n",
      "RLoss: 135.06088256835938\n",
      "current in epoch    411      batch 4\n",
      "RLoss: 20.765960693359375\n",
      "current in epoch    411      batch 5\n",
      "RLoss: 5.438127517700195\n",
      "========================================\n",
      "Epoch 412/1000 - partial_train_loss: 122.1650 \n",
      "Epoch: [412/1000], TrainLoss: 19.944954557078226\n",
      "training Loss has not improved for 135 epochs.\n",
      "current in epoch    412      batch 0\n",
      "RLoss: 207.058837890625\n",
      "current in epoch    412      batch 1\n",
      "RLoss: 206.43919372558594\n",
      "current in epoch    412      batch 2\n",
      "RLoss: 131.9173126220703\n",
      "current in epoch    412      batch 3\n",
      "RLoss: 2.6941511631011963\n",
      "current in epoch    412      batch 4\n",
      "RLoss: 22.782867431640625\n",
      "current in epoch    412      batch 5\n",
      "RLoss: 14.072662353515625\n",
      "========================================\n",
      "Epoch 413/1000 - partial_train_loss: 97.2004 \n",
      "Epoch: [413/1000], TrainLoss: 36.64422927583967\n",
      "training Loss has not improved for 136 epochs.\n",
      "current in epoch    413      batch 0\n",
      "RLoss: 187.3616943359375\n",
      "current in epoch    413      batch 1\n",
      "RLoss: 137.4283905029297\n",
      "current in epoch    413      batch 2\n",
      "RLoss: 96.5367431640625\n",
      "current in epoch    413      batch 3\n",
      "RLoss: 47.91524124145508\n",
      "current in epoch    413      batch 4\n",
      "RLoss: 97.99827575683594\n",
      "current in epoch    413      batch 5\n",
      "RLoss: 61.1224365234375\n",
      "========================================\n",
      "Epoch 414/1000 - partial_train_loss: 84.2988 \n",
      "Epoch: [414/1000], TrainLoss: 59.48828111376081\n",
      "training Loss has not improved for 137 epochs.\n",
      "current in epoch    414      batch 0\n",
      "RLoss: 305.0078125\n",
      "current in epoch    414      batch 1\n",
      "RLoss: 48.924312591552734\n",
      "current in epoch    414      batch 2\n",
      "RLoss: 128.79754638671875\n",
      "current in epoch    414      batch 3\n",
      "RLoss: 44.126304626464844\n",
      "current in epoch    414      batch 4\n",
      "RLoss: 721.842041015625\n",
      "current in epoch    414      batch 5\n",
      "RLoss: 9.026092529296875\n",
      "========================================\n",
      "Epoch 415/1000 - partial_train_loss: 209.1990 \n",
      "Epoch: [415/1000], TrainLoss: 7.748414703777859\n",
      "training Loss has not improved for 138 epochs.\n",
      "current in epoch    415      batch 0\n",
      "RLoss: 25.32549285888672\n",
      "current in epoch    415      batch 1\n",
      "RLoss: 56.96411895751953\n",
      "current in epoch    415      batch 2\n",
      "RLoss: 7.270168781280518\n",
      "current in epoch    415      batch 3\n",
      "RLoss: 18.253225326538086\n",
      "current in epoch    415      batch 4\n",
      "RLoss: 46.946571350097656\n",
      "current in epoch    415      batch 5\n",
      "RLoss: 18.531911849975586\n",
      "========================================\n",
      "Epoch 416/1000 - partial_train_loss: 28.1167 \n",
      "sorting training set\n",
      "Epoch 416/1000 - Training loss: 77.8082 \n",
      "========================================\n",
      "Epoch: [416/1000], TrainLoss: 83.44020767525087\n",
      "training Loss has not improved for 139 epochs.\n",
      "current in epoch    416      batch 0\n",
      "RLoss: 972.191162109375\n",
      "current in epoch    416      batch 1\n",
      "RLoss: 373.7174377441406\n",
      "current in epoch    416      batch 2\n",
      "RLoss: 41.34468460083008\n",
      "current in epoch    416      batch 3\n",
      "RLoss: 1730.931640625\n",
      "current in epoch    416      batch 4\n",
      "RLoss: 34.60744094848633\n",
      "current in epoch    416      batch 5\n",
      "RLoss: 3178.042236328125\n",
      "========================================\n",
      "Epoch 417/1000 - partial_train_loss: 591.2429 \n",
      "Epoch: [417/1000], TrainLoss: 2281.6350708007812\n",
      "training Loss has not improved for 140 epochs.\n",
      "current in epoch    417      batch 0\n",
      "RLoss: 883.1209106445312\n",
      "current in epoch    417      batch 1\n",
      "RLoss: 320.8159484863281\n",
      "current in epoch    417      batch 2\n",
      "RLoss: 12.130139350891113\n",
      "current in epoch    417      batch 3\n",
      "RLoss: 20.444110870361328\n",
      "current in epoch    417      batch 4\n",
      "RLoss: 270.28369140625\n",
      "current in epoch    417      batch 5\n",
      "RLoss: 127.85502624511719\n",
      "========================================\n",
      "Epoch 418/1000 - partial_train_loss: 828.9070 \n",
      "Epoch: [418/1000], TrainLoss: 98.93853051321847\n",
      "training Loss has not improved for 141 epochs.\n",
      "current in epoch    418      batch 0\n",
      "RLoss: 343.57476806640625\n",
      "current in epoch    418      batch 1\n",
      "RLoss: 771.6394653320312\n",
      "current in epoch    418      batch 2\n",
      "RLoss: 121.27062225341797\n",
      "current in epoch    418      batch 3\n",
      "RLoss: 169.30165100097656\n",
      "current in epoch    418      batch 4\n",
      "RLoss: 992.514404296875\n",
      "current in epoch    418      batch 5\n",
      "RLoss: 121.72928619384766\n",
      "========================================\n",
      "Epoch 419/1000 - partial_train_loss: 434.7576 \n",
      "Epoch: [419/1000], TrainLoss: 190.9297945158822\n",
      "training Loss has not improved for 142 epochs.\n",
      "current in epoch    419      batch 0\n",
      "RLoss: 1661.67919921875\n",
      "current in epoch    419      batch 1\n",
      "RLoss: 148.99319458007812\n",
      "current in epoch    419      batch 2\n",
      "RLoss: 2926.28173828125\n",
      "current in epoch    419      batch 3\n",
      "RLoss: 1342.6942138671875\n",
      "current in epoch    419      batch 4\n",
      "RLoss: 459.9991760253906\n",
      "current in epoch    419      batch 5\n",
      "RLoss: 99.9553451538086\n",
      "========================================\n",
      "Epoch 420/1000 - partial_train_loss: 1181.7756 \n",
      "Epoch: [420/1000], TrainLoss: 40.99235132762364\n",
      "training Loss has not improved for 143 epochs.\n",
      "current in epoch    420      batch 0\n",
      "RLoss: 647.7548828125\n",
      "current in epoch    420      batch 1\n",
      "RLoss: 270.97747802734375\n",
      "current in epoch    420      batch 2\n",
      "RLoss: 106.06387329101562\n",
      "current in epoch    420      batch 3\n",
      "RLoss: 150.11810302734375\n",
      "current in epoch    420      batch 4\n",
      "RLoss: 1718.791748046875\n",
      "current in epoch    420      batch 5\n",
      "RLoss: 51.36799621582031\n",
      "========================================\n",
      "Epoch 421/1000 - partial_train_loss: 450.6464 \n",
      "sorting training set\n",
      "Epoch 421/1000 - Training loss: 84.0616 \n",
      "========================================\n",
      "Epoch: [421/1000], TrainLoss: 89.81451194350305\n",
      "training Loss has not improved for 144 epochs.\n",
      "current in epoch    421      batch 0\n",
      "RLoss: 695.5884399414062\n",
      "current in epoch    421      batch 1\n",
      "RLoss: 89.52687072753906\n",
      "current in epoch    421      batch 2\n",
      "RLoss: 185.13682556152344\n",
      "current in epoch    421      batch 3\n",
      "RLoss: 452.3806457519531\n",
      "current in epoch    421      batch 4\n",
      "RLoss: 7.3616509437561035\n",
      "current in epoch    421      batch 5\n",
      "RLoss: 61.476356506347656\n",
      "========================================\n",
      "Epoch 422/1000 - partial_train_loss: 253.8350 \n",
      "Epoch: [422/1000], TrainLoss: 251.3083393914359\n",
      "training Loss has not improved for 145 epochs.\n",
      "current in epoch    422      batch 0\n",
      "RLoss: 321.3841552734375\n",
      "current in epoch    422      batch 1\n",
      "RLoss: 41.892578125\n",
      "current in epoch    422      batch 2\n",
      "RLoss: 61.216068267822266\n",
      "current in epoch    422      batch 3\n",
      "RLoss: 3.596069812774658\n",
      "current in epoch    422      batch 4\n",
      "RLoss: 3.7253379821777344\n",
      "current in epoch    422      batch 5\n",
      "RLoss: 23.357580184936523\n",
      "========================================\n",
      "Epoch 423/1000 - partial_train_loss: 67.2632 \n",
      "Epoch: [423/1000], TrainLoss: 40.428018297467915\n",
      "training Loss has not improved for 146 epochs.\n",
      "current in epoch    423      batch 0\n",
      "RLoss: 11.083681106567383\n",
      "current in epoch    423      batch 1\n",
      "RLoss: 71.21473693847656\n",
      "current in epoch    423      batch 2\n",
      "RLoss: 205.00189208984375\n",
      "current in epoch    423      batch 3\n",
      "RLoss: 219.30477905273438\n",
      "current in epoch    423      batch 4\n",
      "RLoss: 162.9801788330078\n",
      "current in epoch    423      batch 5\n",
      "RLoss: 90.94379425048828\n",
      "========================================\n",
      "Epoch 424/1000 - partial_train_loss: 106.6042 \n",
      "Epoch: [424/1000], TrainLoss: 86.74387277875628\n",
      "training Loss has not improved for 147 epochs.\n",
      "current in epoch    424      batch 0\n",
      "RLoss: 203.39617919921875\n",
      "current in epoch    424      batch 1\n",
      "RLoss: 13.518268585205078\n",
      "current in epoch    424      batch 2\n",
      "RLoss: 345.1041259765625\n",
      "current in epoch    424      batch 3\n",
      "RLoss: 3.204683303833008\n",
      "current in epoch    424      batch 4\n",
      "RLoss: 91.56719207763672\n",
      "current in epoch    424      batch 5\n",
      "RLoss: 7.254532814025879\n",
      "========================================\n",
      "Epoch 425/1000 - partial_train_loss: 155.6310 \n",
      "Epoch: [425/1000], TrainLoss: 9.217428292546954\n",
      "training Loss has not improved for 148 epochs.\n",
      "current in epoch    425      batch 0\n",
      "RLoss: 194.4912109375\n",
      "current in epoch    425      batch 1\n",
      "RLoss: 73.32881927490234\n",
      "current in epoch    425      batch 2\n",
      "RLoss: 11.879517555236816\n",
      "current in epoch    425      batch 3\n",
      "RLoss: 2.6889617443084717\n",
      "current in epoch    425      batch 4\n",
      "RLoss: 8.010499000549316\n",
      "current in epoch    425      batch 5\n",
      "RLoss: 63.269920349121094\n",
      "========================================\n",
      "Epoch 426/1000 - partial_train_loss: 37.7008 \n",
      "sorting training set\n",
      "Epoch 426/1000 - Training loss: 134.5370 \n",
      "========================================\n",
      "Epoch: [426/1000], TrainLoss: 143.82022140554392\n",
      "training Loss has not improved for 149 epochs.\n",
      "current in epoch    426      batch 0\n",
      "RLoss: 23.261030197143555\n",
      "current in epoch    426      batch 1\n",
      "RLoss: 214.62403869628906\n",
      "current in epoch    426      batch 2\n",
      "RLoss: 2351.580810546875\n",
      "current in epoch    426      batch 3\n",
      "RLoss: 1137.6497802734375\n",
      "current in epoch    426      batch 4\n",
      "RLoss: 28.38759422302246\n",
      "current in epoch    426      batch 5\n",
      "RLoss: 16.383930206298828\n",
      "========================================\n",
      "Epoch 427/1000 - partial_train_loss: 747.7490 \n",
      "Epoch: [427/1000], TrainLoss: 30.105631044932775\n",
      "training Loss has not improved for 150 epochs.\n",
      "current in epoch    427      batch 0\n",
      "RLoss: 1619.8897705078125\n",
      "current in epoch    427      batch 1\n",
      "RLoss: 79.29271697998047\n",
      "current in epoch    427      batch 2\n",
      "RLoss: 32.52135467529297\n",
      "current in epoch    427      batch 3\n",
      "RLoss: 1191.9544677734375\n",
      "current in epoch    427      batch 4\n",
      "RLoss: 159.50572204589844\n",
      "current in epoch    427      batch 5\n",
      "RLoss: 70.77913665771484\n",
      "========================================\n",
      "Epoch 428/1000 - partial_train_loss: 509.7306 \n",
      "Epoch: [428/1000], TrainLoss: 105.16817161015102\n",
      "training Loss has not improved for 151 epochs.\n",
      "current in epoch    428      batch 0\n",
      "RLoss: 745.037109375\n",
      "current in epoch    428      batch 1\n",
      "RLoss: 23.620759963989258\n",
      "current in epoch    428      batch 2\n",
      "RLoss: 145.83218383789062\n",
      "current in epoch    428      batch 3\n",
      "RLoss: 75.41342163085938\n",
      "current in epoch    428      batch 4\n",
      "RLoss: 248.62454223632812\n",
      "current in epoch    428      batch 5\n",
      "RLoss: 33.47479248046875\n",
      "========================================\n",
      "Epoch 429/1000 - partial_train_loss: 239.4505 \n",
      "Epoch: [429/1000], TrainLoss: 27.10515376499721\n",
      "training Loss has not improved for 152 epochs.\n",
      "current in epoch    429      batch 0\n",
      "RLoss: 134.51754760742188\n",
      "current in epoch    429      batch 1\n",
      "RLoss: 111.5369873046875\n",
      "current in epoch    429      batch 2\n",
      "RLoss: 651.0592041015625\n",
      "current in epoch    429      batch 3\n",
      "RLoss: 29.070981979370117\n",
      "current in epoch    429      batch 4\n",
      "RLoss: 601.1921997070312\n",
      "current in epoch    429      batch 5\n",
      "RLoss: 33.91815185546875\n",
      "========================================\n",
      "Epoch 430/1000 - partial_train_loss: 246.4230 \n",
      "Epoch: [430/1000], TrainLoss: 54.148239203861785\n",
      "training Loss has not improved for 153 epochs.\n",
      "current in epoch    430      batch 0\n",
      "RLoss: 1136.8516845703125\n",
      "current in epoch    430      batch 1\n",
      "RLoss: 78.09233093261719\n",
      "current in epoch    430      batch 2\n",
      "RLoss: 933.7076416015625\n",
      "current in epoch    430      batch 3\n",
      "RLoss: 129.912353515625\n",
      "current in epoch    430      batch 4\n",
      "RLoss: 69.78557586669922\n",
      "current in epoch    430      batch 5\n",
      "RLoss: 54.923133850097656\n",
      "========================================\n",
      "Epoch 431/1000 - partial_train_loss: 398.1329 \n",
      "sorting training set\n",
      "Epoch 431/1000 - Training loss: 52.0700 \n",
      "========================================\n",
      "Epoch: [431/1000], TrainLoss: 57.697052433906556\n",
      "training Loss has not improved for 154 epochs.\n",
      "current in epoch    431      batch 0\n",
      "RLoss: 752.8228759765625\n",
      "current in epoch    431      batch 1\n",
      "RLoss: 237.96762084960938\n",
      "current in epoch    431      batch 2\n",
      "RLoss: 123.0022201538086\n",
      "current in epoch    431      batch 3\n",
      "RLoss: 29.696027755737305\n",
      "current in epoch    431      batch 4\n",
      "RLoss: 59.14935302734375\n",
      "current in epoch    431      batch 5\n",
      "RLoss: 622.2777099609375\n",
      "========================================\n",
      "Epoch 432/1000 - partial_train_loss: 245.0174 \n",
      "Epoch: [432/1000], TrainLoss: 607.583722795759\n",
      "training Loss has not improved for 155 epochs.\n",
      "current in epoch    432      batch 0\n",
      "RLoss: 702.1458740234375\n",
      "current in epoch    432      batch 1\n",
      "RLoss: 402.11688232421875\n",
      "current in epoch    432      batch 2\n",
      "RLoss: 176.80606079101562\n",
      "current in epoch    432      batch 3\n",
      "RLoss: 22.33866310119629\n",
      "current in epoch    432      batch 4\n",
      "RLoss: 201.0086669921875\n",
      "current in epoch    432      batch 5\n",
      "RLoss: 14.499150276184082\n",
      "========================================\n",
      "Epoch 433/1000 - partial_train_loss: 432.9689 \n",
      "Epoch: [433/1000], TrainLoss: 12.248385463442121\n",
      "training Loss has not improved for 156 epochs.\n",
      "current in epoch    433      batch 0\n",
      "RLoss: 42.40102005004883\n",
      "current in epoch    433      batch 1\n",
      "RLoss: 29.314403533935547\n",
      "current in epoch    433      batch 2\n",
      "RLoss: 308.9302062988281\n",
      "current in epoch    433      batch 3\n",
      "RLoss: 48.2373161315918\n",
      "current in epoch    433      batch 4\n",
      "RLoss: 25.09684944152832\n",
      "current in epoch    433      batch 5\n",
      "RLoss: 13.459700584411621\n",
      "========================================\n",
      "Epoch 434/1000 - partial_train_loss: 74.0730 \n",
      "Epoch: [434/1000], TrainLoss: 15.698497806276594\n",
      "training Loss has not improved for 157 epochs.\n",
      "current in epoch    434      batch 0\n",
      "RLoss: 134.5204620361328\n",
      "current in epoch    434      batch 1\n",
      "RLoss: 515.2254638671875\n",
      "current in epoch    434      batch 2\n",
      "RLoss: 657.4472045898438\n",
      "current in epoch    434      batch 3\n",
      "RLoss: 5.50991153717041\n",
      "current in epoch    434      batch 4\n",
      "RLoss: 151.13987731933594\n",
      "current in epoch    434      batch 5\n",
      "RLoss: 101.51089477539062\n",
      "========================================\n",
      "Epoch 435/1000 - partial_train_loss: 220.8660 \n",
      "Epoch: [435/1000], TrainLoss: 92.26071766444615\n",
      "training Loss has not improved for 158 epochs.\n",
      "current in epoch    435      batch 0\n",
      "RLoss: 158.03741455078125\n",
      "current in epoch    435      batch 1\n",
      "RLoss: 235.2305908203125\n",
      "current in epoch    435      batch 2\n",
      "RLoss: 142.04739379882812\n",
      "current in epoch    435      batch 3\n",
      "RLoss: 384.5885925292969\n",
      "current in epoch    435      batch 4\n",
      "RLoss: 223.03472900390625\n",
      "current in epoch    435      batch 5\n",
      "RLoss: 403.3385925292969\n",
      "========================================\n",
      "Epoch 436/1000 - partial_train_loss: 207.3641 \n",
      "sorting training set\n",
      "Epoch 436/1000 - Training loss: 362.0014 \n",
      "========================================\n",
      "Epoch: [436/1000], TrainLoss: 382.38639977260306\n",
      "training Loss has not improved for 159 epochs.\n",
      "current in epoch    436      batch 0\n",
      "RLoss: 877.7028198242188\n",
      "current in epoch    436      batch 1\n",
      "RLoss: 367.4676208496094\n",
      "current in epoch    436      batch 2\n",
      "RLoss: 263.8721923828125\n",
      "current in epoch    436      batch 3\n",
      "RLoss: 1098.0216064453125\n",
      "current in epoch    436      batch 4\n",
      "RLoss: 296.4508361816406\n",
      "current in epoch    436      batch 5\n",
      "RLoss: 33.5848274230957\n",
      "========================================\n",
      "Epoch 437/1000 - partial_train_loss: 631.1104 \n",
      "Epoch: [437/1000], TrainLoss: 47.92843232836042\n",
      "training Loss has not improved for 160 epochs.\n",
      "current in epoch    437      batch 0\n",
      "RLoss: 49.04811477661133\n",
      "current in epoch    437      batch 1\n",
      "RLoss: 24.753986358642578\n",
      "current in epoch    437      batch 2\n",
      "RLoss: 343.35723876953125\n",
      "current in epoch    437      batch 3\n",
      "RLoss: 114.49784088134766\n",
      "current in epoch    437      batch 4\n",
      "RLoss: 22.369426727294922\n",
      "current in epoch    437      batch 5\n",
      "RLoss: 39.123477935791016\n",
      "========================================\n",
      "Epoch 438/1000 - partial_train_loss: 94.6704 \n",
      "Epoch: [438/1000], TrainLoss: 50.66606133324759\n",
      "training Loss has not improved for 161 epochs.\n",
      "current in epoch    438      batch 0\n",
      "RLoss: 158.34226989746094\n",
      "current in epoch    438      batch 1\n",
      "RLoss: 185.44204711914062\n",
      "current in epoch    438      batch 2\n",
      "RLoss: 205.2870330810547\n",
      "current in epoch    438      batch 3\n",
      "RLoss: 950.6093139648438\n",
      "current in epoch    438      batch 4\n",
      "RLoss: 831.1744384765625\n",
      "current in epoch    438      batch 5\n",
      "RLoss: 20.6337947845459\n",
      "========================================\n",
      "Epoch 439/1000 - partial_train_loss: 398.7624 \n",
      "Epoch: [439/1000], TrainLoss: 14.028219955308098\n",
      "training Loss has not improved for 162 epochs.\n",
      "current in epoch    439      batch 0\n",
      "RLoss: 8.807969093322754\n",
      "current in epoch    439      batch 1\n",
      "RLoss: 103.33699035644531\n",
      "current in epoch    439      batch 2\n",
      "RLoss: 52.18245315551758\n",
      "current in epoch    439      batch 3\n",
      "RLoss: 28.62295913696289\n",
      "current in epoch    439      batch 4\n",
      "RLoss: 20.206008911132812\n",
      "current in epoch    439      batch 5\n",
      "RLoss: 33.4931526184082\n",
      "========================================\n",
      "Epoch 440/1000 - partial_train_loss: 48.4644 \n",
      "Epoch: [440/1000], TrainLoss: 24.36455464363098\n",
      "training Loss has not improved for 163 epochs.\n",
      "current in epoch    440      batch 0\n",
      "RLoss: 10.310677528381348\n",
      "current in epoch    440      batch 1\n",
      "RLoss: 97.77983856201172\n",
      "current in epoch    440      batch 2\n",
      "RLoss: 241.12396240234375\n",
      "current in epoch    440      batch 3\n",
      "RLoss: 289.2454528808594\n",
      "current in epoch    440      batch 4\n",
      "RLoss: 123.82679748535156\n",
      "current in epoch    440      batch 5\n",
      "RLoss: 415.3079833984375\n",
      "========================================\n",
      "Epoch 441/1000 - partial_train_loss: 134.8277 \n",
      "sorting training set\n",
      "Epoch 441/1000 - Training loss: 284.0915 \n",
      "========================================\n",
      "Epoch: [441/1000], TrainLoss: 303.32191655178804\n",
      "training Loss has not improved for 164 epochs.\n",
      "current in epoch    441      batch 0\n",
      "RLoss: 102.54097747802734\n",
      "current in epoch    441      batch 1\n",
      "RLoss: 133.3882598876953\n",
      "current in epoch    441      batch 2\n",
      "RLoss: 177.86460876464844\n",
      "current in epoch    441      batch 3\n",
      "RLoss: 544.8112182617188\n",
      "current in epoch    441      batch 4\n",
      "RLoss: 79.83393859863281\n",
      "current in epoch    441      batch 5\n",
      "RLoss: 172.34315490722656\n",
      "========================================\n",
      "Epoch 442/1000 - partial_train_loss: 363.6201 \n",
      "Epoch: [442/1000], TrainLoss: 289.5022430419922\n",
      "training Loss has not improved for 165 epochs.\n",
      "current in epoch    442      batch 0\n",
      "RLoss: 70.6173324584961\n",
      "current in epoch    442      batch 1\n",
      "RLoss: 112.4935073852539\n",
      "current in epoch    442      batch 2\n",
      "RLoss: 17.43202781677246\n",
      "current in epoch    442      batch 3\n",
      "RLoss: 126.1894760131836\n",
      "current in epoch    442      batch 4\n",
      "RLoss: 141.644287109375\n",
      "current in epoch    442      batch 5\n",
      "RLoss: 610.74560546875\n",
      "========================================\n",
      "Epoch 443/1000 - partial_train_loss: 93.6354 \n",
      "Epoch: [443/1000], TrainLoss: 594.0345110212054\n",
      "training Loss has not improved for 166 epochs.\n",
      "current in epoch    443      batch 0\n",
      "RLoss: 11.884101867675781\n",
      "current in epoch    443      batch 1\n",
      "RLoss: 148.88327026367188\n",
      "current in epoch    443      batch 2\n",
      "RLoss: 25.15558433532715\n",
      "current in epoch    443      batch 3\n",
      "RLoss: 40.3211669921875\n",
      "current in epoch    443      batch 4\n",
      "RLoss: 20.547388076782227\n",
      "current in epoch    443      batch 5\n",
      "RLoss: 84.2327651977539\n",
      "========================================\n",
      "Epoch 444/1000 - partial_train_loss: 154.6271 \n",
      "Epoch: [444/1000], TrainLoss: 51.88628114972796\n",
      "training Loss has not improved for 167 epochs.\n",
      "current in epoch    444      batch 0\n",
      "RLoss: 50.6524543762207\n",
      "current in epoch    444      batch 1\n",
      "RLoss: 65.47766876220703\n",
      "current in epoch    444      batch 2\n",
      "RLoss: 33.80374526977539\n",
      "current in epoch    444      batch 3\n",
      "RLoss: 340.1349792480469\n",
      "current in epoch    444      batch 4\n",
      "RLoss: 670.4271850585938\n",
      "current in epoch    444      batch 5\n",
      "RLoss: 309.8297119140625\n",
      "========================================\n",
      "Epoch 445/1000 - partial_train_loss: 195.1165 \n",
      "Epoch: [445/1000], TrainLoss: 151.72948707853044\n",
      "training Loss has not improved for 168 epochs.\n",
      "current in epoch    445      batch 0\n",
      "RLoss: 236.40330505371094\n",
      "current in epoch    445      batch 1\n",
      "RLoss: 279.581298828125\n",
      "current in epoch    445      batch 2\n",
      "RLoss: 65.37797546386719\n",
      "current in epoch    445      batch 3\n",
      "RLoss: 91.60514831542969\n",
      "current in epoch    445      batch 4\n",
      "RLoss: 50.47273635864258\n",
      "current in epoch    445      batch 5\n",
      "RLoss: 329.34942626953125\n",
      "========================================\n",
      "Epoch 446/1000 - partial_train_loss: 180.7297 \n",
      "sorting training set\n",
      "Epoch 446/1000 - Training loss: 482.6701 \n",
      "========================================\n",
      "Epoch: [446/1000], TrainLoss: 515.262386823445\n",
      "training Loss has not improved for 169 epochs.\n",
      "current in epoch    446      batch 0\n",
      "RLoss: 68.73551177978516\n",
      "current in epoch    446      batch 1\n",
      "RLoss: 92.11244201660156\n",
      "current in epoch    446      batch 2\n",
      "RLoss: 22.555631637573242\n",
      "current in epoch    446      batch 3\n",
      "RLoss: 145.27529907226562\n",
      "current in epoch    446      batch 4\n",
      "RLoss: 312.0106506347656\n",
      "current in epoch    446      batch 5\n",
      "RLoss: 15.192559242248535\n",
      "========================================\n",
      "Epoch 447/1000 - partial_train_loss: 347.7736 \n",
      "Epoch: [447/1000], TrainLoss: 63.40147927829197\n",
      "training Loss has not improved for 170 epochs.\n",
      "current in epoch    447      batch 0\n",
      "RLoss: 184.21217346191406\n",
      "current in epoch    447      batch 1\n",
      "RLoss: 404.306640625\n",
      "current in epoch    447      batch 2\n",
      "RLoss: 112.45874786376953\n",
      "current in epoch    447      batch 3\n",
      "RLoss: 14.806933403015137\n",
      "current in epoch    447      batch 4\n",
      "RLoss: 212.62710571289062\n",
      "current in epoch    447      batch 5\n",
      "RLoss: 35.45444107055664\n",
      "========================================\n",
      "Epoch 448/1000 - partial_train_loss: 162.2986 \n",
      "Epoch: [448/1000], TrainLoss: 35.03860214778355\n",
      "training Loss has not improved for 171 epochs.\n",
      "current in epoch    448      batch 0\n",
      "RLoss: 525.771484375\n",
      "current in epoch    448      batch 1\n",
      "RLoss: 133.95802307128906\n",
      "current in epoch    448      batch 2\n",
      "RLoss: 25.193180084228516\n",
      "current in epoch    448      batch 3\n",
      "RLoss: 137.28668212890625\n",
      "current in epoch    448      batch 4\n",
      "RLoss: 28.138084411621094\n",
      "current in epoch    448      batch 5\n",
      "RLoss: 218.34422302246094\n",
      "========================================\n",
      "Epoch 449/1000 - partial_train_loss: 116.5113 \n",
      "Epoch: [449/1000], TrainLoss: 284.79344395228793\n",
      "training Loss has not improved for 172 epochs.\n",
      "current in epoch    449      batch 0\n",
      "RLoss: 275.1019592285156\n",
      "current in epoch    449      batch 1\n",
      "RLoss: 136.88577270507812\n",
      "current in epoch    449      batch 2\n",
      "RLoss: 394.2931823730469\n",
      "current in epoch    449      batch 3\n",
      "RLoss: 47.348453521728516\n",
      "current in epoch    449      batch 4\n",
      "RLoss: 37.27356719970703\n",
      "current in epoch    449      batch 5\n",
      "RLoss: 11.986777305603027\n",
      "========================================\n",
      "Epoch 450/1000 - partial_train_loss: 159.0205 \n",
      "Epoch: [450/1000], TrainLoss: 10.75763270684651\n",
      "training Loss has not improved for 173 epochs.\n",
      "current in epoch    450      batch 0\n",
      "RLoss: 152.47760009765625\n",
      "current in epoch    450      batch 1\n",
      "RLoss: 20.395835876464844\n",
      "current in epoch    450      batch 2\n",
      "RLoss: 198.28463745117188\n",
      "current in epoch    450      batch 3\n",
      "RLoss: 60.07000732421875\n",
      "current in epoch    450      batch 4\n",
      "RLoss: 166.0922088623047\n",
      "current in epoch    450      batch 5\n",
      "RLoss: 286.945556640625\n",
      "========================================\n",
      "Epoch 451/1000 - partial_train_loss: 83.3015 \n",
      "sorting training set\n",
      "Epoch 451/1000 - Training loss: 254.3298 \n",
      "========================================\n",
      "Epoch: [451/1000], TrainLoss: 271.8444588833824\n",
      "training Loss has not improved for 174 epochs.\n",
      "current in epoch    451      batch 0\n",
      "RLoss: 285.7461853027344\n",
      "current in epoch    451      batch 1\n",
      "RLoss: 179.64889526367188\n",
      "current in epoch    451      batch 2\n",
      "RLoss: 13.2818021774292\n",
      "current in epoch    451      batch 3\n",
      "RLoss: 5.859011173248291\n",
      "current in epoch    451      batch 4\n",
      "RLoss: 76.9452896118164\n",
      "current in epoch    451      batch 5\n",
      "RLoss: 10.52279281616211\n",
      "========================================\n",
      "Epoch 452/1000 - partial_train_loss: 400.5899 \n",
      "Epoch: [452/1000], TrainLoss: 22.521150725228445\n",
      "training Loss has not improved for 175 epochs.\n",
      "current in epoch    452      batch 0\n",
      "RLoss: 21.542770385742188\n",
      "current in epoch    452      batch 1\n",
      "RLoss: 38.39265060424805\n",
      "current in epoch    452      batch 2\n",
      "RLoss: 15.010412216186523\n",
      "current in epoch    452      batch 3\n",
      "RLoss: 29.631900787353516\n",
      "current in epoch    452      batch 4\n",
      "RLoss: 15.625085830688477\n",
      "current in epoch    452      batch 5\n",
      "RLoss: 242.5370635986328\n",
      "========================================\n",
      "Epoch 453/1000 - partial_train_loss: 22.1681 \n",
      "Epoch: [453/1000], TrainLoss: 178.5904336656843\n",
      "training Loss has not improved for 176 epochs.\n",
      "current in epoch    453      batch 0\n",
      "RLoss: 96.00296783447266\n",
      "current in epoch    453      batch 1\n",
      "RLoss: 134.35372924804688\n",
      "current in epoch    453      batch 2\n",
      "RLoss: 31.585792541503906\n",
      "current in epoch    453      batch 3\n",
      "RLoss: 24.243513107299805\n",
      "current in epoch    453      batch 4\n",
      "RLoss: 43.984962463378906\n",
      "current in epoch    453      batch 5\n",
      "RLoss: 18.71322250366211\n",
      "========================================\n",
      "Epoch 454/1000 - partial_train_loss: 120.0282 \n",
      "Epoch: [454/1000], TrainLoss: 33.492116246904644\n",
      "training Loss has not improved for 177 epochs.\n",
      "current in epoch    454      batch 0\n",
      "RLoss: 543.4392700195312\n",
      "current in epoch    454      batch 1\n",
      "RLoss: 12.083585739135742\n",
      "current in epoch    454      batch 2\n",
      "RLoss: 260.28778076171875\n",
      "current in epoch    454      batch 3\n",
      "RLoss: 184.56582641601562\n",
      "current in epoch    454      batch 4\n",
      "RLoss: 210.456787109375\n",
      "current in epoch    454      batch 5\n",
      "RLoss: 40.069400787353516\n",
      "========================================\n",
      "Epoch 455/1000 - partial_train_loss: 190.8750 \n",
      "Epoch: [455/1000], TrainLoss: 96.00182826178414\n",
      "training Loss has not improved for 178 epochs.\n",
      "current in epoch    455      batch 0\n",
      "RLoss: 71.10993957519531\n",
      "current in epoch    455      batch 1\n",
      "RLoss: 259.56494140625\n",
      "current in epoch    455      batch 2\n",
      "RLoss: 63.61286544799805\n",
      "current in epoch    455      batch 3\n",
      "RLoss: 108.68142700195312\n",
      "current in epoch    455      batch 4\n",
      "RLoss: 124.8375015258789\n",
      "current in epoch    455      batch 5\n",
      "RLoss: 77.87736511230469\n",
      "========================================\n",
      "Epoch 456/1000 - partial_train_loss: 109.9562 \n",
      "sorting training set\n",
      "Epoch 456/1000 - Training loss: 47.7984 \n",
      "========================================\n",
      "Epoch: [456/1000], TrainLoss: 51.562133893500814\n",
      "training Loss has not improved for 179 epochs.\n",
      "current in epoch    456      batch 0\n",
      "RLoss: 219.26007080078125\n",
      "current in epoch    456      batch 1\n",
      "RLoss: 145.52056884765625\n",
      "current in epoch    456      batch 2\n",
      "RLoss: 77.70741271972656\n",
      "current in epoch    456      batch 3\n",
      "RLoss: 39.38140869140625\n",
      "current in epoch    456      batch 4\n",
      "RLoss: 32.57572555541992\n",
      "current in epoch    456      batch 5\n",
      "RLoss: 36.69673156738281\n",
      "========================================\n",
      "Epoch 457/1000 - partial_train_loss: 151.4828 \n",
      "Epoch: [457/1000], TrainLoss: 27.70400013242449\n",
      "training Loss has not improved for 180 epochs.\n",
      "current in epoch    457      batch 0\n",
      "RLoss: 38.577308654785156\n",
      "current in epoch    457      batch 1\n",
      "RLoss: 101.92713165283203\n",
      "current in epoch    457      batch 2\n",
      "RLoss: 20.36432647705078\n",
      "current in epoch    457      batch 3\n",
      "RLoss: 124.96697235107422\n",
      "current in epoch    457      batch 4\n",
      "RLoss: 30.32964324951172\n",
      "current in epoch    457      batch 5\n",
      "RLoss: 1853.36962890625\n",
      "========================================\n",
      "Epoch 458/1000 - partial_train_loss: 75.6613 \n",
      "Epoch: [458/1000], TrainLoss: 1888.3187604631696\n",
      "training Loss has not improved for 181 epochs.\n",
      "current in epoch    458      batch 0\n",
      "RLoss: 51.63722229003906\n",
      "current in epoch    458      batch 1\n",
      "RLoss: 17.27787971496582\n",
      "current in epoch    458      batch 2\n",
      "RLoss: 54.01704406738281\n",
      "current in epoch    458      batch 3\n",
      "RLoss: 245.30670166015625\n",
      "current in epoch    458      batch 4\n",
      "RLoss: 205.11781311035156\n",
      "current in epoch    458      batch 5\n",
      "RLoss: 55.48920822143555\n",
      "========================================\n",
      "Epoch 459/1000 - partial_train_loss: 259.1250 \n",
      "Epoch: [459/1000], TrainLoss: 48.985150882175994\n",
      "training Loss has not improved for 182 epochs.\n",
      "current in epoch    459      batch 0\n",
      "RLoss: 18.97531509399414\n",
      "current in epoch    459      batch 1\n",
      "RLoss: 111.83599853515625\n",
      "current in epoch    459      batch 2\n",
      "RLoss: 69.45924377441406\n",
      "current in epoch    459      batch 3\n",
      "RLoss: 44.59611129760742\n",
      "current in epoch    459      batch 4\n",
      "RLoss: 50.95823669433594\n",
      "current in epoch    459      batch 5\n",
      "RLoss: 111.7631607055664\n",
      "========================================\n",
      "Epoch 460/1000 - partial_train_loss: 64.8829 \n",
      "Epoch: [460/1000], TrainLoss: 571.1437486921038\n",
      "training Loss has not improved for 183 epochs.\n",
      "current in epoch    460      batch 0\n",
      "RLoss: 20.577701568603516\n",
      "current in epoch    460      batch 1\n",
      "RLoss: 663.7794189453125\n",
      "current in epoch    460      batch 2\n",
      "RLoss: 144.24765014648438\n",
      "current in epoch    460      batch 3\n",
      "RLoss: 45.76990509033203\n",
      "current in epoch    460      batch 4\n",
      "RLoss: 1546.387451171875\n",
      "current in epoch    460      batch 5\n",
      "RLoss: 1140.083740234375\n",
      "========================================\n",
      "Epoch 461/1000 - partial_train_loss: 443.4484 \n",
      "sorting training set\n",
      "Epoch 461/1000 - Training loss: 1223.0324 \n",
      "========================================\n",
      "Epoch: [461/1000], TrainLoss: 1300.8048973373188\n",
      "training Loss has not improved for 184 epochs.\n",
      "current in epoch    461      batch 0\n",
      "RLoss: 191.49728393554688\n",
      "current in epoch    461      batch 1\n",
      "RLoss: 358.29766845703125\n",
      "current in epoch    461      batch 2\n",
      "RLoss: 195.14210510253906\n",
      "current in epoch    461      batch 3\n",
      "RLoss: 57.992340087890625\n",
      "current in epoch    461      batch 4\n",
      "RLoss: 273.256103515625\n",
      "current in epoch    461      batch 5\n",
      "RLoss: 275.7452392578125\n",
      "========================================\n",
      "Epoch 462/1000 - partial_train_loss: 804.1901 \n",
      "Epoch: [462/1000], TrainLoss: 223.6266724722726\n",
      "training Loss has not improved for 185 epochs.\n",
      "current in epoch    462      batch 0\n",
      "RLoss: 43.247249603271484\n",
      "current in epoch    462      batch 1\n",
      "RLoss: 10.439765930175781\n",
      "current in epoch    462      batch 2\n",
      "RLoss: 39.305110931396484\n",
      "current in epoch    462      batch 3\n",
      "RLoss: 291.81707763671875\n",
      "current in epoch    462      batch 4\n",
      "RLoss: 94.53241729736328\n",
      "current in epoch    462      batch 5\n",
      "RLoss: 91.78768157958984\n",
      "========================================\n",
      "Epoch 463/1000 - partial_train_loss: 198.0855 \n",
      "Epoch: [463/1000], TrainLoss: 75.76116602761405\n",
      "training Loss has not improved for 186 epochs.\n",
      "current in epoch    463      batch 0\n",
      "RLoss: 74.39056396484375\n",
      "current in epoch    463      batch 1\n",
      "RLoss: 42.20283889770508\n",
      "current in epoch    463      batch 2\n",
      "RLoss: 903.8619995117188\n",
      "current in epoch    463      batch 3\n",
      "RLoss: 56.87434005737305\n",
      "current in epoch    463      batch 4\n",
      "RLoss: 56.12482833862305\n",
      "current in epoch    463      batch 5\n",
      "RLoss: 520.6664428710938\n",
      "========================================\n",
      "Epoch 464/1000 - partial_train_loss: 197.8711 \n",
      "Epoch: [464/1000], TrainLoss: 442.78606033325195\n",
      "training Loss has not improved for 187 epochs.\n",
      "current in epoch    464      batch 0\n",
      "RLoss: 626.7418823242188\n",
      "current in epoch    464      batch 1\n",
      "RLoss: 233.8518829345703\n",
      "current in epoch    464      batch 2\n",
      "RLoss: 126.21040344238281\n",
      "current in epoch    464      batch 3\n",
      "RLoss: 206.99095153808594\n",
      "current in epoch    464      batch 4\n",
      "RLoss: 29.137142181396484\n",
      "current in epoch    464      batch 5\n",
      "RLoss: 131.60397338867188\n",
      "========================================\n",
      "Epoch 465/1000 - partial_train_loss: 319.1089 \n",
      "Epoch: [465/1000], TrainLoss: 108.44944735935756\n",
      "training Loss has not improved for 188 epochs.\n",
      "current in epoch    465      batch 0\n",
      "RLoss: 56.755462646484375\n",
      "current in epoch    465      batch 1\n",
      "RLoss: 34.516056060791016\n",
      "current in epoch    465      batch 2\n",
      "RLoss: 197.63885498046875\n",
      "current in epoch    465      batch 3\n",
      "RLoss: 194.83694458007812\n",
      "current in epoch    465      batch 4\n",
      "RLoss: 118.83187866210938\n",
      "current in epoch    465      batch 5\n",
      "RLoss: 27.626314163208008\n",
      "========================================\n",
      "Epoch 466/1000 - partial_train_loss: 125.0848 \n",
      "sorting training set\n",
      "Epoch 466/1000 - Training loss: 19.9683 \n",
      "========================================\n",
      "Epoch: [466/1000], TrainLoss: 21.510876273545144\n",
      "training Loss has not improved for 189 epochs.\n",
      "current in epoch    466      batch 0\n",
      "RLoss: 119.74539184570312\n",
      "current in epoch    466      batch 1\n",
      "RLoss: 71.28986358642578\n",
      "current in epoch    466      batch 2\n",
      "RLoss: 40.060420989990234\n",
      "current in epoch    466      batch 3\n",
      "RLoss: 23.325225830078125\n",
      "current in epoch    466      batch 4\n",
      "RLoss: 220.0959930419922\n",
      "current in epoch    466      batch 5\n",
      "RLoss: 32.1614990234375\n",
      "========================================\n",
      "Epoch 467/1000 - partial_train_loss: 96.8928 \n",
      "Epoch: [467/1000], TrainLoss: 42.45814394950867\n",
      "training Loss has not improved for 190 epochs.\n",
      "current in epoch    467      batch 0\n",
      "RLoss: 32.920570373535156\n",
      "current in epoch    467      batch 1\n",
      "RLoss: 53.37342071533203\n",
      "current in epoch    467      batch 2\n",
      "RLoss: 16.749128341674805\n",
      "current in epoch    467      batch 3\n",
      "RLoss: 48.45464324951172\n",
      "current in epoch    467      batch 4\n",
      "RLoss: 37.04609298706055\n",
      "current in epoch    467      batch 5\n",
      "RLoss: 13.213241577148438\n",
      "========================================\n",
      "Epoch 468/1000 - partial_train_loss: 28.5627 \n",
      "Epoch: [468/1000], TrainLoss: 14.636670691626412\n",
      "training Loss has not improved for 191 epochs.\n",
      "current in epoch    468      batch 0\n",
      "RLoss: 10.19637680053711\n",
      "current in epoch    468      batch 1\n",
      "RLoss: 8.154882431030273\n",
      "current in epoch    468      batch 2\n",
      "RLoss: 16.56157112121582\n",
      "current in epoch    468      batch 3\n",
      "RLoss: 27.872154235839844\n",
      "current in epoch    468      batch 4\n",
      "RLoss: 71.43546295166016\n",
      "current in epoch    468      batch 5\n",
      "RLoss: 8.579522132873535\n",
      "========================================\n",
      "Epoch 469/1000 - partial_train_loss: 32.0208 \n",
      "Epoch: [469/1000], TrainLoss: 10.241335834775652\n",
      "training Loss has not improved for 192 epochs.\n",
      "current in epoch    469      batch 0\n",
      "RLoss: 17.88189697265625\n",
      "current in epoch    469      batch 1\n",
      "RLoss: 321.4801025390625\n",
      "current in epoch    469      batch 2\n",
      "RLoss: 139.6798553466797\n",
      "current in epoch    469      batch 3\n",
      "RLoss: 351.4725036621094\n",
      "current in epoch    469      batch 4\n",
      "RLoss: 93.2937240600586\n",
      "current in epoch    469      batch 5\n",
      "RLoss: 25.461275100708008\n",
      "========================================\n",
      "Epoch 470/1000 - partial_train_loss: 137.9990 \n",
      "Epoch: [470/1000], TrainLoss: 31.401512009756907\n",
      "training Loss has not improved for 193 epochs.\n",
      "current in epoch    470      batch 0\n",
      "RLoss: 4.8764777183532715\n",
      "current in epoch    470      batch 1\n",
      "RLoss: 164.1602783203125\n",
      "current in epoch    470      batch 2\n",
      "RLoss: 16.037927627563477\n",
      "current in epoch    470      batch 3\n",
      "RLoss: 583.36376953125\n",
      "current in epoch    470      batch 4\n",
      "RLoss: 95.55976104736328\n",
      "current in epoch    470      batch 5\n",
      "RLoss: 67.94705963134766\n",
      "========================================\n",
      "Epoch 471/1000 - partial_train_loss: 148.0649 \n",
      "sorting training set\n",
      "Epoch 471/1000 - Training loss: 68.3227 \n",
      "========================================\n",
      "Epoch: [471/1000], TrainLoss: 73.57487927158698\n",
      "training Loss has not improved for 194 epochs.\n",
      "current in epoch    471      batch 0\n",
      "RLoss: 1859.202880859375\n",
      "current in epoch    471      batch 1\n",
      "RLoss: 260.8105163574219\n",
      "current in epoch    471      batch 2\n",
      "RLoss: 312.953369140625\n",
      "current in epoch    471      batch 3\n",
      "RLoss: 113.09848022460938\n",
      "current in epoch    471      batch 4\n",
      "RLoss: 33.85311508178711\n",
      "current in epoch    471      batch 5\n",
      "RLoss: 13.784116744995117\n",
      "========================================\n",
      "Epoch 472/1000 - partial_train_loss: 380.8761 \n",
      "Epoch: [472/1000], TrainLoss: 12.62628858430045\n",
      "training Loss has not improved for 195 epochs.\n",
      "current in epoch    472      batch 0\n",
      "RLoss: 56.4569091796875\n",
      "current in epoch    472      batch 1\n",
      "RLoss: 40.20341491699219\n",
      "current in epoch    472      batch 2\n",
      "RLoss: 76.08856964111328\n",
      "current in epoch    472      batch 3\n",
      "RLoss: 82.45026397705078\n",
      "current in epoch    472      batch 4\n",
      "RLoss: 101.26342010498047\n",
      "current in epoch    472      batch 5\n",
      "RLoss: 50.36997604370117\n",
      "========================================\n",
      "Epoch 473/1000 - partial_train_loss: 58.4675 \n",
      "Epoch: [473/1000], TrainLoss: 75.02561432974679\n",
      "training Loss has not improved for 196 epochs.\n",
      "current in epoch    473      batch 0\n",
      "RLoss: 82.99473571777344\n",
      "current in epoch    473      batch 1\n",
      "RLoss: 20.529558181762695\n",
      "current in epoch    473      batch 2\n",
      "RLoss: 205.5482635498047\n",
      "current in epoch    473      batch 3\n",
      "RLoss: 42.17633056640625\n",
      "current in epoch    473      batch 4\n",
      "RLoss: 29.492046356201172\n",
      "current in epoch    473      batch 5\n",
      "RLoss: 81.78805541992188\n",
      "========================================\n",
      "Epoch 474/1000 - partial_train_loss: 91.6139 \n",
      "Epoch: [474/1000], TrainLoss: 76.7441646030971\n",
      "training Loss has not improved for 197 epochs.\n",
      "current in epoch    474      batch 0\n",
      "RLoss: 41.87541580200195\n",
      "current in epoch    474      batch 1\n",
      "RLoss: 180.35928344726562\n",
      "current in epoch    474      batch 2\n",
      "RLoss: 34.295406341552734\n",
      "current in epoch    474      batch 3\n",
      "RLoss: 162.17410278320312\n",
      "current in epoch    474      batch 4\n",
      "RLoss: 284.3345642089844\n",
      "current in epoch    474      batch 5\n",
      "RLoss: 143.69667053222656\n",
      "========================================\n",
      "Epoch 475/1000 - partial_train_loss: 116.5916 \n",
      "Epoch: [475/1000], TrainLoss: 137.30476215907507\n",
      "training Loss has not improved for 198 epochs.\n",
      "current in epoch    475      batch 0\n",
      "RLoss: 38.98287582397461\n",
      "current in epoch    475      batch 1\n",
      "RLoss: 27.983299255371094\n",
      "current in epoch    475      batch 2\n",
      "RLoss: 184.64939880371094\n",
      "current in epoch    475      batch 3\n",
      "RLoss: 121.6357192993164\n",
      "current in epoch    475      batch 4\n",
      "RLoss: 24.8369197845459\n",
      "current in epoch    475      batch 5\n",
      "RLoss: 21.517126083374023\n",
      "========================================\n",
      "Epoch 476/1000 - partial_train_loss: 115.0023 \n",
      "sorting training set\n",
      "Epoch 476/1000 - Training loss: 21.3717 \n",
      "========================================\n",
      "Epoch: [476/1000], TrainLoss: 23.077916175116787\n",
      "training Loss has not improved for 199 epochs.\n",
      "current in epoch    476      batch 0\n",
      "RLoss: 4.8371992111206055\n",
      "current in epoch    476      batch 1\n",
      "RLoss: 26.68501091003418\n",
      "current in epoch    476      batch 2\n",
      "RLoss: 56.57006072998047\n",
      "current in epoch    476      batch 3\n",
      "RLoss: 254.47872924804688\n",
      "current in epoch    476      batch 4\n",
      "RLoss: 37.406742095947266\n",
      "current in epoch    476      batch 5\n",
      "RLoss: 165.4807891845703\n",
      "========================================\n",
      "Epoch 477/1000 - partial_train_loss: 70.6896 \n",
      "Epoch: [477/1000], TrainLoss: 274.26616287231445\n",
      "training Loss has not improved for 200 epochs.\n",
      "current in epoch    477      batch 0\n",
      "RLoss: 14.405753135681152\n",
      "current in epoch    477      batch 1\n",
      "RLoss: 101.55133056640625\n",
      "current in epoch    477      batch 2\n",
      "RLoss: 38.741302490234375\n",
      "current in epoch    477      batch 3\n",
      "RLoss: 90.83377075195312\n",
      "current in epoch    477      batch 4\n",
      "RLoss: 71.9354476928711\n",
      "current in epoch    477      batch 5\n",
      "RLoss: 172.91444396972656\n",
      "========================================\n",
      "Epoch 478/1000 - partial_train_loss: 93.0058 \n",
      "Epoch: [478/1000], TrainLoss: 449.32835824149\n",
      "training Loss has not improved for 201 epochs.\n",
      "current in epoch    478      batch 0\n",
      "RLoss: 55.6731071472168\n",
      "current in epoch    478      batch 1\n",
      "RLoss: 61.07749938964844\n",
      "current in epoch    478      batch 2\n",
      "RLoss: 12.434242248535156\n",
      "current in epoch    478      batch 3\n",
      "RLoss: 295.5705871582031\n",
      "current in epoch    478      batch 4\n",
      "RLoss: 33.03251647949219\n",
      "current in epoch    478      batch 5\n",
      "RLoss: 2.7910516262054443\n",
      "========================================\n",
      "Epoch 479/1000 - partial_train_loss: 98.9346 \n",
      "Epoch: [479/1000], TrainLoss: 7.916962082896914\n",
      "training Loss has not improved for 202 epochs.\n",
      "current in epoch    479      batch 0\n",
      "RLoss: 173.14463806152344\n",
      "current in epoch    479      batch 1\n",
      "RLoss: 11.485230445861816\n",
      "current in epoch    479      batch 2\n",
      "RLoss: 62.277042388916016\n",
      "current in epoch    479      batch 3\n",
      "RLoss: 318.8301696777344\n",
      "current in epoch    479      batch 4\n",
      "RLoss: 36.27455139160156\n",
      "current in epoch    479      batch 5\n",
      "RLoss: 36.122955322265625\n",
      "========================================\n",
      "Epoch 480/1000 - partial_train_loss: 90.1952 \n",
      "Epoch: [480/1000], TrainLoss: 20.844531842640468\n",
      "training Loss has not improved for 203 epochs.\n",
      "current in epoch    480      batch 0\n",
      "RLoss: 138.1585235595703\n",
      "current in epoch    480      batch 1\n",
      "RLoss: 18.699951171875\n",
      "current in epoch    480      batch 2\n",
      "RLoss: 393.8393249511719\n",
      "current in epoch    480      batch 3\n",
      "RLoss: 13.274800300598145\n",
      "current in epoch    480      batch 4\n",
      "RLoss: 115.10688018798828\n",
      "current in epoch    480      batch 5\n",
      "RLoss: 152.37969970703125\n",
      "========================================\n",
      "Epoch 481/1000 - partial_train_loss: 106.8530 \n",
      "sorting training set\n",
      "Epoch 481/1000 - Training loss: 221.6846 \n",
      "========================================\n",
      "Epoch: [481/1000], TrainLoss: 237.18364137819603\n",
      "training Loss has not improved for 204 epochs.\n",
      "current in epoch    481      batch 0\n",
      "RLoss: 19.308609008789062\n",
      "current in epoch    481      batch 1\n",
      "RLoss: 73.7162094116211\n",
      "current in epoch    481      batch 2\n",
      "RLoss: 152.84194946289062\n",
      "current in epoch    481      batch 3\n",
      "RLoss: 17.338239669799805\n",
      "current in epoch    481      batch 4\n",
      "RLoss: 607.2059936523438\n",
      "current in epoch    481      batch 5\n",
      "RLoss: 35.94212341308594\n",
      "========================================\n",
      "Epoch 482/1000 - partial_train_loss: 300.7343 \n",
      "Epoch: [482/1000], TrainLoss: 40.64742456163679\n",
      "training Loss has not improved for 205 epochs.\n",
      "current in epoch    482      batch 0\n",
      "RLoss: 150.53233337402344\n",
      "current in epoch    482      batch 1\n",
      "RLoss: 1374.564208984375\n",
      "current in epoch    482      batch 2\n",
      "RLoss: 68.41927337646484\n",
      "current in epoch    482      batch 3\n",
      "RLoss: 42.6812629699707\n",
      "current in epoch    482      batch 4\n",
      "RLoss: 1333.90771484375\n",
      "current in epoch    482      batch 5\n",
      "RLoss: 28.44525909423828\n",
      "========================================\n",
      "Epoch 483/1000 - partial_train_loss: 506.4350 \n",
      "Epoch: [483/1000], TrainLoss: 82.31391334533691\n",
      "training Loss has not improved for 206 epochs.\n",
      "current in epoch    483      batch 0\n",
      "RLoss: 186.2733154296875\n",
      "current in epoch    483      batch 1\n",
      "RLoss: 159.30084228515625\n",
      "current in epoch    483      batch 2\n",
      "RLoss: 73.1232681274414\n",
      "current in epoch    483      batch 3\n",
      "RLoss: 365.2406005859375\n",
      "current in epoch    483      batch 4\n",
      "RLoss: 350.2506103515625\n",
      "current in epoch    483      batch 5\n",
      "RLoss: 441.6993713378906\n",
      "========================================\n",
      "Epoch 484/1000 - partial_train_loss: 190.9037 \n",
      "Epoch: [484/1000], TrainLoss: 302.20505469185963\n",
      "training Loss has not improved for 207 epochs.\n",
      "current in epoch    484      batch 0\n",
      "RLoss: 609.6532592773438\n",
      "current in epoch    484      batch 1\n",
      "RLoss: 346.1624450683594\n",
      "current in epoch    484      batch 2\n",
      "RLoss: 17.040122985839844\n",
      "current in epoch    484      batch 3\n",
      "RLoss: 33.938262939453125\n",
      "current in epoch    484      batch 4\n",
      "RLoss: 59.0888557434082\n",
      "current in epoch    484      batch 5\n",
      "RLoss: 5.928430080413818\n",
      "========================================\n",
      "Epoch 485/1000 - partial_train_loss: 275.3688 \n",
      "Epoch: [485/1000], TrainLoss: 5.231304117611477\n",
      "training Loss has not improved for 208 epochs.\n",
      "current in epoch    485      batch 0\n",
      "RLoss: 128.1466827392578\n",
      "current in epoch    485      batch 1\n",
      "RLoss: 101.8980484008789\n",
      "current in epoch    485      batch 2\n",
      "RLoss: 14.559789657592773\n",
      "current in epoch    485      batch 3\n",
      "RLoss: 1237.657470703125\n",
      "current in epoch    485      batch 4\n",
      "RLoss: 58.203529357910156\n",
      "current in epoch    485      batch 5\n",
      "RLoss: 32.141387939453125\n",
      "========================================\n",
      "Epoch 486/1000 - partial_train_loss: 220.2555 \n",
      "sorting training set\n",
      "Epoch 486/1000 - Training loss: 21.4480 \n",
      "========================================\n",
      "Epoch: [486/1000], TrainLoss: 23.000952458564225\n",
      "training Loss has not improved for 209 epochs.\n",
      "current in epoch    486      batch 0\n",
      "RLoss: 240.8770751953125\n",
      "current in epoch    486      batch 1\n",
      "RLoss: 25.424922943115234\n",
      "current in epoch    486      batch 2\n",
      "RLoss: 39.413692474365234\n",
      "current in epoch    486      batch 3\n",
      "RLoss: 89.78416442871094\n",
      "current in epoch    486      batch 4\n",
      "RLoss: 294.52520751953125\n",
      "current in epoch    486      batch 5\n",
      "RLoss: 68.29861450195312\n",
      "========================================\n",
      "Epoch 487/1000 - partial_train_loss: 135.0958 \n",
      "Epoch: [487/1000], TrainLoss: 476.2697045462472\n",
      "training Loss has not improved for 210 epochs.\n",
      "current in epoch    487      batch 0\n",
      "RLoss: 614.61181640625\n",
      "current in epoch    487      batch 1\n",
      "RLoss: 28.1033878326416\n",
      "current in epoch    487      batch 2\n",
      "RLoss: 59.98408508300781\n",
      "current in epoch    487      batch 3\n",
      "RLoss: 106.97488403320312\n",
      "current in epoch    487      batch 4\n",
      "RLoss: 77.98612213134766\n",
      "current in epoch    487      batch 5\n",
      "RLoss: 470.3174133300781\n",
      "========================================\n",
      "Epoch 488/1000 - partial_train_loss: 167.7608 \n",
      "Epoch: [488/1000], TrainLoss: 384.513308933803\n",
      "training Loss has not improved for 211 epochs.\n",
      "current in epoch    488      batch 0\n",
      "RLoss: 355.6219177246094\n",
      "current in epoch    488      batch 1\n",
      "RLoss: 153.9976348876953\n",
      "current in epoch    488      batch 2\n",
      "RLoss: 52.30729293823242\n",
      "current in epoch    488      batch 3\n",
      "RLoss: 1430.5804443359375\n",
      "current in epoch    488      batch 4\n",
      "RLoss: 1214.6414794921875\n",
      "current in epoch    488      batch 5\n",
      "RLoss: 68.37015533447266\n",
      "========================================\n",
      "Epoch 489/1000 - partial_train_loss: 591.0180 \n",
      "Epoch: [489/1000], TrainLoss: 73.94796439579555\n",
      "training Loss has not improved for 212 epochs.\n",
      "current in epoch    489      batch 0\n",
      "RLoss: 1060.7132568359375\n",
      "current in epoch    489      batch 1\n",
      "RLoss: 240.75355529785156\n",
      "current in epoch    489      batch 2\n",
      "RLoss: 764.5107421875\n",
      "current in epoch    489      batch 3\n",
      "RLoss: 1552.575439453125\n",
      "current in epoch    489      batch 4\n",
      "RLoss: 11.294313430786133\n",
      "current in epoch    489      batch 5\n",
      "RLoss: 149.1266632080078\n",
      "========================================\n",
      "Epoch 490/1000 - partial_train_loss: 526.2107 \n",
      "Epoch: [490/1000], TrainLoss: 89.86838109152657\n",
      "training Loss has not improved for 213 epochs.\n",
      "current in epoch    490      batch 0\n",
      "RLoss: 512.2381591796875\n",
      "current in epoch    490      batch 1\n",
      "RLoss: 861.2423706054688\n",
      "current in epoch    490      batch 2\n",
      "RLoss: 126.15029907226562\n",
      "current in epoch    490      batch 3\n",
      "RLoss: 475.6254577636719\n",
      "current in epoch    490      batch 4\n",
      "RLoss: 143.3811798095703\n",
      "current in epoch    490      batch 5\n",
      "RLoss: 341.2316589355469\n",
      "========================================\n",
      "Epoch 491/1000 - partial_train_loss: 346.7547 \n",
      "sorting training set\n",
      "Epoch 491/1000 - Training loss: 244.7235 \n",
      "========================================\n",
      "Epoch: [491/1000], TrainLoss: 262.23582953233887\n",
      "training Loss has not improved for 214 epochs.\n",
      "current in epoch    491      batch 0\n",
      "RLoss: 59.64146041870117\n",
      "current in epoch    491      batch 1\n",
      "RLoss: 57.93780517578125\n",
      "current in epoch    491      batch 2\n",
      "RLoss: 393.92071533203125\n",
      "current in epoch    491      batch 3\n",
      "RLoss: 222.47048950195312\n",
      "current in epoch    491      batch 4\n",
      "RLoss: 32.77676773071289\n",
      "current in epoch    491      batch 5\n",
      "RLoss: 792.3159790039062\n",
      "========================================\n",
      "Epoch 492/1000 - partial_train_loss: 311.3535 \n",
      "Epoch: [492/1000], TrainLoss: 579.534417288644\n",
      "training Loss has not improved for 215 epochs.\n",
      "current in epoch    492      batch 0\n",
      "RLoss: 111.04618072509766\n",
      "current in epoch    492      batch 1\n",
      "RLoss: 10.183106422424316\n",
      "current in epoch    492      batch 2\n",
      "RLoss: 264.6834411621094\n",
      "current in epoch    492      batch 3\n",
      "RLoss: 69.06414794921875\n",
      "current in epoch    492      batch 4\n",
      "RLoss: 68.5017318725586\n",
      "current in epoch    492      batch 5\n",
      "RLoss: 87.40225982666016\n",
      "========================================\n",
      "Epoch 493/1000 - partial_train_loss: 203.6671 \n",
      "Epoch: [493/1000], TrainLoss: 76.99883815220424\n",
      "training Loss has not improved for 216 epochs.\n",
      "current in epoch    493      batch 0\n",
      "RLoss: 85.23176574707031\n",
      "current in epoch    493      batch 1\n",
      "RLoss: 6.975425720214844\n",
      "current in epoch    493      batch 2\n",
      "RLoss: 11.72010612487793\n",
      "current in epoch    493      batch 3\n",
      "RLoss: 115.27941131591797\n",
      "current in epoch    493      batch 4\n",
      "RLoss: 105.9775390625\n",
      "current in epoch    493      batch 5\n",
      "RLoss: 92.14996337890625\n",
      "========================================\n",
      "Epoch 494/1000 - partial_train_loss: 72.1868 \n",
      "Epoch: [494/1000], TrainLoss: 157.58334841047014\n",
      "training Loss has not improved for 217 epochs.\n",
      "current in epoch    494      batch 0\n",
      "RLoss: 23.91963005065918\n",
      "current in epoch    494      batch 1\n",
      "RLoss: 53.11383819580078\n",
      "current in epoch    494      batch 2\n",
      "RLoss: 64.20340728759766\n",
      "current in epoch    494      batch 3\n",
      "RLoss: 42.01062774658203\n",
      "current in epoch    494      batch 4\n",
      "RLoss: 863.7111206054688\n",
      "current in epoch    494      batch 5\n",
      "RLoss: 57.04168701171875\n",
      "========================================\n",
      "Epoch 495/1000 - partial_train_loss: 168.0177 \n",
      "Epoch: [495/1000], TrainLoss: 51.286833354405\n",
      "training Loss has not improved for 218 epochs.\n",
      "current in epoch    495      batch 0\n",
      "RLoss: 47.85258483886719\n",
      "current in epoch    495      batch 1\n",
      "RLoss: 128.65957641601562\n",
      "current in epoch    495      batch 2\n",
      "RLoss: 1373.2412109375\n",
      "current in epoch    495      batch 3\n",
      "RLoss: 34.87974548339844\n",
      "current in epoch    495      batch 4\n",
      "RLoss: 16.71986961364746\n",
      "current in epoch    495      batch 5\n",
      "RLoss: 227.81314086914062\n",
      "========================================\n",
      "Epoch 496/1000 - partial_train_loss: 250.2280 \n",
      "sorting training set\n",
      "Epoch 496/1000 - Training loss: 349.2134 \n",
      "========================================\n",
      "Epoch: [496/1000], TrainLoss: 370.8169141045134\n",
      "training Loss has not improved for 219 epochs.\n",
      "current in epoch    496      batch 0\n",
      "RLoss: 374.718994140625\n",
      "current in epoch    496      batch 1\n",
      "RLoss: 106.29251098632812\n",
      "current in epoch    496      batch 2\n",
      "RLoss: 22.9163875579834\n",
      "current in epoch    496      batch 3\n",
      "RLoss: 65.0093002319336\n",
      "current in epoch    496      batch 4\n",
      "RLoss: 5.637080192565918\n",
      "current in epoch    496      batch 5\n",
      "RLoss: 27.15047836303711\n",
      "========================================\n",
      "Epoch 497/1000 - partial_train_loss: 246.7107 \n",
      "Epoch: [497/1000], TrainLoss: 37.915116616657805\n",
      "training Loss has not improved for 220 epochs.\n",
      "current in epoch    497      batch 0\n",
      "RLoss: 453.94305419921875\n",
      "current in epoch    497      batch 1\n",
      "RLoss: 114.28836822509766\n",
      "current in epoch    497      batch 2\n",
      "RLoss: 278.4208984375\n",
      "current in epoch    497      batch 3\n",
      "RLoss: 283.0380859375\n",
      "current in epoch    497      batch 4\n",
      "RLoss: 52.253047943115234\n",
      "current in epoch    497      batch 5\n",
      "RLoss: 15.185840606689453\n",
      "========================================\n",
      "Epoch 498/1000 - partial_train_loss: 200.6945 \n",
      "Epoch: [498/1000], TrainLoss: 11.13082560471126\n",
      "training Loss has not improved for 221 epochs.\n",
      "current in epoch    498      batch 0\n",
      "RLoss: 167.7637176513672\n",
      "current in epoch    498      batch 1\n",
      "RLoss: 508.4493713378906\n",
      "current in epoch    498      batch 2\n",
      "RLoss: 19.032434463500977\n",
      "current in epoch    498      batch 3\n",
      "RLoss: 20.633060455322266\n",
      "current in epoch    498      batch 4\n",
      "RLoss: 54.17844772338867\n",
      "current in epoch    498      batch 5\n",
      "RLoss: 10.488395690917969\n",
      "========================================\n",
      "Epoch 499/1000 - partial_train_loss: 158.0780 \n",
      "Epoch: [499/1000], TrainLoss: 62.36883555139814\n",
      "training Loss has not improved for 222 epochs.\n",
      "current in epoch    499      batch 0\n",
      "RLoss: 4.882476806640625\n",
      "current in epoch    499      batch 1\n",
      "RLoss: 384.3330993652344\n",
      "current in epoch    499      batch 2\n",
      "RLoss: 115.19755554199219\n",
      "current in epoch    499      batch 3\n",
      "RLoss: 67.20478820800781\n",
      "current in epoch    499      batch 4\n",
      "RLoss: 11.911395072937012\n",
      "current in epoch    499      batch 5\n",
      "RLoss: 365.6950988769531\n",
      "========================================\n",
      "Epoch 500/1000 - partial_train_loss: 103.6578 \n",
      "Epoch: [500/1000], TrainLoss: 345.15898568289623\n",
      "training Loss has not improved for 223 epochs.\n",
      "current in epoch    500      batch 0\n",
      "RLoss: 62.11347579956055\n",
      "current in epoch    500      batch 1\n",
      "RLoss: 26.856468200683594\n",
      "current in epoch    500      batch 2\n",
      "RLoss: 636.6129150390625\n",
      "current in epoch    500      batch 3\n",
      "RLoss: 66.56706237792969\n",
      "current in epoch    500      batch 4\n",
      "RLoss: 650.463134765625\n",
      "current in epoch    500      batch 5\n",
      "RLoss: 43.89491271972656\n",
      "========================================\n",
      "Epoch 501/1000 - partial_train_loss: 315.0258 \n",
      "sorting training set\n",
      "Epoch 501/1000 - Training loss: 43.9017 \n",
      "========================================\n",
      "Epoch: [501/1000], TrainLoss: 47.15352311417529\n",
      "training Loss has not improved for 224 epochs.\n",
      "current in epoch    501      batch 0\n",
      "RLoss: 764.3550415039062\n",
      "current in epoch    501      batch 1\n",
      "RLoss: 180.79539489746094\n",
      "current in epoch    501      batch 2\n",
      "RLoss: 109.62886047363281\n",
      "current in epoch    501      batch 3\n",
      "RLoss: 216.6411895751953\n",
      "current in epoch    501      batch 4\n",
      "RLoss: 33.74385070800781\n",
      "current in epoch    501      batch 5\n",
      "RLoss: 33.981292724609375\n",
      "========================================\n",
      "Epoch 502/1000 - partial_train_loss: 282.0371 \n",
      "Epoch: [502/1000], TrainLoss: 28.518484115600586\n",
      "training Loss has not improved for 225 epochs.\n",
      "current in epoch    502      batch 0\n",
      "RLoss: 169.8433837890625\n",
      "current in epoch    502      batch 1\n",
      "RLoss: 28.472373962402344\n",
      "current in epoch    502      batch 2\n",
      "RLoss: 19.278432846069336\n",
      "current in epoch    502      batch 3\n",
      "RLoss: 16.59449005126953\n",
      "current in epoch    502      batch 4\n",
      "RLoss: 479.1009216308594\n",
      "current in epoch    502      batch 5\n",
      "RLoss: 52.59908676147461\n",
      "========================================\n",
      "Epoch 503/1000 - partial_train_loss: 128.2020 \n",
      "Epoch: [503/1000], TrainLoss: 35.45589678628104\n",
      "training Loss has not improved for 226 epochs.\n",
      "current in epoch    503      batch 0\n",
      "RLoss: 303.4989929199219\n",
      "current in epoch    503      batch 1\n",
      "RLoss: 102.03175354003906\n",
      "current in epoch    503      batch 2\n",
      "RLoss: 144.95635986328125\n",
      "current in epoch    503      batch 3\n",
      "RLoss: 50.15620040893555\n",
      "current in epoch    503      batch 4\n",
      "RLoss: 82.21685028076172\n",
      "current in epoch    503      batch 5\n",
      "RLoss: 91.56700134277344\n",
      "========================================\n",
      "Epoch 504/1000 - partial_train_loss: 95.7020 \n",
      "Epoch: [504/1000], TrainLoss: 69.53544998168945\n",
      "training Loss has not improved for 227 epochs.\n",
      "current in epoch    504      batch 0\n",
      "RLoss: 227.98814392089844\n",
      "current in epoch    504      batch 1\n",
      "RLoss: 3.2076575756073\n",
      "current in epoch    504      batch 2\n",
      "RLoss: 770.49560546875\n",
      "current in epoch    504      batch 3\n",
      "RLoss: 59.91213607788086\n",
      "current in epoch    504      batch 4\n",
      "RLoss: 159.33584594726562\n",
      "current in epoch    504      batch 5\n",
      "RLoss: 39.206642150878906\n",
      "========================================\n",
      "Epoch 505/1000 - partial_train_loss: 251.9189 \n",
      "Epoch: [505/1000], TrainLoss: 37.855056626456125\n",
      "training Loss has not improved for 228 epochs.\n",
      "current in epoch    505      batch 0\n",
      "RLoss: 1261.675537109375\n",
      "current in epoch    505      batch 1\n",
      "RLoss: 86.05448150634766\n",
      "current in epoch    505      batch 2\n",
      "RLoss: 98.52012634277344\n",
      "current in epoch    505      batch 3\n",
      "RLoss: 55.09020233154297\n",
      "current in epoch    505      batch 4\n",
      "RLoss: 131.52032470703125\n",
      "current in epoch    505      batch 5\n",
      "RLoss: 49.82399368286133\n",
      "========================================\n",
      "Epoch 506/1000 - partial_train_loss: 270.6106 \n",
      "sorting training set\n",
      "Epoch 506/1000 - Training loss: 36.0637 \n",
      "========================================\n",
      "Epoch: [506/1000], TrainLoss: 39.05821732050462\n",
      "training Loss has not improved for 229 epochs.\n",
      "current in epoch    506      batch 0\n",
      "RLoss: 86.45340728759766\n",
      "current in epoch    506      batch 1\n",
      "RLoss: 108.20972442626953\n",
      "current in epoch    506      batch 2\n",
      "RLoss: 81.6971664428711\n",
      "current in epoch    506      batch 3\n",
      "RLoss: 13.061263084411621\n",
      "current in epoch    506      batch 4\n",
      "RLoss: 34.75984191894531\n",
      "current in epoch    506      batch 5\n",
      "RLoss: 35.35841751098633\n",
      "========================================\n",
      "Epoch 507/1000 - partial_train_loss: 113.2968 \n",
      "Epoch: [507/1000], TrainLoss: 28.685866492135183\n",
      "training Loss has not improved for 230 epochs.\n",
      "current in epoch    507      batch 0\n",
      "RLoss: 21.654449462890625\n",
      "current in epoch    507      batch 1\n",
      "RLoss: 78.37142181396484\n",
      "current in epoch    507      batch 2\n",
      "RLoss: 50.711219787597656\n",
      "current in epoch    507      batch 3\n",
      "RLoss: 34.53205490112305\n",
      "current in epoch    507      batch 4\n",
      "RLoss: 91.07284545898438\n",
      "current in epoch    507      batch 5\n",
      "RLoss: 46.95350646972656\n",
      "========================================\n",
      "Epoch 508/1000 - partial_train_loss: 52.4827 \n",
      "Epoch: [508/1000], TrainLoss: 53.79975809369768\n",
      "training Loss has not improved for 231 epochs.\n",
      "current in epoch    508      batch 0\n",
      "RLoss: 162.37213134765625\n",
      "current in epoch    508      batch 1\n",
      "RLoss: 24.037700653076172\n",
      "current in epoch    508      batch 2\n",
      "RLoss: 472.1960754394531\n",
      "current in epoch    508      batch 3\n",
      "RLoss: 429.505126953125\n",
      "current in epoch    508      batch 4\n",
      "RLoss: 41.890506744384766\n",
      "current in epoch    508      batch 5\n",
      "RLoss: 28.25225067138672\n",
      "========================================\n",
      "Epoch 509/1000 - partial_train_loss: 203.1081 \n",
      "Epoch: [509/1000], TrainLoss: 20.07815432548523\n",
      "training Loss has not improved for 232 epochs.\n",
      "current in epoch    509      batch 0\n",
      "RLoss: 78.39044189453125\n",
      "current in epoch    509      batch 1\n",
      "RLoss: 282.62530517578125\n",
      "current in epoch    509      batch 2\n",
      "RLoss: 101.47162628173828\n",
      "current in epoch    509      batch 3\n",
      "RLoss: 116.43778991699219\n",
      "current in epoch    509      batch 4\n",
      "RLoss: 151.78843688964844\n",
      "current in epoch    509      batch 5\n",
      "RLoss: 85.25605010986328\n",
      "========================================\n",
      "Epoch 510/1000 - partial_train_loss: 107.5297 \n",
      "Epoch: [510/1000], TrainLoss: 74.7994100025722\n",
      "training Loss has not improved for 233 epochs.\n",
      "current in epoch    510      batch 0\n",
      "RLoss: 122.02119445800781\n",
      "current in epoch    510      batch 1\n",
      "RLoss: 67.47831726074219\n",
      "current in epoch    510      batch 2\n",
      "RLoss: 86.67564392089844\n",
      "current in epoch    510      batch 3\n",
      "RLoss: 191.13430786132812\n",
      "current in epoch    510      batch 4\n",
      "RLoss: 107.40306091308594\n",
      "current in epoch    510      batch 5\n",
      "RLoss: 183.61961364746094\n",
      "========================================\n",
      "Epoch 511/1000 - partial_train_loss: 85.8053 \n",
      "sorting training set\n",
      "Epoch 511/1000 - Training loss: 163.3295 \n",
      "========================================\n",
      "Epoch: [511/1000], TrainLoss: 174.4774123868128\n",
      "training Loss has not improved for 234 epochs.\n",
      "current in epoch    511      batch 0\n",
      "RLoss: 235.14181518554688\n",
      "current in epoch    511      batch 1\n",
      "RLoss: 473.97564697265625\n",
      "current in epoch    511      batch 2\n",
      "RLoss: 776.6624145507812\n",
      "current in epoch    511      batch 3\n",
      "RLoss: 1476.6610107421875\n",
      "current in epoch    511      batch 4\n",
      "RLoss: 337.1158752441406\n",
      "current in epoch    511      batch 5\n",
      "RLoss: 111.69608306884766\n",
      "========================================\n",
      "Epoch 512/1000 - partial_train_loss: 649.2172 \n",
      "Epoch: [512/1000], TrainLoss: 132.3111013684954\n",
      "training Loss has not improved for 235 epochs.\n",
      "current in epoch    512      batch 0\n",
      "RLoss: 321.8682861328125\n",
      "current in epoch    512      batch 1\n",
      "RLoss: 302.96990966796875\n",
      "current in epoch    512      batch 2\n",
      "RLoss: 54.543670654296875\n",
      "current in epoch    512      batch 3\n",
      "RLoss: 606.6929931640625\n",
      "current in epoch    512      batch 4\n",
      "RLoss: 34.660362243652344\n",
      "current in epoch    512      batch 5\n",
      "RLoss: 923.7625732421875\n",
      "========================================\n",
      "Epoch 513/1000 - partial_train_loss: 238.3360 \n",
      "Epoch: [513/1000], TrainLoss: 839.4886997767857\n",
      "training Loss has not improved for 236 epochs.\n",
      "current in epoch    513      batch 0\n",
      "RLoss: 26.748268127441406\n",
      "current in epoch    513      batch 1\n",
      "RLoss: 9.18593978881836\n",
      "current in epoch    513      batch 2\n",
      "RLoss: 755.7046508789062\n",
      "current in epoch    513      batch 3\n",
      "RLoss: 27.314502716064453\n",
      "current in epoch    513      batch 4\n",
      "RLoss: 15.039153099060059\n",
      "current in epoch    513      batch 5\n",
      "RLoss: 29.978776931762695\n",
      "========================================\n",
      "Epoch 514/1000 - partial_train_loss: 268.3551 \n",
      "Epoch: [514/1000], TrainLoss: 28.863070351736887\n",
      "training Loss has not improved for 237 epochs.\n",
      "current in epoch    514      batch 0\n",
      "RLoss: 56.03707504272461\n",
      "current in epoch    514      batch 1\n",
      "RLoss: 110.87052917480469\n",
      "current in epoch    514      batch 2\n",
      "RLoss: 271.9211730957031\n",
      "current in epoch    514      batch 3\n",
      "RLoss: 317.49822998046875\n",
      "current in epoch    514      batch 4\n",
      "RLoss: 110.72505950927734\n",
      "current in epoch    514      batch 5\n",
      "RLoss: 102.71549224853516\n",
      "========================================\n",
      "Epoch 515/1000 - partial_train_loss: 126.4891 \n",
      "Epoch: [515/1000], TrainLoss: 93.09719637462071\n",
      "training Loss has not improved for 238 epochs.\n",
      "current in epoch    515      batch 0\n",
      "RLoss: 147.76263427734375\n",
      "current in epoch    515      batch 1\n",
      "RLoss: 61.07271194458008\n",
      "current in epoch    515      batch 2\n",
      "RLoss: 129.4923095703125\n",
      "current in epoch    515      batch 3\n",
      "RLoss: 16.218957901000977\n",
      "current in epoch    515      batch 4\n",
      "RLoss: 66.5044937133789\n",
      "current in epoch    515      batch 5\n",
      "RLoss: 32.48637390136719\n",
      "========================================\n",
      "Epoch 516/1000 - partial_train_loss: 63.8691 \n",
      "sorting training set\n",
      "Epoch 516/1000 - Training loss: 25.0177 \n",
      "========================================\n",
      "Epoch: [516/1000], TrainLoss: 26.884984909883265\n",
      "training Loss has not improved for 239 epochs.\n",
      "current in epoch    516      batch 0\n",
      "RLoss: 9.648250579833984\n",
      "current in epoch    516      batch 1\n",
      "RLoss: 58.31560134887695\n",
      "current in epoch    516      batch 2\n",
      "RLoss: 763.4896240234375\n",
      "current in epoch    516      batch 3\n",
      "RLoss: 437.13848876953125\n",
      "current in epoch    516      batch 4\n",
      "RLoss: 20.369203567504883\n",
      "current in epoch    516      batch 5\n",
      "RLoss: 56.01450729370117\n",
      "========================================\n",
      "Epoch 517/1000 - partial_train_loss: 242.0658 \n",
      "Epoch: [517/1000], TrainLoss: 48.59995174407959\n",
      "training Loss has not improved for 240 epochs.\n",
      "current in epoch    517      batch 0\n",
      "RLoss: 7.199372291564941\n",
      "current in epoch    517      batch 1\n",
      "RLoss: 318.3763122558594\n",
      "current in epoch    517      batch 2\n",
      "RLoss: 924.8125\n",
      "current in epoch    517      batch 3\n",
      "RLoss: 17.500164031982422\n",
      "current in epoch    517      batch 4\n",
      "RLoss: 58.1573600769043\n",
      "current in epoch    517      batch 5\n",
      "RLoss: 153.62818908691406\n",
      "========================================\n",
      "Epoch 518/1000 - partial_train_loss: 244.8352 \n",
      "Epoch: [518/1000], TrainLoss: 93.26528889792306\n",
      "training Loss has not improved for 241 epochs.\n",
      "current in epoch    518      batch 0\n",
      "RLoss: 87.2241439819336\n",
      "current in epoch    518      batch 1\n",
      "RLoss: 20.644512176513672\n",
      "current in epoch    518      batch 2\n",
      "RLoss: 36.3184928894043\n",
      "current in epoch    518      batch 3\n",
      "RLoss: 140.9863739013672\n",
      "current in epoch    518      batch 4\n",
      "RLoss: 214.7168731689453\n",
      "current in epoch    518      batch 5\n",
      "RLoss: 65.78040313720703\n",
      "========================================\n",
      "Epoch 519/1000 - partial_train_loss: 108.8513 \n",
      "Epoch: [519/1000], TrainLoss: 84.42759677342006\n",
      "training Loss has not improved for 242 epochs.\n",
      "current in epoch    519      batch 0\n",
      "RLoss: 249.1647491455078\n",
      "current in epoch    519      batch 1\n",
      "RLoss: 111.99703216552734\n",
      "current in epoch    519      batch 2\n",
      "RLoss: 14.101924896240234\n",
      "current in epoch    519      batch 3\n",
      "RLoss: 15.701852798461914\n",
      "current in epoch    519      batch 4\n",
      "RLoss: 56.40895080566406\n",
      "current in epoch    519      batch 5\n",
      "RLoss: 27.79166603088379\n",
      "========================================\n",
      "Epoch 520/1000 - partial_train_loss: 87.9390 \n",
      "Epoch: [520/1000], TrainLoss: 33.44558869089399\n",
      "training Loss has not improved for 243 epochs.\n",
      "current in epoch    520      batch 0\n",
      "RLoss: 137.0450897216797\n",
      "current in epoch    520      batch 1\n",
      "RLoss: 81.30206298828125\n",
      "current in epoch    520      batch 2\n",
      "RLoss: 28.897823333740234\n",
      "current in epoch    520      batch 3\n",
      "RLoss: 526.9666137695312\n",
      "current in epoch    520      batch 4\n",
      "RLoss: 1156.7579345703125\n",
      "current in epoch    520      batch 5\n",
      "RLoss: 447.06597900390625\n",
      "========================================\n",
      "Epoch 521/1000 - partial_train_loss: 319.6817 \n",
      "sorting training set\n",
      "Epoch 521/1000 - Training loss: 336.2004 \n",
      "========================================\n",
      "Epoch: [521/1000], TrainLoss: 358.89923478477016\n",
      "training Loss has not improved for 244 epochs.\n",
      "current in epoch    521      batch 0\n",
      "RLoss: 104.06080627441406\n",
      "current in epoch    521      batch 1\n",
      "RLoss: 167.04220581054688\n",
      "current in epoch    521      batch 2\n",
      "RLoss: 20.72099494934082\n",
      "current in epoch    521      batch 3\n",
      "RLoss: 278.8530578613281\n",
      "current in epoch    521      batch 4\n",
      "RLoss: 12.434123992919922\n",
      "current in epoch    521      batch 5\n",
      "RLoss: 31.335142135620117\n",
      "========================================\n",
      "Epoch 522/1000 - partial_train_loss: 328.1294 \n",
      "Epoch: [522/1000], TrainLoss: 16.49513986280986\n",
      "training Loss has not improved for 245 epochs.\n",
      "current in epoch    522      batch 0\n",
      "RLoss: 33.23714065551758\n",
      "current in epoch    522      batch 1\n",
      "RLoss: 152.9945831298828\n",
      "current in epoch    522      batch 2\n",
      "RLoss: 1167.52587890625\n",
      "current in epoch    522      batch 3\n",
      "RLoss: 713.6015014648438\n",
      "current in epoch    522      batch 4\n",
      "RLoss: 45.538673400878906\n",
      "current in epoch    522      batch 5\n",
      "RLoss: 14.68288516998291\n",
      "========================================\n",
      "Epoch 523/1000 - partial_train_loss: 327.6271 \n",
      "Epoch: [523/1000], TrainLoss: 12.604519878114973\n",
      "training Loss has not improved for 246 epochs.\n",
      "current in epoch    523      batch 0\n",
      "RLoss: 11.682225227355957\n",
      "current in epoch    523      batch 1\n",
      "RLoss: 267.5105285644531\n",
      "current in epoch    523      batch 2\n",
      "RLoss: 23.924562454223633\n",
      "current in epoch    523      batch 3\n",
      "RLoss: 287.3589782714844\n",
      "current in epoch    523      batch 4\n",
      "RLoss: 482.4962158203125\n",
      "current in epoch    523      batch 5\n",
      "RLoss: 39.566627502441406\n",
      "========================================\n",
      "Epoch 524/1000 - partial_train_loss: 180.6775 \n",
      "Epoch: [524/1000], TrainLoss: 25.55097872870309\n",
      "training Loss has not improved for 247 epochs.\n",
      "current in epoch    524      batch 0\n",
      "RLoss: 392.7428283691406\n",
      "current in epoch    524      batch 1\n",
      "RLoss: 189.73480224609375\n",
      "current in epoch    524      batch 2\n",
      "RLoss: 165.6349334716797\n",
      "current in epoch    524      batch 3\n",
      "RLoss: 26.648027420043945\n",
      "current in epoch    524      batch 4\n",
      "RLoss: 78.07749938964844\n",
      "current in epoch    524      batch 5\n",
      "RLoss: 20.477035522460938\n",
      "========================================\n",
      "Epoch 525/1000 - partial_train_loss: 120.1593 \n",
      "Epoch: [525/1000], TrainLoss: 24.718789764813014\n",
      "training Loss has not improved for 248 epochs.\n",
      "current in epoch    525      batch 0\n",
      "RLoss: 10.08878231048584\n",
      "current in epoch    525      batch 1\n",
      "RLoss: 116.8040771484375\n",
      "current in epoch    525      batch 2\n",
      "RLoss: 109.37818908691406\n",
      "current in epoch    525      batch 3\n",
      "RLoss: 147.01524353027344\n",
      "current in epoch    525      batch 4\n",
      "RLoss: 32.15312957763672\n",
      "current in epoch    525      batch 5\n",
      "RLoss: 157.0887451171875\n",
      "========================================\n",
      "Epoch 526/1000 - partial_train_loss: 69.7567 \n",
      "sorting training set\n",
      "Epoch 526/1000 - Training loss: 102.2377 \n",
      "========================================\n",
      "Epoch: [526/1000], TrainLoss: 109.78400957258702\n",
      "training Loss has not improved for 249 epochs.\n",
      "current in epoch    526      batch 0\n",
      "RLoss: 88.36563873291016\n",
      "current in epoch    526      batch 1\n",
      "RLoss: 136.38844299316406\n",
      "current in epoch    526      batch 2\n",
      "RLoss: 163.79922485351562\n",
      "current in epoch    526      batch 3\n",
      "RLoss: 615.8590698242188\n",
      "current in epoch    526      batch 4\n",
      "RLoss: 36.500343322753906\n",
      "current in epoch    526      batch 5\n",
      "RLoss: 377.7549743652344\n",
      "========================================\n",
      "Epoch 527/1000 - partial_train_loss: 294.4424 \n",
      "Epoch: [527/1000], TrainLoss: 280.81405912126814\n",
      "training Loss has not improved for 250 epochs.\n",
      "current in epoch    527      batch 0\n",
      "RLoss: 32.22126388549805\n",
      "current in epoch    527      batch 1\n",
      "RLoss: 276.5809020996094\n",
      "current in epoch    527      batch 2\n",
      "RLoss: 205.90528869628906\n",
      "current in epoch    527      batch 3\n",
      "RLoss: 101.66407012939453\n",
      "current in epoch    527      batch 4\n",
      "RLoss: 207.84512329101562\n",
      "current in epoch    527      batch 5\n",
      "RLoss: 84.0179443359375\n",
      "========================================\n",
      "Epoch 528/1000 - partial_train_loss: 219.7462 \n",
      "Epoch: [528/1000], TrainLoss: 65.41766820635114\n",
      "training Loss has not improved for 251 epochs.\n",
      "current in epoch    528      batch 0\n",
      "RLoss: 2572.576904296875\n",
      "current in epoch    528      batch 1\n",
      "RLoss: 308.46722412109375\n",
      "current in epoch    528      batch 2\n",
      "RLoss: 72.59901428222656\n",
      "current in epoch    528      batch 3\n",
      "RLoss: 77.39257049560547\n",
      "current in epoch    528      batch 4\n",
      "RLoss: 47.49895095825195\n",
      "current in epoch    528      batch 5\n",
      "RLoss: 137.61866760253906\n",
      "========================================\n",
      "Epoch 529/1000 - partial_train_loss: 410.6323 \n",
      "Epoch: [529/1000], TrainLoss: 119.79069301060268\n",
      "training Loss has not improved for 252 epochs.\n",
      "current in epoch    529      batch 0\n",
      "RLoss: 578.1826171875\n",
      "current in epoch    529      batch 1\n",
      "RLoss: 908.827392578125\n",
      "current in epoch    529      batch 2\n",
      "RLoss: 1993.5179443359375\n",
      "current in epoch    529      batch 3\n",
      "RLoss: 612.9138793945312\n",
      "current in epoch    529      batch 4\n",
      "RLoss: 325.8594055175781\n",
      "current in epoch    529      batch 5\n",
      "RLoss: 21.429304122924805\n",
      "========================================\n",
      "Epoch 530/1000 - partial_train_loss: 572.1242 \n",
      "Epoch: [530/1000], TrainLoss: 28.10963307108198\n",
      "training Loss has not improved for 253 epochs.\n",
      "current in epoch    530      batch 0\n",
      "RLoss: 62.254478454589844\n",
      "current in epoch    530      batch 1\n",
      "RLoss: 133.1307830810547\n",
      "current in epoch    530      batch 2\n",
      "RLoss: 194.74693298339844\n",
      "current in epoch    530      batch 3\n",
      "RLoss: 40.490753173828125\n",
      "current in epoch    530      batch 4\n",
      "RLoss: 23.0745906829834\n",
      "current in epoch    530      batch 5\n",
      "RLoss: 58.4755744934082\n",
      "========================================\n",
      "Epoch 531/1000 - partial_train_loss: 65.8964 \n",
      "sorting training set\n",
      "Epoch 531/1000 - Training loss: 58.0023 \n",
      "========================================\n",
      "Epoch: [531/1000], TrainLoss: 61.545220963771776\n",
      "training Loss has not improved for 254 epochs.\n",
      "current in epoch    531      batch 0\n",
      "RLoss: 150.09390258789062\n",
      "current in epoch    531      batch 1\n",
      "RLoss: 6.436931610107422\n",
      "current in epoch    531      batch 2\n",
      "RLoss: 32.49655532836914\n",
      "current in epoch    531      batch 3\n",
      "RLoss: 29.946657180786133\n",
      "current in epoch    531      batch 4\n",
      "RLoss: 30.123289108276367\n",
      "current in epoch    531      batch 5\n",
      "RLoss: 114.71956634521484\n",
      "========================================\n",
      "Epoch 532/1000 - partial_train_loss: 65.1720 \n",
      "Epoch: [532/1000], TrainLoss: 105.87370954241071\n",
      "training Loss has not improved for 255 epochs.\n",
      "current in epoch    532      batch 0\n",
      "RLoss: 719.5403442382812\n",
      "current in epoch    532      batch 1\n",
      "RLoss: 148.75619506835938\n",
      "current in epoch    532      batch 2\n",
      "RLoss: 72.30906677246094\n",
      "current in epoch    532      batch 3\n",
      "RLoss: 389.90277099609375\n",
      "current in epoch    532      batch 4\n",
      "RLoss: 7.768204212188721\n",
      "current in epoch    532      batch 5\n",
      "RLoss: 50.75211715698242\n",
      "========================================\n",
      "Epoch 533/1000 - partial_train_loss: 149.0392 \n",
      "Epoch: [533/1000], TrainLoss: 47.195023672921316\n",
      "training Loss has not improved for 256 epochs.\n",
      "current in epoch    533      batch 0\n",
      "RLoss: 422.67840576171875\n",
      "current in epoch    533      batch 1\n",
      "RLoss: 320.59912109375\n",
      "current in epoch    533      batch 2\n",
      "RLoss: 33.251861572265625\n",
      "current in epoch    533      batch 3\n",
      "RLoss: 19.734668731689453\n",
      "current in epoch    533      batch 4\n",
      "RLoss: 42.26535415649414\n",
      "current in epoch    533      batch 5\n",
      "RLoss: 151.3327178955078\n",
      "========================================\n",
      "Epoch 534/1000 - partial_train_loss: 145.0700 \n",
      "Epoch: [534/1000], TrainLoss: 130.6532372065953\n",
      "training Loss has not improved for 257 epochs.\n",
      "current in epoch    534      batch 0\n",
      "RLoss: 645.4470825195312\n",
      "current in epoch    534      batch 1\n",
      "RLoss: 127.30316925048828\n",
      "current in epoch    534      batch 2\n",
      "RLoss: 51.429954528808594\n",
      "current in epoch    534      batch 3\n",
      "RLoss: 78.13258361816406\n",
      "current in epoch    534      batch 4\n",
      "RLoss: 8.671828269958496\n",
      "current in epoch    534      batch 5\n",
      "RLoss: 8.668970108032227\n",
      "========================================\n",
      "Epoch 535/1000 - partial_train_loss: 209.0802 \n",
      "Epoch: [535/1000], TrainLoss: 9.607119645391192\n",
      "training Loss has not improved for 258 epochs.\n",
      "current in epoch    535      batch 0\n",
      "RLoss: 29.177274703979492\n",
      "current in epoch    535      batch 1\n",
      "RLoss: 42.014278411865234\n",
      "current in epoch    535      batch 2\n",
      "RLoss: 147.74375915527344\n",
      "current in epoch    535      batch 3\n",
      "RLoss: 39.34895324707031\n",
      "current in epoch    535      batch 4\n",
      "RLoss: 3.613502264022827\n",
      "current in epoch    535      batch 5\n",
      "RLoss: 3.9750733375549316\n",
      "========================================\n",
      "Epoch 536/1000 - partial_train_loss: 40.6260 \n",
      "sorting training set\n",
      "Epoch 536/1000 - Training loss: 3.1882 \n",
      "========================================\n",
      "Epoch: [536/1000], TrainLoss: 3.7167863527756375\n",
      "training Loss has not improved for 259 epochs.\n",
      "current in epoch    536      batch 0\n",
      "RLoss: 59.05356216430664\n",
      "current in epoch    536      batch 1\n",
      "RLoss: 9.63522720336914\n",
      "current in epoch    536      batch 2\n",
      "RLoss: 49.58240509033203\n",
      "current in epoch    536      batch 3\n",
      "RLoss: 15.438039779663086\n",
      "current in epoch    536      batch 4\n",
      "RLoss: 1051.57177734375\n",
      "current in epoch    536      batch 5\n",
      "RLoss: 13.999155044555664\n",
      "========================================\n",
      "Epoch 537/1000 - partial_train_loss: 178.3051 \n",
      "Epoch: [537/1000], TrainLoss: 16.795106206621444\n",
      "training Loss has not improved for 260 epochs.\n",
      "current in epoch    537      batch 0\n",
      "RLoss: 110.71855926513672\n",
      "current in epoch    537      batch 1\n",
      "RLoss: 264.1593322753906\n",
      "current in epoch    537      batch 2\n",
      "RLoss: 20.91912841796875\n",
      "current in epoch    537      batch 3\n",
      "RLoss: 20.34644889831543\n",
      "current in epoch    537      batch 4\n",
      "RLoss: 352.54730224609375\n",
      "current in epoch    537      batch 5\n",
      "RLoss: 101.18779754638672\n",
      "========================================\n",
      "Epoch 538/1000 - partial_train_loss: 125.1089 \n",
      "Epoch: [538/1000], TrainLoss: 77.46743352072579\n",
      "training Loss has not improved for 261 epochs.\n",
      "current in epoch    538      batch 0\n",
      "RLoss: 201.12696838378906\n",
      "current in epoch    538      batch 1\n",
      "RLoss: 14.582527160644531\n",
      "current in epoch    538      batch 2\n",
      "RLoss: 21.334125518798828\n",
      "current in epoch    538      batch 3\n",
      "RLoss: 77.6650161743164\n",
      "current in epoch    538      batch 4\n",
      "RLoss: 54.336456298828125\n",
      "current in epoch    538      batch 5\n",
      "RLoss: 152.9141082763672\n",
      "========================================\n",
      "Epoch 539/1000 - partial_train_loss: 76.4379 \n",
      "Epoch: [539/1000], TrainLoss: 159.4871153150286\n",
      "training Loss has not improved for 262 epochs.\n",
      "current in epoch    539      batch 0\n",
      "RLoss: 307.50897216796875\n",
      "current in epoch    539      batch 1\n",
      "RLoss: 223.37245178222656\n",
      "current in epoch    539      batch 2\n",
      "RLoss: 55.05390548706055\n",
      "current in epoch    539      batch 3\n",
      "RLoss: 76.2476806640625\n",
      "current in epoch    539      batch 4\n",
      "RLoss: 89.21122741699219\n",
      "current in epoch    539      batch 5\n",
      "RLoss: 148.000732421875\n",
      "========================================\n",
      "Epoch 540/1000 - partial_train_loss: 123.4763 \n",
      "Epoch: [540/1000], TrainLoss: 148.8074210030692\n",
      "training Loss has not improved for 263 epochs.\n",
      "current in epoch    540      batch 0\n",
      "RLoss: 3.135295867919922\n",
      "current in epoch    540      batch 1\n",
      "RLoss: 54.97617721557617\n",
      "current in epoch    540      batch 2\n",
      "RLoss: 308.6302185058594\n",
      "current in epoch    540      batch 3\n",
      "RLoss: 18.36327362060547\n",
      "current in epoch    540      batch 4\n",
      "RLoss: 1103.69091796875\n",
      "current in epoch    540      batch 5\n",
      "RLoss: 83.68408966064453\n",
      "========================================\n",
      "Epoch 541/1000 - partial_train_loss: 248.0958 \n",
      "sorting training set\n",
      "Epoch 541/1000 - Training loss: 94.7070 \n",
      "========================================\n",
      "Epoch: [541/1000], TrainLoss: 101.26752352634396\n",
      "training Loss has not improved for 264 epochs.\n",
      "current in epoch    541      batch 0\n",
      "RLoss: 12.274949073791504\n",
      "current in epoch    541      batch 1\n",
      "RLoss: 27.137664794921875\n",
      "current in epoch    541      batch 2\n",
      "RLoss: 16.144317626953125\n",
      "current in epoch    541      batch 3\n",
      "RLoss: 222.9823760986328\n",
      "current in epoch    541      batch 4\n",
      "RLoss: 10.498806953430176\n",
      "current in epoch    541      batch 5\n",
      "RLoss: 163.3406524658203\n",
      "========================================\n",
      "Epoch 542/1000 - partial_train_loss: 108.0102 \n",
      "Epoch: [542/1000], TrainLoss: 126.81545175824847\n",
      "training Loss has not improved for 265 epochs.\n",
      "current in epoch    542      batch 0\n",
      "RLoss: 65.00482177734375\n",
      "current in epoch    542      batch 1\n",
      "RLoss: 170.43260192871094\n",
      "current in epoch    542      batch 2\n",
      "RLoss: 207.18356323242188\n",
      "current in epoch    542      batch 3\n",
      "RLoss: 112.9329605102539\n",
      "current in epoch    542      batch 4\n",
      "RLoss: 29.02486228942871\n",
      "current in epoch    542      batch 5\n",
      "RLoss: 48.12950134277344\n",
      "========================================\n",
      "Epoch 543/1000 - partial_train_loss: 132.6666 \n",
      "Epoch: [543/1000], TrainLoss: 46.225729601723806\n",
      "training Loss has not improved for 266 epochs.\n",
      "current in epoch    543      batch 0\n",
      "RLoss: 41.34329605102539\n",
      "current in epoch    543      batch 1\n",
      "RLoss: 9.086984634399414\n",
      "current in epoch    543      batch 2\n",
      "RLoss: 22.86761474609375\n",
      "current in epoch    543      batch 3\n",
      "RLoss: 28.921627044677734\n",
      "current in epoch    543      batch 4\n",
      "RLoss: 154.29983520507812\n",
      "current in epoch    543      batch 5\n",
      "RLoss: 186.7913055419922\n",
      "========================================\n",
      "Epoch 544/1000 - partial_train_loss: 58.3260 \n",
      "Epoch: [544/1000], TrainLoss: 154.52643040248327\n",
      "training Loss has not improved for 267 epochs.\n",
      "current in epoch    544      batch 0\n",
      "RLoss: 94.12907409667969\n",
      "current in epoch    544      batch 1\n",
      "RLoss: 186.03335571289062\n",
      "current in epoch    544      batch 2\n",
      "RLoss: 114.56890106201172\n",
      "current in epoch    544      batch 3\n",
      "RLoss: 8.463406562805176\n",
      "current in epoch    544      batch 4\n",
      "RLoss: 10.65688705444336\n",
      "current in epoch    544      batch 5\n",
      "RLoss: 6.066826820373535\n",
      "========================================\n",
      "Epoch 545/1000 - partial_train_loss: 101.9498 \n",
      "Epoch: [545/1000], TrainLoss: 8.87891171659742\n",
      "training Loss has not improved for 268 epochs.\n",
      "current in epoch    545      batch 0\n",
      "RLoss: 20.963491439819336\n",
      "current in epoch    545      batch 1\n",
      "RLoss: 19.162538528442383\n",
      "current in epoch    545      batch 2\n",
      "RLoss: 224.01905822753906\n",
      "current in epoch    545      batch 3\n",
      "RLoss: 67.28120422363281\n",
      "current in epoch    545      batch 4\n",
      "RLoss: 74.25581359863281\n",
      "current in epoch    545      batch 5\n",
      "RLoss: 70.35052490234375\n",
      "========================================\n",
      "Epoch 546/1000 - partial_train_loss: 78.4925 \n",
      "sorting training set\n",
      "Epoch 546/1000 - Training loss: 52.0228 \n",
      "========================================\n",
      "Epoch: [546/1000], TrainLoss: 55.611593586470086\n",
      "training Loss has not improved for 269 epochs.\n",
      "current in epoch    546      batch 0\n",
      "RLoss: 83.2464370727539\n",
      "current in epoch    546      batch 1\n",
      "RLoss: 108.62007904052734\n",
      "current in epoch    546      batch 2\n",
      "RLoss: 109.38277435302734\n",
      "current in epoch    546      batch 3\n",
      "RLoss: 22.327299118041992\n",
      "current in epoch    546      batch 4\n",
      "RLoss: 10.080719947814941\n",
      "current in epoch    546      batch 5\n",
      "RLoss: 112.61442565917969\n",
      "========================================\n",
      "Epoch 547/1000 - partial_train_loss: 103.3055 \n",
      "Epoch: [547/1000], TrainLoss: 81.957688331604\n",
      "training Loss has not improved for 270 epochs.\n",
      "current in epoch    547      batch 0\n",
      "RLoss: 21.86471176147461\n",
      "current in epoch    547      batch 1\n",
      "RLoss: 77.32022857666016\n",
      "current in epoch    547      batch 2\n",
      "RLoss: 41.373043060302734\n",
      "current in epoch    547      batch 3\n",
      "RLoss: 160.84339904785156\n",
      "current in epoch    547      batch 4\n",
      "RLoss: 100.78923034667969\n",
      "current in epoch    547      batch 5\n",
      "RLoss: 17.161842346191406\n",
      "========================================\n",
      "Epoch 548/1000 - partial_train_loss: 90.9557 \n",
      "Epoch: [548/1000], TrainLoss: 15.073348879814148\n",
      "training Loss has not improved for 271 epochs.\n",
      "current in epoch    548      batch 0\n",
      "RLoss: 16.46815299987793\n",
      "current in epoch    548      batch 1\n",
      "RLoss: 51.5101318359375\n",
      "current in epoch    548      batch 2\n",
      "RLoss: 8.870070457458496\n",
      "current in epoch    548      batch 3\n",
      "RLoss: 472.1719970703125\n",
      "current in epoch    548      batch 4\n",
      "RLoss: 461.471435546875\n",
      "current in epoch    548      batch 5\n",
      "RLoss: 203.289794921875\n",
      "========================================\n",
      "Epoch 549/1000 - partial_train_loss: 171.6508 \n",
      "Epoch: [549/1000], TrainLoss: 143.4449806213379\n",
      "training Loss has not improved for 272 epochs.\n",
      "current in epoch    549      batch 0\n",
      "RLoss: 173.2718048095703\n",
      "current in epoch    549      batch 1\n",
      "RLoss: 42.294334411621094\n",
      "current in epoch    549      batch 2\n",
      "RLoss: 100.45323181152344\n",
      "current in epoch    549      batch 3\n",
      "RLoss: 29.94611358642578\n",
      "current in epoch    549      batch 4\n",
      "RLoss: 32.0388069152832\n",
      "current in epoch    549      batch 5\n",
      "RLoss: 123.62077331542969\n",
      "========================================\n",
      "Epoch 550/1000 - partial_train_loss: 128.4098 \n",
      "Epoch: [550/1000], TrainLoss: 118.31002153669085\n",
      "training Loss has not improved for 273 epochs.\n",
      "current in epoch    550      batch 0\n",
      "RLoss: 28.3565673828125\n",
      "current in epoch    550      batch 1\n",
      "RLoss: 17.352252960205078\n",
      "current in epoch    550      batch 2\n",
      "RLoss: 46.48859405517578\n",
      "current in epoch    550      batch 3\n",
      "RLoss: 17.694747924804688\n",
      "current in epoch    550      batch 4\n",
      "RLoss: 43.724876403808594\n",
      "current in epoch    550      batch 5\n",
      "RLoss: 54.19622039794922\n",
      "========================================\n",
      "Epoch 551/1000 - partial_train_loss: 54.8012 \n",
      "sorting training set\n",
      "Epoch 551/1000 - Training loss: 77.5966 \n",
      "========================================\n",
      "Epoch: [551/1000], TrainLoss: 84.60833775464393\n",
      "training Loss has not improved for 274 epochs.\n",
      "current in epoch    551      batch 0\n",
      "RLoss: 328.699951171875\n",
      "current in epoch    551      batch 1\n",
      "RLoss: 1265.749755859375\n",
      "current in epoch    551      batch 2\n",
      "RLoss: 104.59974670410156\n",
      "current in epoch    551      batch 3\n",
      "RLoss: 4.179994106292725\n",
      "current in epoch    551      batch 4\n",
      "RLoss: 91.51815795898438\n",
      "current in epoch    551      batch 5\n",
      "RLoss: 13.333667755126953\n",
      "========================================\n",
      "Epoch 552/1000 - partial_train_loss: 365.5426 \n",
      "Epoch: [552/1000], TrainLoss: 15.295471838542394\n",
      "training Loss has not improved for 275 epochs.\n",
      "current in epoch    552      batch 0\n",
      "RLoss: 2.098604679107666\n",
      "current in epoch    552      batch 1\n",
      "RLoss: 80.23847198486328\n",
      "current in epoch    552      batch 2\n",
      "RLoss: 30.979034423828125\n",
      "current in epoch    552      batch 3\n",
      "RLoss: 18.168468475341797\n",
      "current in epoch    552      batch 4\n",
      "RLoss: 4.671699523925781\n",
      "current in epoch    552      batch 5\n",
      "RLoss: 92.7892837524414\n",
      "========================================\n",
      "Epoch 553/1000 - partial_train_loss: 44.1674 \n",
      "Epoch: [553/1000], TrainLoss: 115.80672509329659\n",
      "training Loss has not improved for 276 epochs.\n",
      "current in epoch    553      batch 0\n",
      "RLoss: 34.4947395324707\n",
      "current in epoch    553      batch 1\n",
      "RLoss: 450.83380126953125\n",
      "current in epoch    553      batch 2\n",
      "RLoss: 21.53853988647461\n",
      "current in epoch    553      batch 3\n",
      "RLoss: 71.5787582397461\n",
      "current in epoch    553      batch 4\n",
      "RLoss: 36.16666030883789\n",
      "current in epoch    553      batch 5\n",
      "RLoss: 141.4612274169922\n",
      "========================================\n",
      "Epoch 554/1000 - partial_train_loss: 107.3773 \n",
      "Epoch: [554/1000], TrainLoss: 154.77865954807825\n",
      "training Loss has not improved for 277 epochs.\n",
      "current in epoch    554      batch 0\n",
      "RLoss: 442.2651062011719\n",
      "current in epoch    554      batch 1\n",
      "RLoss: 563.411376953125\n",
      "current in epoch    554      batch 2\n",
      "RLoss: 51.42619705200195\n",
      "current in epoch    554      batch 3\n",
      "RLoss: 74.53802490234375\n",
      "current in epoch    554      batch 4\n",
      "RLoss: 27.850128173828125\n",
      "current in epoch    554      batch 5\n",
      "RLoss: 30.130430221557617\n",
      "========================================\n",
      "Epoch 555/1000 - partial_train_loss: 185.2690 \n",
      "Epoch: [555/1000], TrainLoss: 19.602901935577393\n",
      "training Loss has not improved for 278 epochs.\n",
      "current in epoch    555      batch 0\n",
      "RLoss: 144.9421844482422\n",
      "current in epoch    555      batch 1\n",
      "RLoss: 11.108325004577637\n",
      "current in epoch    555      batch 2\n",
      "RLoss: 32.837093353271484\n",
      "current in epoch    555      batch 3\n",
      "RLoss: 53.623043060302734\n",
      "current in epoch    555      batch 4\n",
      "RLoss: 30.30671501159668\n",
      "current in epoch    555      batch 5\n",
      "RLoss: 162.05177307128906\n",
      "========================================\n",
      "Epoch 556/1000 - partial_train_loss: 51.7013 \n",
      "sorting training set\n",
      "Epoch 556/1000 - Training loss: 205.3582 \n",
      "========================================\n",
      "Epoch: [556/1000], TrainLoss: 219.37171397515243\n",
      "training Loss has not improved for 279 epochs.\n",
      "current in epoch    556      batch 0\n",
      "RLoss: 240.91390991210938\n",
      "current in epoch    556      batch 1\n",
      "RLoss: 14.980311393737793\n",
      "current in epoch    556      batch 2\n",
      "RLoss: 80.05204772949219\n",
      "current in epoch    556      batch 3\n",
      "RLoss: 53.14527130126953\n",
      "current in epoch    556      batch 4\n",
      "RLoss: 109.75838470458984\n",
      "current in epoch    556      batch 5\n",
      "RLoss: 10.256598472595215\n",
      "========================================\n",
      "Epoch 557/1000 - partial_train_loss: 153.6019 \n",
      "Epoch: [557/1000], TrainLoss: 112.21484688350132\n",
      "training Loss has not improved for 280 epochs.\n",
      "current in epoch    557      batch 0\n",
      "RLoss: 230.80918884277344\n",
      "current in epoch    557      batch 1\n",
      "RLoss: 10.604119300842285\n",
      "current in epoch    557      batch 2\n",
      "RLoss: 15.81612491607666\n",
      "current in epoch    557      batch 3\n",
      "RLoss: 79.55679321289062\n",
      "current in epoch    557      batch 4\n",
      "RLoss: 33.22004699707031\n",
      "current in epoch    557      batch 5\n",
      "RLoss: 30.20334243774414\n",
      "========================================\n",
      "Epoch 558/1000 - partial_train_loss: 45.6219 \n",
      "Epoch: [558/1000], TrainLoss: 23.733963319233485\n",
      "training Loss has not improved for 281 epochs.\n",
      "current in epoch    558      batch 0\n",
      "RLoss: 969.3208618164062\n",
      "current in epoch    558      batch 1\n",
      "RLoss: 25.395444869995117\n",
      "current in epoch    558      batch 2\n",
      "RLoss: 278.53076171875\n",
      "current in epoch    558      batch 3\n",
      "RLoss: 64.29545593261719\n",
      "current in epoch    558      batch 4\n",
      "RLoss: 135.79637145996094\n",
      "current in epoch    558      batch 5\n",
      "RLoss: 22.307353973388672\n",
      "========================================\n",
      "Epoch 559/1000 - partial_train_loss: 159.7393 \n",
      "Epoch: [559/1000], TrainLoss: 29.62204803739275\n",
      "training Loss has not improved for 282 epochs.\n",
      "current in epoch    559      batch 0\n",
      "RLoss: 31.959638595581055\n",
      "current in epoch    559      batch 1\n",
      "RLoss: 122.80400848388672\n",
      "current in epoch    559      batch 2\n",
      "RLoss: 25.143444061279297\n",
      "current in epoch    559      batch 3\n",
      "RLoss: 29.759544372558594\n",
      "current in epoch    559      batch 4\n",
      "RLoss: 14.190262794494629\n",
      "current in epoch    559      batch 5\n",
      "RLoss: 222.5644073486328\n",
      "========================================\n",
      "Epoch 560/1000 - partial_train_loss: 40.0840 \n",
      "Epoch: [560/1000], TrainLoss: 246.7422834123884\n",
      "training Loss has not improved for 283 epochs.\n",
      "current in epoch    560      batch 0\n",
      "RLoss: 57.217079162597656\n",
      "current in epoch    560      batch 1\n",
      "RLoss: 8.961362838745117\n",
      "current in epoch    560      batch 2\n",
      "RLoss: 73.75273132324219\n",
      "current in epoch    560      batch 3\n",
      "RLoss: 727.563232421875\n",
      "current in epoch    560      batch 4\n",
      "RLoss: 458.4100036621094\n",
      "current in epoch    560      batch 5\n",
      "RLoss: 32.925113677978516\n",
      "========================================\n",
      "Epoch 561/1000 - partial_train_loss: 276.6593 \n",
      "sorting training set\n",
      "Epoch 561/1000 - Training loss: 40.0035 \n",
      "========================================\n",
      "Epoch: [561/1000], TrainLoss: 43.00216816189345\n",
      "training Loss has not improved for 284 epochs.\n",
      "current in epoch    561      batch 0\n",
      "RLoss: 133.19728088378906\n",
      "current in epoch    561      batch 1\n",
      "RLoss: 112.96564483642578\n",
      "current in epoch    561      batch 2\n",
      "RLoss: 91.68071746826172\n",
      "current in epoch    561      batch 3\n",
      "RLoss: 136.9539794921875\n",
      "current in epoch    561      batch 4\n",
      "RLoss: 140.56036376953125\n",
      "current in epoch    561      batch 5\n",
      "RLoss: 66.28138732910156\n",
      "========================================\n",
      "Epoch 562/1000 - partial_train_loss: 147.6245 \n",
      "Epoch: [562/1000], TrainLoss: 61.30823394230434\n",
      "training Loss has not improved for 285 epochs.\n",
      "current in epoch    562      batch 0\n",
      "RLoss: 137.0794219970703\n",
      "current in epoch    562      batch 1\n",
      "RLoss: 50.26033020019531\n",
      "current in epoch    562      batch 2\n",
      "RLoss: 12.466583251953125\n",
      "current in epoch    562      batch 3\n",
      "RLoss: 79.9217529296875\n",
      "current in epoch    562      batch 4\n",
      "RLoss: 193.2067413330078\n",
      "current in epoch    562      batch 5\n",
      "RLoss: 13.848186492919922\n",
      "========================================\n",
      "Epoch 563/1000 - partial_train_loss: 83.0569 \n",
      "Epoch: [563/1000], TrainLoss: 12.704479234559196\n",
      "training Loss has not improved for 286 epochs.\n",
      "current in epoch    563      batch 0\n",
      "RLoss: 42.00016784667969\n",
      "current in epoch    563      batch 1\n",
      "RLoss: 9.26904010772705\n",
      "current in epoch    563      batch 2\n",
      "RLoss: 327.5256042480469\n",
      "current in epoch    563      batch 3\n",
      "RLoss: 41.60752868652344\n",
      "current in epoch    563      batch 4\n",
      "RLoss: 267.4407958984375\n",
      "current in epoch    563      batch 5\n",
      "RLoss: 345.37725830078125\n",
      "========================================\n",
      "Epoch 564/1000 - partial_train_loss: 127.3516 \n",
      "Epoch: [564/1000], TrainLoss: 294.5679223196847\n",
      "training Loss has not improved for 287 epochs.\n",
      "current in epoch    564      batch 0\n",
      "RLoss: 166.5420379638672\n",
      "current in epoch    564      batch 1\n",
      "RLoss: 50.78007125854492\n",
      "current in epoch    564      batch 2\n",
      "RLoss: 5.632968425750732\n",
      "current in epoch    564      batch 3\n",
      "RLoss: 35.147117614746094\n",
      "current in epoch    564      batch 4\n",
      "RLoss: 18.88761329650879\n",
      "current in epoch    564      batch 5\n",
      "RLoss: 13.137068748474121\n",
      "========================================\n",
      "Epoch 565/1000 - partial_train_loss: 142.7312 \n",
      "Epoch: [565/1000], TrainLoss: 14.441321066447667\n",
      "training Loss has not improved for 288 epochs.\n",
      "current in epoch    565      batch 0\n",
      "RLoss: 82.80814361572266\n",
      "current in epoch    565      batch 1\n",
      "RLoss: 14.684576034545898\n",
      "current in epoch    565      batch 2\n",
      "RLoss: 9.371678352355957\n",
      "current in epoch    565      batch 3\n",
      "RLoss: 46.758792877197266\n",
      "current in epoch    565      batch 4\n",
      "RLoss: 26.027099609375\n",
      "current in epoch    565      batch 5\n",
      "RLoss: 35.74677658081055\n",
      "========================================\n",
      "Epoch 566/1000 - partial_train_loss: 34.7426 \n",
      "sorting training set\n",
      "Epoch 566/1000 - Training loss: 34.4779 \n",
      "========================================\n",
      "Epoch: [566/1000], TrainLoss: 36.89546735085459\n",
      "training Loss has not improved for 289 epochs.\n",
      "current in epoch    566      batch 0\n",
      "RLoss: 171.1410369873047\n",
      "current in epoch    566      batch 1\n",
      "RLoss: 50.00803756713867\n",
      "current in epoch    566      batch 2\n",
      "RLoss: 1182.5450439453125\n",
      "current in epoch    566      batch 3\n",
      "RLoss: 55.151832580566406\n",
      "current in epoch    566      batch 4\n",
      "RLoss: 2542.89697265625\n",
      "current in epoch    566      batch 5\n",
      "RLoss: 28.789583206176758\n",
      "========================================\n",
      "Epoch 567/1000 - partial_train_loss: 625.2959 \n",
      "Epoch: [567/1000], TrainLoss: 40.014666250773836\n",
      "training Loss has not improved for 290 epochs.\n",
      "current in epoch    567      batch 0\n",
      "RLoss: 320.0547790527344\n",
      "current in epoch    567      batch 1\n",
      "RLoss: 123.5153579711914\n",
      "current in epoch    567      batch 2\n",
      "RLoss: 197.8042755126953\n",
      "current in epoch    567      batch 3\n",
      "RLoss: 26.068687438964844\n",
      "current in epoch    567      batch 4\n",
      "RLoss: 97.63468933105469\n",
      "current in epoch    567      batch 5\n",
      "RLoss: 366.67572021484375\n",
      "========================================\n",
      "Epoch 568/1000 - partial_train_loss: 103.7383 \n",
      "Epoch: [568/1000], TrainLoss: 565.9986931937082\n",
      "training Loss has not improved for 291 epochs.\n",
      "current in epoch    568      batch 0\n",
      "RLoss: 82.21304321289062\n",
      "current in epoch    568      batch 1\n",
      "RLoss: 342.6933288574219\n",
      "current in epoch    568      batch 2\n",
      "RLoss: 163.4488525390625\n",
      "current in epoch    568      batch 3\n",
      "RLoss: 562.644287109375\n",
      "current in epoch    568      batch 4\n",
      "RLoss: 13.963513374328613\n",
      "current in epoch    568      batch 5\n",
      "RLoss: 39.258914947509766\n",
      "========================================\n",
      "Epoch 569/1000 - partial_train_loss: 236.6801 \n",
      "Epoch: [569/1000], TrainLoss: 29.59432521888188\n",
      "training Loss has not improved for 292 epochs.\n",
      "current in epoch    569      batch 0\n",
      "RLoss: 14.070505142211914\n",
      "current in epoch    569      batch 1\n",
      "RLoss: 202.67831420898438\n",
      "current in epoch    569      batch 2\n",
      "RLoss: 105.0584945678711\n",
      "current in epoch    569      batch 3\n",
      "RLoss: 224.1652069091797\n",
      "current in epoch    569      batch 4\n",
      "RLoss: 348.1476135253906\n",
      "current in epoch    569      batch 5\n",
      "RLoss: 507.20635986328125\n",
      "========================================\n",
      "Epoch 570/1000 - partial_train_loss: 172.4937 \n",
      "Epoch: [570/1000], TrainLoss: 525.2808347429548\n",
      "training Loss has not improved for 293 epochs.\n",
      "current in epoch    570      batch 0\n",
      "RLoss: 789.3505249023438\n",
      "current in epoch    570      batch 1\n",
      "RLoss: 95.02042388916016\n",
      "current in epoch    570      batch 2\n",
      "RLoss: 133.2913818359375\n",
      "current in epoch    570      batch 3\n",
      "RLoss: 56.464656829833984\n",
      "current in epoch    570      batch 4\n",
      "RLoss: 7.884761333465576\n",
      "current in epoch    570      batch 5\n",
      "RLoss: 6.135974407196045\n",
      "========================================\n",
      "Epoch 571/1000 - partial_train_loss: 302.2180 \n",
      "sorting training set\n",
      "Epoch 571/1000 - Training loss: 19.6833 \n",
      "========================================\n",
      "Epoch: [571/1000], TrainLoss: 20.997312551991463\n",
      "training Loss has not improved for 294 epochs.\n",
      "current in epoch    571      batch 0\n",
      "RLoss: 69.55754089355469\n",
      "current in epoch    571      batch 1\n",
      "RLoss: 144.12466430664062\n",
      "current in epoch    571      batch 2\n",
      "RLoss: 13.626757621765137\n",
      "current in epoch    571      batch 3\n",
      "RLoss: 11.77413272857666\n",
      "current in epoch    571      batch 4\n",
      "RLoss: 3.118443012237549\n",
      "current in epoch    571      batch 5\n",
      "RLoss: 31.894750595092773\n",
      "========================================\n",
      "Epoch 572/1000 - partial_train_loss: 90.4096 \n",
      "Epoch: [572/1000], TrainLoss: 28.03405918393816\n",
      "training Loss has not improved for 295 epochs.\n",
      "current in epoch    572      batch 0\n",
      "RLoss: 61.573326110839844\n",
      "current in epoch    572      batch 1\n",
      "RLoss: 32.75492858886719\n",
      "current in epoch    572      batch 2\n",
      "RLoss: 175.84222412109375\n",
      "current in epoch    572      batch 3\n",
      "RLoss: 42.16358947753906\n",
      "current in epoch    572      batch 4\n",
      "RLoss: 152.94308471679688\n",
      "current in epoch    572      batch 5\n",
      "RLoss: 186.7755889892578\n",
      "========================================\n",
      "Epoch 573/1000 - partial_train_loss: 92.9870 \n",
      "Epoch: [573/1000], TrainLoss: 225.24140058244978\n",
      "training Loss has not improved for 296 epochs.\n",
      "current in epoch    573      batch 0\n",
      "RLoss: 46.18427276611328\n",
      "current in epoch    573      batch 1\n",
      "RLoss: 15.713671684265137\n",
      "current in epoch    573      batch 2\n",
      "RLoss: 9.202584266662598\n",
      "current in epoch    573      batch 3\n",
      "RLoss: 12.181753158569336\n",
      "current in epoch    573      batch 4\n",
      "RLoss: 3.716155529022217\n",
      "current in epoch    573      batch 5\n",
      "RLoss: 57.9773063659668\n",
      "========================================\n",
      "Epoch 574/1000 - partial_train_loss: 34.0361 \n",
      "Epoch: [574/1000], TrainLoss: 97.9187936101641\n",
      "training Loss has not improved for 297 epochs.\n",
      "current in epoch    574      batch 0\n",
      "RLoss: 21.7342529296875\n",
      "current in epoch    574      batch 1\n",
      "RLoss: 13.782679557800293\n",
      "current in epoch    574      batch 2\n",
      "RLoss: 156.57894897460938\n",
      "current in epoch    574      batch 3\n",
      "RLoss: 58.789276123046875\n",
      "current in epoch    574      batch 4\n",
      "RLoss: 31.134353637695312\n",
      "current in epoch    574      batch 5\n",
      "RLoss: 5.045099258422852\n",
      "========================================\n",
      "Epoch 575/1000 - partial_train_loss: 44.8445 \n",
      "Epoch: [575/1000], TrainLoss: 8.684681245258876\n",
      "training Loss has not improved for 298 epochs.\n",
      "current in epoch    575      batch 0\n",
      "RLoss: 7.04558801651001\n",
      "current in epoch    575      batch 1\n",
      "RLoss: 41.288326263427734\n",
      "current in epoch    575      batch 2\n",
      "RLoss: 20.689472198486328\n",
      "current in epoch    575      batch 3\n",
      "RLoss: 28.21063232421875\n",
      "current in epoch    575      batch 4\n",
      "RLoss: 142.23915100097656\n",
      "current in epoch    575      batch 5\n",
      "RLoss: 9.728106498718262\n",
      "========================================\n",
      "Epoch 576/1000 - partial_train_loss: 46.1272 \n",
      "sorting training set\n",
      "Epoch 576/1000 - Training loss: 17.6879 \n",
      "========================================\n",
      "Epoch: [576/1000], TrainLoss: 19.519321556122584\n",
      "training Loss has not improved for 299 epochs.\n",
      "current in epoch    576      batch 0\n",
      "RLoss: 5.220580101013184\n",
      "current in epoch    576      batch 1\n",
      "RLoss: 55.556793212890625\n",
      "current in epoch    576      batch 2\n",
      "RLoss: 10.88409423828125\n",
      "current in epoch    576      batch 3\n",
      "RLoss: 3.9370431900024414\n",
      "current in epoch    576      batch 4\n",
      "RLoss: 148.76397705078125\n",
      "current in epoch    576      batch 5\n",
      "RLoss: 63.35385513305664\n",
      "========================================\n",
      "Epoch 577/1000 - partial_train_loss: 62.1553 \n",
      "Epoch: [577/1000], TrainLoss: 40.40507241657802\n",
      "training Loss has not improved for 300 epochs.\n",
      "current in epoch    577      batch 0\n",
      "RLoss: 173.3338623046875\n",
      "current in epoch    577      batch 1\n",
      "RLoss: 28.1115665435791\n",
      "current in epoch    577      batch 2\n",
      "RLoss: 13.558571815490723\n",
      "current in epoch    577      batch 3\n",
      "RLoss: 8.042607307434082\n",
      "current in epoch    577      batch 4\n",
      "RLoss: 18.667850494384766\n",
      "current in epoch    577      batch 5\n",
      "RLoss: 10.245197296142578\n",
      "========================================\n",
      "Epoch 578/1000 - partial_train_loss: 57.1732 \n",
      "Epoch: [578/1000], TrainLoss: 10.245817388807025\n",
      "training Loss has not improved for 301 epochs.\n",
      "current in epoch    578      batch 0\n",
      "RLoss: 32.43336868286133\n",
      "current in epoch    578      batch 1\n",
      "RLoss: 12.93799114227295\n",
      "current in epoch    578      batch 2\n",
      "RLoss: 10.876351356506348\n",
      "current in epoch    578      batch 3\n",
      "RLoss: 31.8077449798584\n",
      "current in epoch    578      batch 4\n",
      "RLoss: 11.958257675170898\n",
      "current in epoch    578      batch 5\n",
      "RLoss: 18.258581161499023\n",
      "========================================\n",
      "Epoch 579/1000 - partial_train_loss: 19.0173 \n",
      "Epoch: [579/1000], TrainLoss: 14.546785303524562\n",
      "training Loss has not improved for 302 epochs.\n",
      "current in epoch    579      batch 0\n",
      "RLoss: 32.056819915771484\n",
      "current in epoch    579      batch 1\n",
      "RLoss: 8.216493606567383\n",
      "current in epoch    579      batch 2\n",
      "RLoss: 11.75473690032959\n",
      "current in epoch    579      batch 3\n",
      "RLoss: 40.904296875\n",
      "current in epoch    579      batch 4\n",
      "RLoss: 34.659488677978516\n",
      "current in epoch    579      batch 5\n",
      "RLoss: 21.744022369384766\n",
      "========================================\n",
      "Epoch 580/1000 - partial_train_loss: 26.1176 \n",
      "Epoch: [580/1000], TrainLoss: 20.049331665039062\n",
      "training Loss has not improved for 303 epochs.\n",
      "current in epoch    580      batch 0\n",
      "RLoss: 19.849809646606445\n",
      "current in epoch    580      batch 1\n",
      "RLoss: 97.84309387207031\n",
      "current in epoch    580      batch 2\n",
      "RLoss: 41.63123321533203\n",
      "current in epoch    580      batch 3\n",
      "RLoss: 106.8770980834961\n",
      "current in epoch    580      batch 4\n",
      "RLoss: 38.559226989746094\n",
      "current in epoch    580      batch 5\n",
      "RLoss: 86.9708480834961\n",
      "========================================\n",
      "Epoch 581/1000 - partial_train_loss: 60.1514 \n",
      "sorting training set\n",
      "Epoch 581/1000 - Training loss: 52.4925 \n",
      "========================================\n",
      "Epoch: [581/1000], TrainLoss: 56.148314075752346\n",
      "training Loss has not improved for 304 epochs.\n",
      "current in epoch    581      batch 0\n",
      "RLoss: 63.41325378417969\n",
      "current in epoch    581      batch 1\n",
      "RLoss: 85.58442687988281\n",
      "current in epoch    581      batch 2\n",
      "RLoss: 58.22486877441406\n",
      "current in epoch    581      batch 3\n",
      "RLoss: 14.907512664794922\n",
      "current in epoch    581      batch 4\n",
      "RLoss: 140.0747833251953\n",
      "current in epoch    581      batch 5\n",
      "RLoss: 121.7242431640625\n",
      "========================================\n",
      "Epoch 582/1000 - partial_train_loss: 115.4847 \n",
      "Epoch: [582/1000], TrainLoss: 80.51178523472377\n",
      "training Loss has not improved for 305 epochs.\n",
      "current in epoch    582      batch 0\n",
      "RLoss: 373.618408203125\n",
      "current in epoch    582      batch 1\n",
      "RLoss: 48.851646423339844\n",
      "current in epoch    582      batch 2\n",
      "RLoss: 178.63592529296875\n",
      "current in epoch    582      batch 3\n",
      "RLoss: 319.1340637207031\n",
      "current in epoch    582      batch 4\n",
      "RLoss: 27.739351272583008\n",
      "current in epoch    582      batch 5\n",
      "RLoss: 133.71844482421875\n",
      "========================================\n",
      "Epoch 583/1000 - partial_train_loss: 199.5357 \n",
      "Epoch: [583/1000], TrainLoss: 118.89950507027763\n",
      "training Loss has not improved for 306 epochs.\n",
      "current in epoch    583      batch 0\n",
      "RLoss: 62.10454559326172\n",
      "current in epoch    583      batch 1\n",
      "RLoss: 72.67101287841797\n",
      "current in epoch    583      batch 2\n",
      "RLoss: 125.62239074707031\n",
      "current in epoch    583      batch 3\n",
      "RLoss: 7.901432514190674\n",
      "current in epoch    583      batch 4\n",
      "RLoss: 13.30233097076416\n",
      "current in epoch    583      batch 5\n",
      "RLoss: 107.35265350341797\n",
      "========================================\n",
      "Epoch 584/1000 - partial_train_loss: 77.3571 \n",
      "Epoch: [584/1000], TrainLoss: 68.09421621050153\n",
      "training Loss has not improved for 307 epochs.\n",
      "current in epoch    584      batch 0\n",
      "RLoss: 430.0351257324219\n",
      "current in epoch    584      batch 1\n",
      "RLoss: 272.3981018066406\n",
      "current in epoch    584      batch 2\n",
      "RLoss: 45.064788818359375\n",
      "current in epoch    584      batch 3\n",
      "RLoss: 24.920764923095703\n",
      "current in epoch    584      batch 4\n",
      "RLoss: 28.717937469482422\n",
      "current in epoch    584      batch 5\n",
      "RLoss: 14.557541847229004\n",
      "========================================\n",
      "Epoch 585/1000 - partial_train_loss: 139.8546 \n",
      "Epoch: [585/1000], TrainLoss: 12.530986036573138\n",
      "training Loss has not improved for 308 epochs.\n",
      "current in epoch    585      batch 0\n",
      "RLoss: 10.893095016479492\n",
      "current in epoch    585      batch 1\n",
      "RLoss: 101.24446868896484\n",
      "current in epoch    585      batch 2\n",
      "RLoss: 470.6993103027344\n",
      "current in epoch    585      batch 3\n",
      "RLoss: 465.85540771484375\n",
      "current in epoch    585      batch 4\n",
      "RLoss: 19.447484970092773\n",
      "current in epoch    585      batch 5\n",
      "RLoss: 107.2570571899414\n",
      "========================================\n",
      "Epoch 586/1000 - partial_train_loss: 170.1814 \n",
      "sorting training set\n",
      "Epoch 586/1000 - Training loss: 93.9909 \n",
      "========================================\n",
      "Epoch: [586/1000], TrainLoss: 100.06626769956567\n",
      "training Loss has not improved for 309 epochs.\n",
      "current in epoch    586      batch 0\n",
      "RLoss: 1748.2078857421875\n",
      "current in epoch    586      batch 1\n",
      "RLoss: 208.82632446289062\n",
      "current in epoch    586      batch 2\n",
      "RLoss: 379.2657165527344\n",
      "current in epoch    586      batch 3\n",
      "RLoss: 20.854528427124023\n",
      "current in epoch    586      batch 4\n",
      "RLoss: 72.1492691040039\n",
      "current in epoch    586      batch 5\n",
      "RLoss: 9.342239379882812\n",
      "========================================\n",
      "Epoch 587/1000 - partial_train_loss: 464.7169 \n",
      "Epoch: [587/1000], TrainLoss: 7.3124946015221735\n",
      "training Loss has not improved for 310 epochs.\n",
      "current in epoch    587      batch 0\n",
      "RLoss: 191.30429077148438\n",
      "current in epoch    587      batch 1\n",
      "RLoss: 104.15557861328125\n",
      "current in epoch    587      batch 2\n",
      "RLoss: 42.76747512817383\n",
      "current in epoch    587      batch 3\n",
      "RLoss: 46.94813537597656\n",
      "current in epoch    587      batch 4\n",
      "RLoss: 22.092388153076172\n",
      "current in epoch    587      batch 5\n",
      "RLoss: 46.6110954284668\n",
      "========================================\n",
      "Epoch 588/1000 - partial_train_loss: 72.9202 \n",
      "Epoch: [588/1000], TrainLoss: 33.17038198879787\n",
      "training Loss has not improved for 311 epochs.\n",
      "current in epoch    588      batch 0\n",
      "RLoss: 60.30951690673828\n",
      "current in epoch    588      batch 1\n",
      "RLoss: 72.62100219726562\n",
      "current in epoch    588      batch 2\n",
      "RLoss: 8.61201000213623\n",
      "current in epoch    588      batch 3\n",
      "RLoss: 105.17339324951172\n",
      "current in epoch    588      batch 4\n",
      "RLoss: 14.932727813720703\n",
      "current in epoch    588      batch 5\n",
      "RLoss: 14.593378067016602\n",
      "========================================\n",
      "Epoch 589/1000 - partial_train_loss: 61.6799 \n",
      "Epoch: [589/1000], TrainLoss: 12.525042567934308\n",
      "training Loss has not improved for 312 epochs.\n",
      "current in epoch    589      batch 0\n",
      "RLoss: 51.7840576171875\n",
      "current in epoch    589      batch 1\n",
      "RLoss: 15.99941635131836\n",
      "current in epoch    589      batch 2\n",
      "RLoss: 27.91361427307129\n",
      "current in epoch    589      batch 3\n",
      "RLoss: 38.052490234375\n",
      "current in epoch    589      batch 4\n",
      "RLoss: 29.074777603149414\n",
      "current in epoch    589      batch 5\n",
      "RLoss: 18.11614227294922\n",
      "========================================\n",
      "Epoch 590/1000 - partial_train_loss: 27.1354 \n",
      "Epoch: [590/1000], TrainLoss: 10.517278066703252\n",
      "training Loss has not improved for 313 epochs.\n",
      "current in epoch    590      batch 0\n",
      "RLoss: 63.27223587036133\n",
      "current in epoch    590      batch 1\n",
      "RLoss: 79.82701110839844\n",
      "current in epoch    590      batch 2\n",
      "RLoss: 247.6919708251953\n",
      "current in epoch    590      batch 3\n",
      "RLoss: 93.630615234375\n",
      "current in epoch    590      batch 4\n",
      "RLoss: 34.619056701660156\n",
      "current in epoch    590      batch 5\n",
      "RLoss: 2.925537347793579\n",
      "========================================\n",
      "Epoch 591/1000 - partial_train_loss: 92.7396 \n",
      "sorting training set\n",
      "Epoch 591/1000 - Training loss: 2.3491 \n",
      "========================================\n",
      "Epoch: [591/1000], TrainLoss: 2.5857382416656662\n",
      "training Loss has not improved for 314 epochs.\n",
      "current in epoch    591      batch 0\n",
      "RLoss: 238.00308227539062\n",
      "current in epoch    591      batch 1\n",
      "RLoss: 6.427648067474365\n",
      "current in epoch    591      batch 2\n",
      "RLoss: 38.096092224121094\n",
      "current in epoch    591      batch 3\n",
      "RLoss: 17.828231811523438\n",
      "current in epoch    591      batch 4\n",
      "RLoss: 28.96314811706543\n",
      "current in epoch    591      batch 5\n",
      "RLoss: 20.030078887939453\n",
      "========================================\n",
      "Epoch 592/1000 - partial_train_loss: 43.7812 \n",
      "Epoch: [592/1000], TrainLoss: 39.65090029580252\n",
      "training Loss has not improved for 315 epochs.\n",
      "current in epoch    592      batch 0\n",
      "RLoss: 0.7067778706550598\n",
      "current in epoch    592      batch 1\n",
      "RLoss: 3.377103805541992\n",
      "current in epoch    592      batch 2\n",
      "RLoss: 36.33945083618164\n",
      "current in epoch    592      batch 3\n",
      "RLoss: 373.50537109375\n",
      "current in epoch    592      batch 4\n",
      "RLoss: 184.41928100585938\n",
      "current in epoch    592      batch 5\n",
      "RLoss: 108.05513000488281\n",
      "========================================\n",
      "Epoch 593/1000 - partial_train_loss: 102.5776 \n",
      "Epoch: [593/1000], TrainLoss: 94.75135803222656\n",
      "training Loss has not improved for 316 epochs.\n",
      "current in epoch    593      batch 0\n",
      "RLoss: 112.13169860839844\n",
      "current in epoch    593      batch 1\n",
      "RLoss: 86.67213439941406\n",
      "current in epoch    593      batch 2\n",
      "RLoss: 57.680908203125\n",
      "current in epoch    593      batch 3\n",
      "RLoss: 13.321929931640625\n",
      "current in epoch    593      batch 4\n",
      "RLoss: 70.6884765625\n",
      "current in epoch    593      batch 5\n",
      "RLoss: 5.805188179016113\n",
      "========================================\n",
      "Epoch 594/1000 - partial_train_loss: 81.7858 \n",
      "Epoch: [594/1000], TrainLoss: 4.083959860461099\n",
      "training Loss has not improved for 317 epochs.\n",
      "current in epoch    594      batch 0\n",
      "RLoss: 74.64736938476562\n",
      "current in epoch    594      batch 1\n",
      "RLoss: 26.887104034423828\n",
      "current in epoch    594      batch 2\n",
      "RLoss: 19.800987243652344\n",
      "current in epoch    594      batch 3\n",
      "RLoss: 24.36012077331543\n",
      "current in epoch    594      batch 4\n",
      "RLoss: 177.40943908691406\n",
      "current in epoch    594      batch 5\n",
      "RLoss: 21.655887603759766\n",
      "========================================\n",
      "Epoch 595/1000 - partial_train_loss: 50.8801 \n",
      "Epoch: [595/1000], TrainLoss: 15.154299565723964\n",
      "training Loss has not improved for 318 epochs.\n",
      "current in epoch    595      batch 0\n",
      "RLoss: 4.986444473266602\n",
      "current in epoch    595      batch 1\n",
      "RLoss: 152.422607421875\n",
      "current in epoch    595      batch 2\n",
      "RLoss: 74.24725341796875\n",
      "current in epoch    595      batch 3\n",
      "RLoss: 67.79878234863281\n",
      "current in epoch    595      batch 4\n",
      "RLoss: 4.884276390075684\n",
      "current in epoch    595      batch 5\n",
      "RLoss: 299.5133056640625\n",
      "========================================\n",
      "Epoch 596/1000 - partial_train_loss: 51.9585 \n",
      "sorting training set\n",
      "Epoch 596/1000 - Training loss: 267.4205 \n",
      "========================================\n",
      "Epoch: [596/1000], TrainLoss: 285.62550136247677\n",
      "training Loss has not improved for 319 epochs.\n",
      "current in epoch    596      batch 0\n",
      "RLoss: 263.9631042480469\n",
      "current in epoch    596      batch 1\n",
      "RLoss: 147.9883270263672\n",
      "current in epoch    596      batch 2\n",
      "RLoss: 304.1241149902344\n",
      "current in epoch    596      batch 3\n",
      "RLoss: 71.72636413574219\n",
      "current in epoch    596      batch 4\n",
      "RLoss: 100.14411163330078\n",
      "current in epoch    596      batch 5\n",
      "RLoss: 17.114784240722656\n",
      "========================================\n",
      "Epoch 597/1000 - partial_train_loss: 311.5522 \n",
      "Epoch: [597/1000], TrainLoss: 14.070555686950684\n",
      "training Loss has not improved for 320 epochs.\n",
      "current in epoch    597      batch 0\n",
      "RLoss: 11.841102600097656\n",
      "current in epoch    597      batch 1\n",
      "RLoss: 113.53956604003906\n",
      "current in epoch    597      batch 2\n",
      "RLoss: 87.54109191894531\n",
      "current in epoch    597      batch 3\n",
      "RLoss: 15.952908515930176\n",
      "current in epoch    597      batch 4\n",
      "RLoss: 20.183622360229492\n",
      "current in epoch    597      batch 5\n",
      "RLoss: 2.8666930198669434\n",
      "========================================\n",
      "Epoch 598/1000 - partial_train_loss: 46.2666 \n",
      "Epoch: [598/1000], TrainLoss: 5.124177136591503\n",
      "training Loss has not improved for 321 epochs.\n",
      "current in epoch    598      batch 0\n",
      "RLoss: 8.429060935974121\n",
      "current in epoch    598      batch 1\n",
      "RLoss: 74.28496551513672\n",
      "current in epoch    598      batch 2\n",
      "RLoss: 44.61549377441406\n",
      "current in epoch    598      batch 3\n",
      "RLoss: 129.9316864013672\n",
      "current in epoch    598      batch 4\n",
      "RLoss: 69.53939819335938\n",
      "current in epoch    598      batch 5\n",
      "RLoss: 59.47809600830078\n",
      "========================================\n",
      "Epoch 599/1000 - partial_train_loss: 60.0306 \n",
      "Epoch: [599/1000], TrainLoss: 42.41991186141968\n",
      "training Loss has not improved for 322 epochs.\n",
      "current in epoch    599      batch 0\n",
      "RLoss: 41.91871643066406\n",
      "current in epoch    599      batch 1\n",
      "RLoss: 60.336448669433594\n",
      "current in epoch    599      batch 2\n",
      "RLoss: 21.26091194152832\n",
      "current in epoch    599      batch 3\n",
      "RLoss: 36.98472595214844\n",
      "current in epoch    599      batch 4\n",
      "RLoss: 10.639503479003906\n",
      "current in epoch    599      batch 5\n",
      "RLoss: 12.677921295166016\n",
      "========================================\n",
      "Epoch 600/1000 - partial_train_loss: 36.8787 \n",
      "Epoch: [600/1000], TrainLoss: 7.951727786234447\n",
      "training Loss has not improved for 323 epochs.\n",
      "current in epoch    600      batch 0\n",
      "RLoss: 11.952109336853027\n",
      "current in epoch    600      batch 1\n",
      "RLoss: 4.398815631866455\n",
      "current in epoch    600      batch 2\n",
      "RLoss: 137.2913818359375\n",
      "current in epoch    600      batch 3\n",
      "RLoss: 28.180862426757812\n",
      "current in epoch    600      batch 4\n",
      "RLoss: 107.35851287841797\n",
      "current in epoch    600      batch 5\n",
      "RLoss: 38.90825653076172\n",
      "========================================\n",
      "Epoch 601/1000 - partial_train_loss: 48.3196 \n",
      "sorting training set\n",
      "Epoch 601/1000 - Training loss: 31.0502 \n",
      "========================================\n",
      "Epoch: [601/1000], TrainLoss: 33.19163222289934\n",
      "training Loss has not improved for 324 epochs.\n",
      "current in epoch    601      batch 0\n",
      "RLoss: 65.63166809082031\n",
      "current in epoch    601      batch 1\n",
      "RLoss: 131.28297424316406\n",
      "current in epoch    601      batch 2\n",
      "RLoss: 14.161731719970703\n",
      "current in epoch    601      batch 3\n",
      "RLoss: 18.812376022338867\n",
      "current in epoch    601      batch 4\n",
      "RLoss: 1.6383724212646484\n",
      "current in epoch    601      batch 5\n",
      "RLoss: 7.584142208099365\n",
      "========================================\n",
      "Epoch 602/1000 - partial_train_loss: 77.7852 \n",
      "Epoch: [602/1000], TrainLoss: 7.729500991957528\n",
      "training Loss has not improved for 325 epochs.\n",
      "current in epoch    602      batch 0\n",
      "RLoss: 143.25729370117188\n",
      "current in epoch    602      batch 1\n",
      "RLoss: 246.7904510498047\n",
      "current in epoch    602      batch 2\n",
      "RLoss: 47.281028747558594\n",
      "current in epoch    602      batch 3\n",
      "RLoss: 123.30471801757812\n",
      "current in epoch    602      batch 4\n",
      "RLoss: 245.68479919433594\n",
      "current in epoch    602      batch 5\n",
      "RLoss: 11.693153381347656\n",
      "========================================\n",
      "Epoch 603/1000 - partial_train_loss: 117.6063 \n",
      "Epoch: [603/1000], TrainLoss: 13.60485737664359\n",
      "training Loss has not improved for 326 epochs.\n",
      "current in epoch    603      batch 0\n",
      "RLoss: 486.65625\n",
      "current in epoch    603      batch 1\n",
      "RLoss: 125.34356689453125\n",
      "current in epoch    603      batch 2\n",
      "RLoss: 65.89118957519531\n",
      "current in epoch    603      batch 3\n",
      "RLoss: 362.44976806640625\n",
      "current in epoch    603      batch 4\n",
      "RLoss: 127.07965850830078\n",
      "current in epoch    603      batch 5\n",
      "RLoss: 16.01156234741211\n",
      "========================================\n",
      "Epoch 604/1000 - partial_train_loss: 174.3383 \n",
      "Epoch: [604/1000], TrainLoss: 19.734144926071167\n",
      "training Loss has not improved for 327 epochs.\n",
      "current in epoch    604      batch 0\n",
      "RLoss: 156.78533935546875\n",
      "current in epoch    604      batch 1\n",
      "RLoss: 277.5293273925781\n",
      "current in epoch    604      batch 2\n",
      "RLoss: 230.08998107910156\n",
      "current in epoch    604      batch 3\n",
      "RLoss: 22.36638832092285\n",
      "current in epoch    604      batch 4\n",
      "RLoss: 6.840227127075195\n",
      "current in epoch    604      batch 5\n",
      "RLoss: 15.678728103637695\n",
      "========================================\n",
      "Epoch 605/1000 - partial_train_loss: 106.6625 \n",
      "Epoch: [605/1000], TrainLoss: 15.645853076662336\n",
      "training Loss has not improved for 328 epochs.\n",
      "current in epoch    605      batch 0\n",
      "RLoss: 31.953554153442383\n",
      "current in epoch    605      batch 1\n",
      "RLoss: 866.1273803710938\n",
      "current in epoch    605      batch 2\n",
      "RLoss: 663.9212036132812\n",
      "current in epoch    605      batch 3\n",
      "RLoss: 25.164966583251953\n",
      "current in epoch    605      batch 4\n",
      "RLoss: 1408.665771484375\n",
      "current in epoch    605      batch 5\n",
      "RLoss: 49.03065490722656\n",
      "========================================\n",
      "Epoch 606/1000 - partial_train_loss: 447.4270 \n",
      "sorting training set\n",
      "Epoch 606/1000 - Training loss: 51.7593 \n",
      "========================================\n",
      "Epoch: [606/1000], TrainLoss: 55.69742495005699\n",
      "training Loss has not improved for 329 epochs.\n",
      "current in epoch    606      batch 0\n",
      "RLoss: 68.94978332519531\n",
      "current in epoch    606      batch 1\n",
      "RLoss: 138.8312530517578\n",
      "current in epoch    606      batch 2\n",
      "RLoss: 40.36020278930664\n",
      "current in epoch    606      batch 3\n",
      "RLoss: 20.05268096923828\n",
      "current in epoch    606      batch 4\n",
      "RLoss: 55.02886962890625\n",
      "current in epoch    606      batch 5\n",
      "RLoss: 163.81639099121094\n",
      "========================================\n",
      "Epoch 607/1000 - partial_train_loss: 168.5934 \n",
      "Epoch: [607/1000], TrainLoss: 161.0747721535819\n",
      "training Loss has not improved for 330 epochs.\n",
      "current in epoch    607      batch 0\n",
      "RLoss: 45.13747024536133\n",
      "current in epoch    607      batch 1\n",
      "RLoss: 15.10289478302002\n",
      "current in epoch    607      batch 2\n",
      "RLoss: 142.85763549804688\n",
      "current in epoch    607      batch 3\n",
      "RLoss: 75.69385528564453\n",
      "current in epoch    607      batch 4\n",
      "RLoss: 24.165771484375\n",
      "current in epoch    607      batch 5\n",
      "RLoss: 98.31974029541016\n",
      "========================================\n",
      "Epoch 608/1000 - partial_train_loss: 62.1399 \n",
      "Epoch: [608/1000], TrainLoss: 90.24940163748605\n",
      "training Loss has not improved for 331 epochs.\n",
      "current in epoch    608      batch 0\n",
      "RLoss: 29.791114807128906\n",
      "current in epoch    608      batch 1\n",
      "RLoss: 53.04771423339844\n",
      "current in epoch    608      batch 2\n",
      "RLoss: 96.5030746459961\n",
      "current in epoch    608      batch 3\n",
      "RLoss: 47.82887649536133\n",
      "current in epoch    608      batch 4\n",
      "RLoss: 186.2203826904297\n",
      "current in epoch    608      batch 5\n",
      "RLoss: 82.38267517089844\n",
      "========================================\n",
      "Epoch 609/1000 - partial_train_loss: 98.8229 \n",
      "Epoch: [609/1000], TrainLoss: 82.47896221705845\n",
      "training Loss has not improved for 332 epochs.\n",
      "current in epoch    609      batch 0\n",
      "RLoss: 38.86372375488281\n",
      "current in epoch    609      batch 1\n",
      "RLoss: 227.7916259765625\n",
      "current in epoch    609      batch 2\n",
      "RLoss: 14.086835861206055\n",
      "current in epoch    609      batch 3\n",
      "RLoss: 55.11328887939453\n",
      "current in epoch    609      batch 4\n",
      "RLoss: 8.614726066589355\n",
      "current in epoch    609      batch 5\n",
      "RLoss: 82.26973724365234\n",
      "========================================\n",
      "Epoch 610/1000 - partial_train_loss: 82.0575 \n",
      "Epoch: [610/1000], TrainLoss: 86.252352305821\n",
      "training Loss has not improved for 333 epochs.\n",
      "current in epoch    610      batch 0\n",
      "RLoss: 86.09825134277344\n",
      "current in epoch    610      batch 1\n",
      "RLoss: 275.3686218261719\n",
      "current in epoch    610      batch 2\n",
      "RLoss: 87.53907012939453\n",
      "current in epoch    610      batch 3\n",
      "RLoss: 33.02523422241211\n",
      "current in epoch    610      batch 4\n",
      "RLoss: 92.94395446777344\n",
      "current in epoch    610      batch 5\n",
      "RLoss: 104.69537353515625\n",
      "========================================\n",
      "Epoch 611/1000 - partial_train_loss: 122.4817 \n",
      "sorting training set\n",
      "Epoch 611/1000 - Training loss: 103.3657 \n",
      "========================================\n",
      "Epoch: [611/1000], TrainLoss: 110.5420619544996\n",
      "training Loss has not improved for 334 epochs.\n",
      "current in epoch    611      batch 0\n",
      "RLoss: 20.71599006652832\n",
      "current in epoch    611      batch 1\n",
      "RLoss: 951.9805908203125\n",
      "current in epoch    611      batch 2\n",
      "RLoss: 31.09730339050293\n",
      "current in epoch    611      batch 3\n",
      "RLoss: 58.58286666870117\n",
      "current in epoch    611      batch 4\n",
      "RLoss: 24.47623062133789\n",
      "current in epoch    611      batch 5\n",
      "RLoss: 119.3622817993164\n",
      "========================================\n",
      "Epoch 612/1000 - partial_train_loss: 213.8062 \n",
      "Epoch: [612/1000], TrainLoss: 131.0486204964774\n",
      "training Loss has not improved for 335 epochs.\n",
      "current in epoch    612      batch 0\n",
      "RLoss: 913.380859375\n",
      "current in epoch    612      batch 1\n",
      "RLoss: 64.83161926269531\n",
      "current in epoch    612      batch 2\n",
      "RLoss: 6.484081268310547\n",
      "current in epoch    612      batch 3\n",
      "RLoss: 50.675437927246094\n",
      "current in epoch    612      batch 4\n",
      "RLoss: 11.986577033996582\n",
      "current in epoch    612      batch 5\n",
      "RLoss: 322.3790588378906\n",
      "========================================\n",
      "Epoch 613/1000 - partial_train_loss: 158.7363 \n",
      "Epoch: [613/1000], TrainLoss: 470.21177891322543\n",
      "training Loss has not improved for 336 epochs.\n",
      "current in epoch    613      batch 0\n",
      "RLoss: 235.2347869873047\n",
      "current in epoch    613      batch 1\n",
      "RLoss: 30.865314483642578\n",
      "current in epoch    613      batch 2\n",
      "RLoss: 47.04806900024414\n",
      "current in epoch    613      batch 3\n",
      "RLoss: 32.645751953125\n",
      "current in epoch    613      batch 4\n",
      "RLoss: 177.99203491210938\n",
      "current in epoch    613      batch 5\n",
      "RLoss: 1013.30810546875\n",
      "========================================\n",
      "Epoch 614/1000 - partial_train_loss: 178.5137 \n",
      "Epoch: [614/1000], TrainLoss: 709.7543552943638\n",
      "training Loss has not improved for 337 epochs.\n",
      "current in epoch    614      batch 0\n",
      "RLoss: 172.703857421875\n",
      "current in epoch    614      batch 1\n",
      "RLoss: 8.818851470947266\n",
      "current in epoch    614      batch 2\n",
      "RLoss: 5.393215656280518\n",
      "current in epoch    614      batch 3\n",
      "RLoss: 23.06591033935547\n",
      "current in epoch    614      batch 4\n",
      "RLoss: 52.32363510131836\n",
      "current in epoch    614      batch 5\n",
      "RLoss: 20.65900230407715\n",
      "========================================\n",
      "Epoch 615/1000 - partial_train_loss: 466.8869 \n",
      "Epoch: [615/1000], TrainLoss: 25.597401993615286\n",
      "training Loss has not improved for 338 epochs.\n",
      "current in epoch    615      batch 0\n",
      "RLoss: 52.841556549072266\n",
      "current in epoch    615      batch 1\n",
      "RLoss: 263.0506591796875\n",
      "current in epoch    615      batch 2\n",
      "RLoss: 317.02362060546875\n",
      "current in epoch    615      batch 3\n",
      "RLoss: 112.91429138183594\n",
      "current in epoch    615      batch 4\n",
      "RLoss: 16.422426223754883\n",
      "current in epoch    615      batch 5\n",
      "RLoss: 6.790580749511719\n",
      "========================================\n",
      "Epoch 616/1000 - partial_train_loss: 110.3276 \n",
      "sorting training set\n",
      "Epoch 616/1000 - Training loss: 8.0209 \n",
      "========================================\n",
      "Epoch: [616/1000], TrainLoss: 8.667608585207281\n",
      "training Loss has not improved for 339 epochs.\n",
      "current in epoch    616      batch 0\n",
      "RLoss: 21.806638717651367\n",
      "current in epoch    616      batch 1\n",
      "RLoss: 40.797080993652344\n",
      "current in epoch    616      batch 2\n",
      "RLoss: 6.070370197296143\n",
      "current in epoch    616      batch 3\n",
      "RLoss: 187.96055603027344\n",
      "current in epoch    616      batch 4\n",
      "RLoss: 55.80496597290039\n",
      "current in epoch    616      batch 5\n",
      "RLoss: 265.3547668457031\n",
      "========================================\n",
      "Epoch 617/1000 - partial_train_loss: 53.3737 \n",
      "Epoch: [617/1000], TrainLoss: 233.04224341256278\n",
      "training Loss has not improved for 340 epochs.\n",
      "current in epoch    617      batch 0\n",
      "RLoss: 42.609195709228516\n",
      "current in epoch    617      batch 1\n",
      "RLoss: 25.093914031982422\n",
      "current in epoch    617      batch 2\n",
      "RLoss: 289.27911376953125\n",
      "current in epoch    617      batch 3\n",
      "RLoss: 38.08409881591797\n",
      "current in epoch    617      batch 4\n",
      "RLoss: 390.412841796875\n",
      "current in epoch    617      batch 5\n",
      "RLoss: 36.21388244628906\n",
      "========================================\n",
      "Epoch 618/1000 - partial_train_loss: 181.7732 \n",
      "Epoch: [618/1000], TrainLoss: 39.414431367601665\n",
      "training Loss has not improved for 341 epochs.\n",
      "current in epoch    618      batch 0\n",
      "RLoss: 60.0179557800293\n",
      "current in epoch    618      batch 1\n",
      "RLoss: 20.12183952331543\n",
      "current in epoch    618      batch 2\n",
      "RLoss: 4.7217936515808105\n",
      "current in epoch    618      batch 3\n",
      "RLoss: 22.6750431060791\n",
      "current in epoch    618      batch 4\n",
      "RLoss: 6.036323070526123\n",
      "current in epoch    618      batch 5\n",
      "RLoss: 136.80743408203125\n",
      "========================================\n",
      "Epoch 619/1000 - partial_train_loss: 32.6127 \n",
      "Epoch: [619/1000], TrainLoss: 148.65444019862585\n",
      "training Loss has not improved for 342 epochs.\n",
      "current in epoch    619      batch 0\n",
      "RLoss: 13.271526336669922\n",
      "current in epoch    619      batch 1\n",
      "RLoss: 12.223604202270508\n",
      "current in epoch    619      batch 2\n",
      "RLoss: 73.5228500366211\n",
      "current in epoch    619      batch 3\n",
      "RLoss: 88.60628509521484\n",
      "current in epoch    619      batch 4\n",
      "RLoss: 10.276997566223145\n",
      "current in epoch    619      batch 5\n",
      "RLoss: 117.48692321777344\n",
      "========================================\n",
      "Epoch 620/1000 - partial_train_loss: 62.1324 \n",
      "Epoch: [620/1000], TrainLoss: 118.6987054007394\n",
      "training Loss has not improved for 343 epochs.\n",
      "current in epoch    620      batch 0\n",
      "RLoss: 12.696075439453125\n",
      "current in epoch    620      batch 1\n",
      "RLoss: 11.743372917175293\n",
      "current in epoch    620      batch 2\n",
      "RLoss: 3.486165761947632\n",
      "current in epoch    620      batch 3\n",
      "RLoss: 2.9642317295074463\n",
      "current in epoch    620      batch 4\n",
      "RLoss: 5.019116401672363\n",
      "current in epoch    620      batch 5\n",
      "RLoss: 119.97053527832031\n",
      "========================================\n",
      "Epoch 621/1000 - partial_train_loss: 39.4582 \n",
      "sorting training set\n",
      "Epoch 621/1000 - Training loss: 93.6389 \n",
      "========================================\n",
      "Epoch: [621/1000], TrainLoss: 100.12106957418526\n",
      "training Loss has not improved for 344 epochs.\n",
      "current in epoch    621      batch 0\n",
      "RLoss: 85.08024597167969\n",
      "current in epoch    621      batch 1\n",
      "RLoss: 52.31264114379883\n",
      "current in epoch    621      batch 2\n",
      "RLoss: 241.63919067382812\n",
      "current in epoch    621      batch 3\n",
      "RLoss: 17.842777252197266\n",
      "current in epoch    621      batch 4\n",
      "RLoss: 38.10344314575195\n",
      "current in epoch    621      batch 5\n",
      "RLoss: 160.78982543945312\n",
      "========================================\n",
      "Epoch 622/1000 - partial_train_loss: 140.0983 \n",
      "Epoch: [622/1000], TrainLoss: 166.4994948250907\n",
      "training Loss has not improved for 345 epochs.\n",
      "current in epoch    622      batch 0\n",
      "RLoss: 45.45673370361328\n",
      "current in epoch    622      batch 1\n",
      "RLoss: 86.27488708496094\n",
      "current in epoch    622      batch 2\n",
      "RLoss: 5.098269939422607\n",
      "current in epoch    622      batch 3\n",
      "RLoss: 11.404956817626953\n",
      "current in epoch    622      batch 4\n",
      "RLoss: 61.07582473754883\n",
      "current in epoch    622      batch 5\n",
      "RLoss: 262.2123107910156\n",
      "========================================\n",
      "Epoch 623/1000 - partial_train_loss: 60.6316 \n",
      "Epoch: [623/1000], TrainLoss: 235.4772584097726\n",
      "training Loss has not improved for 346 epochs.\n",
      "current in epoch    623      batch 0\n",
      "RLoss: 1055.697021484375\n",
      "current in epoch    623      batch 1\n",
      "RLoss: 11.794988632202148\n",
      "current in epoch    623      batch 2\n",
      "RLoss: 177.9637908935547\n",
      "current in epoch    623      batch 3\n",
      "RLoss: 66.43217468261719\n",
      "current in epoch    623      batch 4\n",
      "RLoss: 171.0423583984375\n",
      "current in epoch    623      batch 5\n",
      "RLoss: 18.32261848449707\n",
      "========================================\n",
      "Epoch 624/1000 - partial_train_loss: 247.1767 \n",
      "Epoch: [624/1000], TrainLoss: 22.71108548981803\n",
      "training Loss has not improved for 347 epochs.\n",
      "current in epoch    624      batch 0\n",
      "RLoss: 397.1913146972656\n",
      "current in epoch    624      batch 1\n",
      "RLoss: 13.177337646484375\n",
      "current in epoch    624      batch 2\n",
      "RLoss: 50.663970947265625\n",
      "current in epoch    624      batch 3\n",
      "RLoss: 7.023897171020508\n",
      "current in epoch    624      batch 4\n",
      "RLoss: 64.40108489990234\n",
      "current in epoch    624      batch 5\n",
      "RLoss: 31.222837448120117\n",
      "========================================\n",
      "Epoch 625/1000 - partial_train_loss: 85.6608 \n",
      "Epoch: [625/1000], TrainLoss: 132.5954556465149\n",
      "training Loss has not improved for 348 epochs.\n",
      "current in epoch    625      batch 0\n",
      "RLoss: 20.360397338867188\n",
      "current in epoch    625      batch 1\n",
      "RLoss: 18.552406311035156\n",
      "current in epoch    625      batch 2\n",
      "RLoss: 57.421974182128906\n",
      "current in epoch    625      batch 3\n",
      "RLoss: 130.83401489257812\n",
      "current in epoch    625      batch 4\n",
      "RLoss: 20.490787506103516\n",
      "current in epoch    625      batch 5\n",
      "RLoss: 59.729652404785156\n",
      "========================================\n",
      "Epoch 626/1000 - partial_train_loss: 69.2541 \n",
      "sorting training set\n",
      "Epoch 626/1000 - Training loss: 96.3458 \n",
      "========================================\n",
      "Epoch: [626/1000], TrainLoss: 103.19070685520221\n",
      "training Loss has not improved for 349 epochs.\n",
      "current in epoch    626      batch 0\n",
      "RLoss: 94.47821807861328\n",
      "current in epoch    626      batch 1\n",
      "RLoss: 90.040771484375\n",
      "current in epoch    626      batch 2\n",
      "RLoss: 26.76418113708496\n",
      "current in epoch    626      batch 3\n",
      "RLoss: 8.786174774169922\n",
      "current in epoch    626      batch 4\n",
      "RLoss: 8.541190147399902\n",
      "current in epoch    626      batch 5\n",
      "RLoss: 29.873106002807617\n",
      "========================================\n",
      "Epoch 627/1000 - partial_train_loss: 154.7679 \n",
      "Epoch: [627/1000], TrainLoss: 51.71015610013689\n",
      "training Loss has not improved for 350 epochs.\n",
      "current in epoch    627      batch 0\n",
      "RLoss: 33.92862319946289\n",
      "current in epoch    627      batch 1\n",
      "RLoss: 14.890769004821777\n",
      "current in epoch    627      batch 2\n",
      "RLoss: 32.50048065185547\n",
      "current in epoch    627      batch 3\n",
      "RLoss: 31.87984848022461\n",
      "current in epoch    627      batch 4\n",
      "RLoss: 79.00904846191406\n",
      "current in epoch    627      batch 5\n",
      "RLoss: 88.0426025390625\n",
      "========================================\n",
      "Epoch 628/1000 - partial_train_loss: 40.8985 \n",
      "Epoch: [628/1000], TrainLoss: 93.96286719185966\n",
      "training Loss has not improved for 351 epochs.\n",
      "current in epoch    628      batch 0\n",
      "RLoss: 110.31532287597656\n",
      "current in epoch    628      batch 1\n",
      "RLoss: 5.382534980773926\n",
      "current in epoch    628      batch 2\n",
      "RLoss: 249.4528350830078\n",
      "current in epoch    628      batch 3\n",
      "RLoss: 213.27980041503906\n",
      "current in epoch    628      batch 4\n",
      "RLoss: 6.155646800994873\n",
      "current in epoch    628      batch 5\n",
      "RLoss: 10.988545417785645\n",
      "========================================\n",
      "Epoch 629/1000 - partial_train_loss: 119.4518 \n",
      "Epoch: [629/1000], TrainLoss: 7.483588874340057\n",
      "training Loss has not improved for 352 epochs.\n",
      "current in epoch    629      batch 0\n",
      "RLoss: 16.041170120239258\n",
      "current in epoch    629      batch 1\n",
      "RLoss: 81.95486450195312\n",
      "current in epoch    629      batch 2\n",
      "RLoss: 44.06729507446289\n",
      "current in epoch    629      batch 3\n",
      "RLoss: 233.63568115234375\n",
      "current in epoch    629      batch 4\n",
      "RLoss: 273.81011962890625\n",
      "current in epoch    629      batch 5\n",
      "RLoss: 45.95469665527344\n",
      "========================================\n",
      "Epoch 630/1000 - partial_train_loss: 95.7018 \n",
      "Epoch: [630/1000], TrainLoss: 42.521992155483794\n",
      "training Loss has not improved for 353 epochs.\n",
      "current in epoch    630      batch 0\n",
      "RLoss: 184.80384826660156\n",
      "current in epoch    630      batch 1\n",
      "RLoss: 18.434066772460938\n",
      "current in epoch    630      batch 2\n",
      "RLoss: 25.656200408935547\n",
      "current in epoch    630      batch 3\n",
      "RLoss: 79.32772064208984\n",
      "current in epoch    630      batch 4\n",
      "RLoss: 24.834712982177734\n",
      "current in epoch    630      batch 5\n",
      "RLoss: 31.309476852416992\n",
      "========================================\n",
      "Epoch 631/1000 - partial_train_loss: 46.6801 \n",
      "sorting training set\n",
      "Epoch 631/1000 - Training loss: 21.1662 \n",
      "========================================\n",
      "Epoch: [631/1000], TrainLoss: 22.539713152329824\n",
      "training Loss has not improved for 354 epochs.\n",
      "current in epoch    631      batch 0\n",
      "RLoss: 8.581722259521484\n",
      "current in epoch    631      batch 1\n",
      "RLoss: 2.89597225189209\n",
      "current in epoch    631      batch 2\n",
      "RLoss: 32.783935546875\n",
      "current in epoch    631      batch 3\n",
      "RLoss: 61.47695541381836\n",
      "current in epoch    631      batch 4\n",
      "RLoss: 318.4198303222656\n",
      "current in epoch    631      batch 5\n",
      "RLoss: 152.41091918945312\n",
      "========================================\n",
      "Epoch 632/1000 - partial_train_loss: 100.6032 \n",
      "Epoch: [632/1000], TrainLoss: 210.94526290893555\n",
      "training Loss has not improved for 355 epochs.\n",
      "current in epoch    632      batch 0\n",
      "RLoss: 83.03328704833984\n",
      "current in epoch    632      batch 1\n",
      "RLoss: 224.99012756347656\n",
      "current in epoch    632      batch 2\n",
      "RLoss: 117.16217041015625\n",
      "current in epoch    632      batch 3\n",
      "RLoss: 388.9937438964844\n",
      "current in epoch    632      batch 4\n",
      "RLoss: 51.64072799682617\n",
      "current in epoch    632      batch 5\n",
      "RLoss: 94.42072296142578\n",
      "========================================\n",
      "Epoch 633/1000 - partial_train_loss: 176.6959 \n",
      "Epoch: [633/1000], TrainLoss: 109.92505727495465\n",
      "training Loss has not improved for 356 epochs.\n",
      "current in epoch    633      batch 0\n",
      "RLoss: 144.92755126953125\n",
      "current in epoch    633      batch 1\n",
      "RLoss: 53.75252151489258\n",
      "current in epoch    633      batch 2\n",
      "RLoss: 3.4110219478607178\n",
      "current in epoch    633      batch 3\n",
      "RLoss: 54.24276351928711\n",
      "current in epoch    633      batch 4\n",
      "RLoss: 66.66016387939453\n",
      "current in epoch    633      batch 5\n",
      "RLoss: 30.50189208984375\n",
      "========================================\n",
      "Epoch 634/1000 - partial_train_loss: 79.9245 \n",
      "Epoch: [634/1000], TrainLoss: 23.537754467555455\n",
      "training Loss has not improved for 357 epochs.\n",
      "current in epoch    634      batch 0\n",
      "RLoss: 1.9613362550735474\n",
      "current in epoch    634      batch 1\n",
      "RLoss: 8.846144676208496\n",
      "current in epoch    634      batch 2\n",
      "RLoss: 44.51243591308594\n",
      "current in epoch    634      batch 3\n",
      "RLoss: 19.886404037475586\n",
      "current in epoch    634      batch 4\n",
      "RLoss: 320.2784118652344\n",
      "current in epoch    634      batch 5\n",
      "RLoss: 176.12002563476562\n",
      "========================================\n",
      "Epoch 635/1000 - partial_train_loss: 82.4060 \n",
      "Epoch: [635/1000], TrainLoss: 157.7235930306571\n",
      "training Loss has not improved for 358 epochs.\n",
      "current in epoch    635      batch 0\n",
      "RLoss: 24.66324234008789\n",
      "current in epoch    635      batch 1\n",
      "RLoss: 151.85594177246094\n",
      "current in epoch    635      batch 2\n",
      "RLoss: 42.04030227661133\n",
      "current in epoch    635      batch 3\n",
      "RLoss: 494.92596435546875\n",
      "current in epoch    635      batch 4\n",
      "RLoss: 65.21125793457031\n",
      "current in epoch    635      batch 5\n",
      "RLoss: 17.19385528564453\n",
      "========================================\n",
      "Epoch 636/1000 - partial_train_loss: 214.1332 \n",
      "sorting training set\n",
      "Epoch 636/1000 - Training loss: 12.5563 \n",
      "========================================\n",
      "Epoch: [636/1000], TrainLoss: 13.425500914611433\n",
      "training Loss has not improved for 359 epochs.\n",
      "current in epoch    636      batch 0\n",
      "RLoss: 42.644840240478516\n",
      "current in epoch    636      batch 1\n",
      "RLoss: 20.909215927124023\n",
      "current in epoch    636      batch 2\n",
      "RLoss: 61.988834381103516\n",
      "current in epoch    636      batch 3\n",
      "RLoss: 64.89926147460938\n",
      "current in epoch    636      batch 4\n",
      "RLoss: 17.406681060791016\n",
      "current in epoch    636      batch 5\n",
      "RLoss: 251.37905883789062\n",
      "========================================\n",
      "Epoch 637/1000 - partial_train_loss: 41.9449 \n",
      "Epoch: [637/1000], TrainLoss: 253.25243323189872\n",
      "training Loss has not improved for 360 epochs.\n",
      "current in epoch    637      batch 0\n",
      "RLoss: 32.79910659790039\n",
      "current in epoch    637      batch 1\n",
      "RLoss: 68.71196746826172\n",
      "current in epoch    637      batch 2\n",
      "RLoss: 70.56440734863281\n",
      "current in epoch    637      batch 3\n",
      "RLoss: 32.961280822753906\n",
      "current in epoch    637      batch 4\n",
      "RLoss: 317.7026672363281\n",
      "current in epoch    637      batch 5\n",
      "RLoss: 57.7465705871582\n",
      "========================================\n",
      "Epoch 638/1000 - partial_train_loss: 120.2469 \n",
      "Epoch: [638/1000], TrainLoss: 58.77872017451695\n",
      "training Loss has not improved for 361 epochs.\n",
      "current in epoch    638      batch 0\n",
      "RLoss: 162.3848876953125\n",
      "current in epoch    638      batch 1\n",
      "RLoss: 64.28446197509766\n",
      "current in epoch    638      batch 2\n",
      "RLoss: 62.97701644897461\n",
      "current in epoch    638      batch 3\n",
      "RLoss: 9.557438850402832\n",
      "current in epoch    638      batch 4\n",
      "RLoss: 50.259830474853516\n",
      "current in epoch    638      batch 5\n",
      "RLoss: 45.41501235961914\n",
      "========================================\n",
      "Epoch 639/1000 - partial_train_loss: 67.7219 \n",
      "Epoch: [639/1000], TrainLoss: 51.53148535319737\n",
      "training Loss has not improved for 362 epochs.\n",
      "current in epoch    639      batch 0\n",
      "RLoss: 52.54544448852539\n",
      "current in epoch    639      batch 1\n",
      "RLoss: 162.8177947998047\n",
      "current in epoch    639      batch 2\n",
      "RLoss: 5.87523078918457\n",
      "current in epoch    639      batch 3\n",
      "RLoss: 346.9718322753906\n",
      "current in epoch    639      batch 4\n",
      "RLoss: 26.451202392578125\n",
      "current in epoch    639      batch 5\n",
      "RLoss: 50.82434844970703\n",
      "========================================\n",
      "Epoch 640/1000 - partial_train_loss: 100.0447 \n",
      "Epoch: [640/1000], TrainLoss: 40.43301377977644\n",
      "training Loss has not improved for 363 epochs.\n",
      "current in epoch    640      batch 0\n",
      "RLoss: 487.1303405761719\n",
      "current in epoch    640      batch 1\n",
      "RLoss: 71.88334655761719\n",
      "current in epoch    640      batch 2\n",
      "RLoss: 56.114200592041016\n",
      "current in epoch    640      batch 3\n",
      "RLoss: 39.181026458740234\n",
      "current in epoch    640      batch 4\n",
      "RLoss: 40.402442932128906\n",
      "current in epoch    640      batch 5\n",
      "RLoss: 66.30867004394531\n",
      "========================================\n",
      "Epoch 641/1000 - partial_train_loss: 93.6837 \n",
      "sorting training set\n",
      "Epoch 641/1000 - Training loss: 85.7736 \n",
      "========================================\n",
      "Epoch: [641/1000], TrainLoss: 91.7315306702869\n",
      "training Loss has not improved for 364 epochs.\n",
      "current in epoch    641      batch 0\n",
      "RLoss: 599.5040283203125\n",
      "current in epoch    641      batch 1\n",
      "RLoss: 336.2786865234375\n",
      "current in epoch    641      batch 2\n",
      "RLoss: 5.263847351074219\n",
      "current in epoch    641      batch 3\n",
      "RLoss: 128.72911071777344\n",
      "current in epoch    641      batch 4\n",
      "RLoss: 57.583106994628906\n",
      "current in epoch    641      batch 5\n",
      "RLoss: 141.97889709472656\n",
      "========================================\n",
      "Epoch 642/1000 - partial_train_loss: 262.8770 \n",
      "Epoch: [642/1000], TrainLoss: 101.78459562574115\n",
      "training Loss has not improved for 365 epochs.\n",
      "current in epoch    642      batch 0\n",
      "RLoss: 7.641586780548096\n",
      "current in epoch    642      batch 1\n",
      "RLoss: 199.5404815673828\n",
      "current in epoch    642      batch 2\n",
      "RLoss: 297.64202880859375\n",
      "current in epoch    642      batch 3\n",
      "RLoss: 8.27676010131836\n",
      "current in epoch    642      batch 4\n",
      "RLoss: 65.03678894042969\n",
      "current in epoch    642      batch 5\n",
      "RLoss: 25.264869689941406\n",
      "========================================\n",
      "Epoch 643/1000 - partial_train_loss: 110.0696 \n",
      "Epoch: [643/1000], TrainLoss: 17.82817942755563\n",
      "training Loss has not improved for 366 epochs.\n",
      "current in epoch    643      batch 0\n",
      "RLoss: 55.9616813659668\n",
      "current in epoch    643      batch 1\n",
      "RLoss: 28.034297943115234\n",
      "current in epoch    643      batch 2\n",
      "RLoss: 18.65355110168457\n",
      "current in epoch    643      batch 3\n",
      "RLoss: 37.04433822631836\n",
      "current in epoch    643      batch 4\n",
      "RLoss: 10.031224250793457\n",
      "current in epoch    643      batch 5\n",
      "RLoss: 17.20307159423828\n",
      "========================================\n",
      "Epoch 644/1000 - partial_train_loss: 38.0705 \n",
      "Epoch: [644/1000], TrainLoss: 14.302894609315056\n",
      "training Loss has not improved for 367 epochs.\n",
      "current in epoch    644      batch 0\n",
      "RLoss: 5.2778000831604\n",
      "current in epoch    644      batch 1\n",
      "RLoss: 7.828440189361572\n",
      "current in epoch    644      batch 2\n",
      "RLoss: 28.05738639831543\n",
      "current in epoch    644      batch 3\n",
      "RLoss: 28.155590057373047\n",
      "current in epoch    644      batch 4\n",
      "RLoss: 673.5542602539062\n",
      "current in epoch    644      batch 5\n",
      "RLoss: 74.84130859375\n",
      "========================================\n",
      "Epoch 645/1000 - partial_train_loss: 118.7795 \n",
      "Epoch: [645/1000], TrainLoss: 61.787960052490234\n",
      "training Loss has not improved for 368 epochs.\n",
      "current in epoch    645      batch 0\n",
      "RLoss: 474.9090576171875\n",
      "current in epoch    645      batch 1\n",
      "RLoss: 128.1343231201172\n",
      "current in epoch    645      batch 2\n",
      "RLoss: 365.052001953125\n",
      "current in epoch    645      batch 3\n",
      "RLoss: 74.56855773925781\n",
      "current in epoch    645      batch 4\n",
      "RLoss: 22.268390655517578\n",
      "current in epoch    645      batch 5\n",
      "RLoss: 17.154781341552734\n",
      "========================================\n",
      "Epoch 646/1000 - partial_train_loss: 193.0737 \n",
      "sorting training set\n",
      "Epoch 646/1000 - Training loss: 16.2605 \n",
      "========================================\n",
      "Epoch: [646/1000], TrainLoss: 17.45932102098431\n",
      "training Loss has not improved for 369 epochs.\n",
      "current in epoch    646      batch 0\n",
      "RLoss: 24.25196647644043\n",
      "current in epoch    646      batch 1\n",
      "RLoss: 9.91958236694336\n",
      "current in epoch    646      batch 2\n",
      "RLoss: 28.995553970336914\n",
      "current in epoch    646      batch 3\n",
      "RLoss: 6.394357204437256\n",
      "current in epoch    646      batch 4\n",
      "RLoss: 24.097623825073242\n",
      "current in epoch    646      batch 5\n",
      "RLoss: 1.1059435606002808\n",
      "========================================\n",
      "Epoch 647/1000 - partial_train_loss: 48.3016 \n",
      "Epoch: [647/1000], TrainLoss: 0.9399727327483041\n",
      "current in epoch    647      batch 0\n",
      "RLoss: 1.7279367446899414\n",
      "current in epoch    647      batch 1\n",
      "RLoss: 5.672894477844238\n",
      "current in epoch    647      batch 2\n",
      "RLoss: 9.143752098083496\n",
      "current in epoch    647      batch 3\n",
      "RLoss: 26.11629867553711\n",
      "current in epoch    647      batch 4\n",
      "RLoss: 40.368160247802734\n",
      "current in epoch    647      batch 5\n",
      "RLoss: 10.743735313415527\n",
      "========================================\n",
      "Epoch 648/1000 - partial_train_loss: 16.3397 \n",
      "Epoch: [648/1000], TrainLoss: 9.438912187303815\n",
      "training Loss has not improved for 1 epochs.\n",
      "current in epoch    648      batch 0\n",
      "RLoss: 5.675124168395996\n",
      "current in epoch    648      batch 1\n",
      "RLoss: 21.234956741333008\n",
      "current in epoch    648      batch 2\n",
      "RLoss: 10.528738021850586\n",
      "current in epoch    648      batch 3\n",
      "RLoss: 328.44122314453125\n",
      "current in epoch    648      batch 4\n",
      "RLoss: 11.382615089416504\n",
      "current in epoch    648      batch 5\n",
      "RLoss: 4.321288108825684\n",
      "========================================\n",
      "Epoch 649/1000 - partial_train_loss: 68.7957 \n",
      "Epoch: [649/1000], TrainLoss: 4.432666940348489\n",
      "training Loss has not improved for 2 epochs.\n",
      "current in epoch    649      batch 0\n",
      "RLoss: 40.04125213623047\n",
      "current in epoch    649      batch 1\n",
      "RLoss: 7.941787242889404\n",
      "current in epoch    649      batch 2\n",
      "RLoss: 13.229683876037598\n",
      "current in epoch    649      batch 3\n",
      "RLoss: 34.258811950683594\n",
      "current in epoch    649      batch 4\n",
      "RLoss: 13.43040657043457\n",
      "current in epoch    649      batch 5\n",
      "RLoss: 62.68610763549805\n",
      "========================================\n",
      "Epoch 650/1000 - partial_train_loss: 22.1160 \n",
      "Epoch: [650/1000], TrainLoss: 107.36934348515102\n",
      "training Loss has not improved for 3 epochs.\n",
      "current in epoch    650      batch 0\n",
      "RLoss: 22.247690200805664\n",
      "current in epoch    650      batch 1\n",
      "RLoss: 14.4140043258667\n",
      "current in epoch    650      batch 2\n",
      "RLoss: 56.35154342651367\n",
      "current in epoch    650      batch 3\n",
      "RLoss: 107.70352172851562\n",
      "current in epoch    650      batch 4\n",
      "RLoss: 2.734975814819336\n",
      "current in epoch    650      batch 5\n",
      "RLoss: 6.188955783843994\n",
      "========================================\n",
      "Epoch 651/1000 - partial_train_loss: 44.1224 \n",
      "sorting training set\n",
      "Epoch 651/1000 - Training loss: 7.5480 \n",
      "========================================\n",
      "Epoch: [651/1000], TrainLoss: 8.64799450255451\n",
      "training Loss has not improved for 4 epochs.\n",
      "current in epoch    651      batch 0\n",
      "RLoss: 10.055944442749023\n",
      "current in epoch    651      batch 1\n",
      "RLoss: 9.760025978088379\n",
      "current in epoch    651      batch 2\n",
      "RLoss: 21.376140594482422\n",
      "current in epoch    651      batch 3\n",
      "RLoss: 23.625574111938477\n",
      "current in epoch    651      batch 4\n",
      "RLoss: 3.1686513423919678\n",
      "current in epoch    651      batch 5\n",
      "RLoss: 4.89700174331665\n",
      "========================================\n",
      "Epoch 652/1000 - partial_train_loss: 19.9831 \n",
      "Epoch: [652/1000], TrainLoss: 6.34976373400007\n",
      "training Loss has not improved for 5 epochs.\n",
      "current in epoch    652      batch 0\n",
      "RLoss: 28.073274612426758\n",
      "current in epoch    652      batch 1\n",
      "RLoss: 188.35450744628906\n",
      "current in epoch    652      batch 2\n",
      "RLoss: 185.825927734375\n",
      "current in epoch    652      batch 3\n",
      "RLoss: 86.23767852783203\n",
      "current in epoch    652      batch 4\n",
      "RLoss: 92.44459533691406\n",
      "current in epoch    652      batch 5\n",
      "RLoss: 113.08808135986328\n",
      "========================================\n",
      "Epoch 653/1000 - partial_train_loss: 101.0076 \n",
      "Epoch: [653/1000], TrainLoss: 136.6753944669451\n",
      "training Loss has not improved for 6 epochs.\n",
      "current in epoch    653      batch 0\n",
      "RLoss: 262.3086853027344\n",
      "current in epoch    653      batch 1\n",
      "RLoss: 287.0057373046875\n",
      "current in epoch    653      batch 2\n",
      "RLoss: 58.531646728515625\n",
      "current in epoch    653      batch 3\n",
      "RLoss: 374.26837158203125\n",
      "current in epoch    653      batch 4\n",
      "RLoss: 15.397907257080078\n",
      "current in epoch    653      batch 5\n",
      "RLoss: 40.404029846191406\n",
      "========================================\n",
      "Epoch 654/1000 - partial_train_loss: 164.8309 \n",
      "Epoch: [654/1000], TrainLoss: 39.5329726764134\n",
      "training Loss has not improved for 7 epochs.\n",
      "current in epoch    654      batch 0\n",
      "RLoss: 660.6898193359375\n",
      "current in epoch    654      batch 1\n",
      "RLoss: 53.7548713684082\n",
      "current in epoch    654      batch 2\n",
      "RLoss: 2.691533327102661\n",
      "current in epoch    654      batch 3\n",
      "RLoss: 17.84673309326172\n",
      "current in epoch    654      batch 4\n",
      "RLoss: 5.700449466705322\n",
      "current in epoch    654      batch 5\n",
      "RLoss: 28.975967407226562\n",
      "========================================\n",
      "Epoch 655/1000 - partial_train_loss: 112.3235 \n",
      "Epoch: [655/1000], TrainLoss: 36.59903410502842\n",
      "training Loss has not improved for 8 epochs.\n",
      "current in epoch    655      batch 0\n",
      "RLoss: 28.1806640625\n",
      "current in epoch    655      batch 1\n",
      "RLoss: 44.03507614135742\n",
      "current in epoch    655      batch 2\n",
      "RLoss: 7.264435768127441\n",
      "current in epoch    655      batch 3\n",
      "RLoss: 7.584248065948486\n",
      "current in epoch    655      batch 4\n",
      "RLoss: 14.026320457458496\n",
      "current in epoch    655      batch 5\n",
      "RLoss: 36.51180648803711\n",
      "========================================\n",
      "Epoch 656/1000 - partial_train_loss: 20.1230 \n",
      "sorting training set\n",
      "Epoch 656/1000 - Training loss: 40.6113 \n",
      "========================================\n",
      "Epoch: [656/1000], TrainLoss: 43.437095775955605\n",
      "training Loss has not improved for 9 epochs.\n",
      "current in epoch    656      batch 0\n",
      "RLoss: 51.249305725097656\n",
      "current in epoch    656      batch 1\n",
      "RLoss: 41.86314010620117\n",
      "current in epoch    656      batch 2\n",
      "RLoss: 22.780088424682617\n",
      "current in epoch    656      batch 3\n",
      "RLoss: 9.813337326049805\n",
      "current in epoch    656      batch 4\n",
      "RLoss: 1.7029963731765747\n",
      "current in epoch    656      batch 5\n",
      "RLoss: 4.377872943878174\n",
      "========================================\n",
      "Epoch 657/1000 - partial_train_loss: 57.7662 \n",
      "Epoch: [657/1000], TrainLoss: 7.294492176600865\n",
      "training Loss has not improved for 10 epochs.\n",
      "current in epoch    657      batch 0\n",
      "RLoss: 11.469287872314453\n",
      "current in epoch    657      batch 1\n",
      "RLoss: 84.19027709960938\n",
      "current in epoch    657      batch 2\n",
      "RLoss: 23.038368225097656\n",
      "current in epoch    657      batch 3\n",
      "RLoss: 40.16538619995117\n",
      "current in epoch    657      batch 4\n",
      "RLoss: 2.156937599182129\n",
      "current in epoch    657      batch 5\n",
      "RLoss: 5.656789779663086\n",
      "========================================\n",
      "Epoch 658/1000 - partial_train_loss: 26.4549 \n",
      "Epoch: [658/1000], TrainLoss: 5.948443233966827\n",
      "training Loss has not improved for 11 epochs.\n",
      "current in epoch    658      batch 0\n",
      "RLoss: 26.05118179321289\n",
      "current in epoch    658      batch 1\n",
      "RLoss: 16.241533279418945\n",
      "current in epoch    658      batch 2\n",
      "RLoss: 49.38627243041992\n",
      "current in epoch    658      batch 3\n",
      "RLoss: 168.60227966308594\n",
      "current in epoch    658      batch 4\n",
      "RLoss: 11.140244483947754\n",
      "current in epoch    658      batch 5\n",
      "RLoss: 24.353349685668945\n",
      "========================================\n",
      "Epoch 659/1000 - partial_train_loss: 42.3845 \n",
      "Epoch: [659/1000], TrainLoss: 26.75429677963257\n",
      "training Loss has not improved for 12 epochs.\n",
      "current in epoch    659      batch 0\n",
      "RLoss: 77.69097137451172\n",
      "current in epoch    659      batch 1\n",
      "RLoss: 24.890830993652344\n",
      "current in epoch    659      batch 2\n",
      "RLoss: 26.57736587524414\n",
      "current in epoch    659      batch 3\n",
      "RLoss: 18.8183536529541\n",
      "current in epoch    659      batch 4\n",
      "RLoss: 10.619744300842285\n",
      "current in epoch    659      batch 5\n",
      "RLoss: 47.00424575805664\n",
      "========================================\n",
      "Epoch 660/1000 - partial_train_loss: 29.2011 \n",
      "Epoch: [660/1000], TrainLoss: 40.86784437724522\n",
      "training Loss has not improved for 13 epochs.\n",
      "current in epoch    660      batch 0\n",
      "RLoss: 36.540767669677734\n",
      "current in epoch    660      batch 1\n",
      "RLoss: 17.94624900817871\n",
      "current in epoch    660      batch 2\n",
      "RLoss: 42.08891677856445\n",
      "current in epoch    660      batch 3\n",
      "RLoss: 30.739355087280273\n",
      "current in epoch    660      batch 4\n",
      "RLoss: 270.8541564941406\n",
      "current in epoch    660      batch 5\n",
      "RLoss: 6.053673267364502\n",
      "========================================\n",
      "Epoch 661/1000 - partial_train_loss: 72.2107 \n",
      "sorting training set\n",
      "Epoch 661/1000 - Training loss: 7.8674 \n",
      "========================================\n",
      "Epoch: [661/1000], TrainLoss: 8.240392969521881\n",
      "training Loss has not improved for 14 epochs.\n",
      "current in epoch    661      batch 0\n",
      "RLoss: 140.9151611328125\n",
      "current in epoch    661      batch 1\n",
      "RLoss: 62.166324615478516\n",
      "current in epoch    661      batch 2\n",
      "RLoss: 3.6091525554656982\n",
      "current in epoch    661      batch 3\n",
      "RLoss: 3.151419162750244\n",
      "current in epoch    661      batch 4\n",
      "RLoss: 56.5937614440918\n",
      "current in epoch    661      batch 5\n",
      "RLoss: 21.03917121887207\n",
      "========================================\n",
      "Epoch 662/1000 - partial_train_loss: 60.8879 \n",
      "Epoch: [662/1000], TrainLoss: 21.53879233769008\n",
      "training Loss has not improved for 15 epochs.\n",
      "current in epoch    662      batch 0\n",
      "RLoss: 22.496517181396484\n",
      "current in epoch    662      batch 1\n",
      "RLoss: 12.838689804077148\n",
      "current in epoch    662      batch 2\n",
      "RLoss: 32.2972412109375\n",
      "current in epoch    662      batch 3\n",
      "RLoss: 5.062504291534424\n",
      "current in epoch    662      batch 4\n",
      "RLoss: 43.39187240600586\n",
      "current in epoch    662      batch 5\n",
      "RLoss: 184.0404510498047\n",
      "========================================\n",
      "Epoch 663/1000 - partial_train_loss: 19.8493 \n",
      "Epoch: [663/1000], TrainLoss: 170.01596777779716\n",
      "training Loss has not improved for 16 epochs.\n",
      "current in epoch    663      batch 0\n",
      "RLoss: 10.694426536560059\n",
      "current in epoch    663      batch 1\n",
      "RLoss: 441.2974548339844\n",
      "current in epoch    663      batch 2\n",
      "RLoss: 54.04036331176758\n",
      "current in epoch    663      batch 3\n",
      "RLoss: 207.4042510986328\n",
      "current in epoch    663      batch 4\n",
      "RLoss: 181.02394104003906\n",
      "current in epoch    663      batch 5\n",
      "RLoss: 1.6486091613769531\n",
      "========================================\n",
      "Epoch 664/1000 - partial_train_loss: 173.3145 \n",
      "Epoch: [664/1000], TrainLoss: 1.2730208656617574\n",
      "training Loss has not improved for 17 epochs.\n",
      "current in epoch    664      batch 0\n",
      "RLoss: 38.00822067260742\n",
      "current in epoch    664      batch 1\n",
      "RLoss: 2.3464889526367188\n",
      "current in epoch    664      batch 2\n",
      "RLoss: 3.128565549850464\n",
      "current in epoch    664      batch 3\n",
      "RLoss: 4.095577716827393\n",
      "current in epoch    664      batch 4\n",
      "RLoss: 6.772610187530518\n",
      "current in epoch    664      batch 5\n",
      "RLoss: 15.030009269714355\n",
      "========================================\n",
      "Epoch 665/1000 - partial_train_loss: 7.6678 \n",
      "Epoch: [665/1000], TrainLoss: 15.326537609100342\n",
      "training Loss has not improved for 18 epochs.\n",
      "current in epoch    665      batch 0\n",
      "RLoss: 50.95215606689453\n",
      "current in epoch    665      batch 1\n",
      "RLoss: 2.6299962997436523\n",
      "current in epoch    665      batch 2\n",
      "RLoss: 44.6512565612793\n",
      "current in epoch    665      batch 3\n",
      "RLoss: 4.955100059509277\n",
      "current in epoch    665      batch 4\n",
      "RLoss: 13.63915729522705\n",
      "current in epoch    665      batch 5\n",
      "RLoss: 6.3139543533325195\n",
      "========================================\n",
      "Epoch 666/1000 - partial_train_loss: 31.6891 \n",
      "sorting training set\n",
      "Epoch 666/1000 - Training loss: 6.8066 \n",
      "========================================\n",
      "Epoch: [666/1000], TrainLoss: 7.4960536741273005\n",
      "training Loss has not improved for 19 epochs.\n",
      "current in epoch    666      batch 0\n",
      "RLoss: 206.53460693359375\n",
      "current in epoch    666      batch 1\n",
      "RLoss: 143.7755584716797\n",
      "current in epoch    666      batch 2\n",
      "RLoss: 7.466568470001221\n",
      "current in epoch    666      batch 3\n",
      "RLoss: 18.61261558532715\n",
      "current in epoch    666      batch 4\n",
      "RLoss: 56.23429870605469\n",
      "current in epoch    666      batch 5\n",
      "RLoss: 49.22795486450195\n",
      "========================================\n",
      "Epoch 667/1000 - partial_train_loss: 61.5619 \n",
      "Epoch: [667/1000], TrainLoss: 75.5312602179391\n",
      "training Loss has not improved for 20 epochs.\n",
      "current in epoch    667      batch 0\n",
      "RLoss: 171.3385467529297\n",
      "current in epoch    667      batch 1\n",
      "RLoss: 164.33848571777344\n",
      "current in epoch    667      batch 2\n",
      "RLoss: 88.27677154541016\n",
      "current in epoch    667      batch 3\n",
      "RLoss: 101.86614227294922\n",
      "current in epoch    667      batch 4\n",
      "RLoss: 30.852413177490234\n",
      "current in epoch    667      batch 5\n",
      "RLoss: 77.13050079345703\n",
      "========================================\n",
      "Epoch 668/1000 - partial_train_loss: 80.6303 \n",
      "Epoch: [668/1000], TrainLoss: 70.65435995374408\n",
      "training Loss has not improved for 21 epochs.\n",
      "current in epoch    668      batch 0\n",
      "RLoss: 32.97862243652344\n",
      "current in epoch    668      batch 1\n",
      "RLoss: 90.10474395751953\n",
      "current in epoch    668      batch 2\n",
      "RLoss: 98.31777954101562\n",
      "current in epoch    668      batch 3\n",
      "RLoss: 110.63509368896484\n",
      "current in epoch    668      batch 4\n",
      "RLoss: 63.84075164794922\n",
      "current in epoch    668      batch 5\n",
      "RLoss: 866.366455078125\n",
      "========================================\n",
      "Epoch 669/1000 - partial_train_loss: 97.8125 \n",
      "Epoch: [669/1000], TrainLoss: 822.3221827915737\n",
      "training Loss has not improved for 22 epochs.\n",
      "current in epoch    669      batch 0\n",
      "RLoss: 32.056270599365234\n",
      "current in epoch    669      batch 1\n",
      "RLoss: 100.16616821289062\n",
      "current in epoch    669      batch 2\n",
      "RLoss: 101.90277862548828\n",
      "current in epoch    669      batch 3\n",
      "RLoss: 30.877960205078125\n",
      "current in epoch    669      batch 4\n",
      "RLoss: 114.47096252441406\n",
      "current in epoch    669      batch 5\n",
      "RLoss: 83.87177276611328\n",
      "========================================\n",
      "Epoch 670/1000 - partial_train_loss: 244.4317 \n",
      "Epoch: [670/1000], TrainLoss: 91.0470665522984\n",
      "training Loss has not improved for 23 epochs.\n",
      "current in epoch    670      batch 0\n",
      "RLoss: 174.56100463867188\n",
      "current in epoch    670      batch 1\n",
      "RLoss: 9.45236587524414\n",
      "current in epoch    670      batch 2\n",
      "RLoss: 55.5709228515625\n",
      "current in epoch    670      batch 3\n",
      "RLoss: 347.3169860839844\n",
      "current in epoch    670      batch 4\n",
      "RLoss: 139.0724639892578\n",
      "current in epoch    670      batch 5\n",
      "RLoss: 57.76862716674805\n",
      "========================================\n",
      "Epoch 671/1000 - partial_train_loss: 141.2674 \n",
      "sorting training set\n",
      "Epoch 671/1000 - Training loss: 137.9962 \n",
      "========================================\n",
      "Epoch: [671/1000], TrainLoss: 146.6612341512034\n",
      "training Loss has not improved for 24 epochs.\n",
      "current in epoch    671      batch 0\n",
      "RLoss: 104.15286254882812\n",
      "current in epoch    671      batch 1\n",
      "RLoss: 498.2563171386719\n",
      "current in epoch    671      batch 2\n",
      "RLoss: 71.68741607666016\n",
      "current in epoch    671      batch 3\n",
      "RLoss: 225.37838745117188\n",
      "current in epoch    671      batch 4\n",
      "RLoss: 55.51469802856445\n",
      "current in epoch    671      batch 5\n",
      "RLoss: 21.23970603942871\n",
      "========================================\n",
      "Epoch 672/1000 - partial_train_loss: 251.4508 \n",
      "Epoch: [672/1000], TrainLoss: 146.07336929866247\n",
      "training Loss has not improved for 25 epochs.\n",
      "current in epoch    672      batch 0\n",
      "RLoss: 424.92999267578125\n",
      "current in epoch    672      batch 1\n",
      "RLoss: 22.31587791442871\n",
      "current in epoch    672      batch 2\n",
      "RLoss: 6.628862380981445\n",
      "current in epoch    672      batch 3\n",
      "RLoss: 15.604700088500977\n",
      "current in epoch    672      batch 4\n",
      "RLoss: 11.128617286682129\n",
      "current in epoch    672      batch 5\n",
      "RLoss: 4.2826948165893555\n",
      "========================================\n",
      "Epoch 673/1000 - partial_train_loss: 52.9755 \n",
      "Epoch: [673/1000], TrainLoss: 6.766200167792184\n",
      "training Loss has not improved for 26 epochs.\n",
      "current in epoch    673      batch 0\n",
      "RLoss: 37.66616439819336\n",
      "current in epoch    673      batch 1\n",
      "RLoss: 20.7633113861084\n",
      "current in epoch    673      batch 2\n",
      "RLoss: 23.33188819885254\n",
      "current in epoch    673      batch 3\n",
      "RLoss: 27.437721252441406\n",
      "current in epoch    673      batch 4\n",
      "RLoss: 105.03605651855469\n",
      "current in epoch    673      batch 5\n",
      "RLoss: 28.24469566345215\n",
      "========================================\n",
      "Epoch 674/1000 - partial_train_loss: 30.8795 \n",
      "Epoch: [674/1000], TrainLoss: 73.66271768297467\n",
      "training Loss has not improved for 27 epochs.\n",
      "current in epoch    674      batch 0\n",
      "RLoss: 63.84078598022461\n",
      "current in epoch    674      batch 1\n",
      "RLoss: 154.7706298828125\n",
      "current in epoch    674      batch 2\n",
      "RLoss: 55.28894805908203\n",
      "current in epoch    674      batch 3\n",
      "RLoss: 11.031455039978027\n",
      "current in epoch    674      batch 4\n",
      "RLoss: 257.835205078125\n",
      "current in epoch    674      batch 5\n",
      "RLoss: 44.02722930908203\n",
      "========================================\n",
      "Epoch 675/1000 - partial_train_loss: 91.1593 \n",
      "Epoch: [675/1000], TrainLoss: 101.82535634722028\n",
      "training Loss has not improved for 28 epochs.\n",
      "current in epoch    675      batch 0\n",
      "RLoss: 26.890125274658203\n",
      "current in epoch    675      batch 1\n",
      "RLoss: 706.8902587890625\n",
      "current in epoch    675      batch 2\n",
      "RLoss: 2.7987029552459717\n",
      "current in epoch    675      batch 3\n",
      "RLoss: 421.3493347167969\n",
      "current in epoch    675      batch 4\n",
      "RLoss: 38.9141845703125\n",
      "current in epoch    675      batch 5\n",
      "RLoss: 456.6509094238281\n",
      "========================================\n",
      "Epoch 676/1000 - partial_train_loss: 217.7414 \n",
      "sorting training set\n",
      "Epoch 676/1000 - Training loss: 386.3735 \n",
      "========================================\n",
      "Epoch: [676/1000], TrainLoss: 412.4584754729285\n",
      "training Loss has not improved for 29 epochs.\n",
      "current in epoch    676      batch 0\n",
      "RLoss: 139.55030822753906\n",
      "current in epoch    676      batch 1\n",
      "RLoss: 121.40094757080078\n",
      "current in epoch    676      batch 2\n",
      "RLoss: 33.26426696777344\n",
      "current in epoch    676      batch 3\n",
      "RLoss: 304.36944580078125\n",
      "current in epoch    676      batch 4\n",
      "RLoss: 97.14351654052734\n",
      "current in epoch    676      batch 5\n",
      "RLoss: 14.181306838989258\n",
      "========================================\n",
      "Epoch 677/1000 - partial_train_loss: 495.4563 \n",
      "Epoch: [677/1000], TrainLoss: 10.31122374534607\n",
      "training Loss has not improved for 30 epochs.\n",
      "current in epoch    677      batch 0\n",
      "RLoss: 122.63664245605469\n",
      "current in epoch    677      batch 1\n",
      "RLoss: 25.417736053466797\n",
      "current in epoch    677      batch 2\n",
      "RLoss: 40.36892318725586\n",
      "current in epoch    677      batch 3\n",
      "RLoss: 27.593990325927734\n",
      "current in epoch    677      batch 4\n",
      "RLoss: 19.462440490722656\n",
      "current in epoch    677      batch 5\n",
      "RLoss: 50.97806930541992\n",
      "========================================\n",
      "Epoch 678/1000 - partial_train_loss: 32.6285 \n",
      "Epoch: [678/1000], TrainLoss: 77.60681860787528\n",
      "training Loss has not improved for 31 epochs.\n",
      "current in epoch    678      batch 0\n",
      "RLoss: 440.35943603515625\n",
      "current in epoch    678      batch 1\n",
      "RLoss: 589.4263916015625\n",
      "current in epoch    678      batch 2\n",
      "RLoss: 16.470014572143555\n",
      "current in epoch    678      batch 3\n",
      "RLoss: 50.53913497924805\n",
      "current in epoch    678      batch 4\n",
      "RLoss: 55.629295349121094\n",
      "current in epoch    678      batch 5\n",
      "RLoss: 93.35011291503906\n",
      "========================================\n",
      "Epoch 679/1000 - partial_train_loss: 185.4823 \n",
      "Epoch: [679/1000], TrainLoss: 77.0559971673148\n",
      "training Loss has not improved for 32 epochs.\n",
      "current in epoch    679      batch 0\n",
      "RLoss: 1646.141357421875\n",
      "current in epoch    679      batch 1\n",
      "RLoss: 108.31778717041016\n",
      "current in epoch    679      batch 2\n",
      "RLoss: 69.95918273925781\n",
      "current in epoch    679      batch 3\n",
      "RLoss: 24.12627601623535\n",
      "current in epoch    679      batch 4\n",
      "RLoss: 15.519474983215332\n",
      "current in epoch    679      batch 5\n",
      "RLoss: 118.34568786621094\n",
      "========================================\n",
      "Epoch 680/1000 - partial_train_loss: 193.2875 \n",
      "Epoch: [680/1000], TrainLoss: 108.01330321175712\n",
      "training Loss has not improved for 33 epochs.\n",
      "current in epoch    680      batch 0\n",
      "RLoss: 84.2130126953125\n",
      "current in epoch    680      batch 1\n",
      "RLoss: 156.0900115966797\n",
      "current in epoch    680      batch 2\n",
      "RLoss: 759.520263671875\n",
      "current in epoch    680      batch 3\n",
      "RLoss: 13.439397811889648\n",
      "current in epoch    680      batch 4\n",
      "RLoss: 159.7139892578125\n",
      "current in epoch    680      batch 5\n",
      "RLoss: 40.55259704589844\n",
      "========================================\n",
      "Epoch 681/1000 - partial_train_loss: 192.2841 \n",
      "sorting training set\n",
      "Epoch 681/1000 - Training loss: 45.0273 \n",
      "========================================\n",
      "Epoch: [681/1000], TrainLoss: 46.72909125560734\n",
      "training Loss has not improved for 34 epochs.\n",
      "current in epoch    681      batch 0\n",
      "RLoss: 755.0735473632812\n",
      "current in epoch    681      batch 1\n",
      "RLoss: 44.57843780517578\n",
      "current in epoch    681      batch 2\n",
      "RLoss: 9.071173667907715\n",
      "current in epoch    681      batch 3\n",
      "RLoss: 8.088506698608398\n",
      "current in epoch    681      batch 4\n",
      "RLoss: 38.348533630371094\n",
      "current in epoch    681      batch 5\n",
      "RLoss: 106.05790710449219\n",
      "========================================\n",
      "Epoch 682/1000 - partial_train_loss: 156.3796 \n",
      "Epoch: [682/1000], TrainLoss: 100.50647708347866\n",
      "training Loss has not improved for 35 epochs.\n",
      "current in epoch    682      batch 0\n",
      "RLoss: 391.2394714355469\n",
      "current in epoch    682      batch 1\n",
      "RLoss: 73.35685729980469\n",
      "current in epoch    682      batch 2\n",
      "RLoss: 5.545509338378906\n",
      "current in epoch    682      batch 3\n",
      "RLoss: 125.5453872680664\n",
      "current in epoch    682      batch 4\n",
      "RLoss: 15.961576461791992\n",
      "current in epoch    682      batch 5\n",
      "RLoss: 86.26197052001953\n",
      "========================================\n",
      "Epoch 683/1000 - partial_train_loss: 87.7373 \n",
      "Epoch: [683/1000], TrainLoss: 82.38348933628627\n",
      "training Loss has not improved for 36 epochs.\n",
      "current in epoch    683      batch 0\n",
      "RLoss: 28.945119857788086\n",
      "current in epoch    683      batch 1\n",
      "RLoss: 128.057373046875\n",
      "current in epoch    683      batch 2\n",
      "RLoss: 80.46015167236328\n",
      "current in epoch    683      batch 3\n",
      "RLoss: 9.40832805633545\n",
      "current in epoch    683      batch 4\n",
      "RLoss: 40.5004768371582\n",
      "current in epoch    683      batch 5\n",
      "RLoss: 18.938121795654297\n",
      "========================================\n",
      "Epoch 684/1000 - partial_train_loss: 59.2902 \n",
      "Epoch: [684/1000], TrainLoss: 23.443072966166906\n",
      "training Loss has not improved for 37 epochs.\n",
      "current in epoch    684      batch 0\n",
      "RLoss: 267.3655090332031\n",
      "current in epoch    684      batch 1\n",
      "RLoss: 10.521967887878418\n",
      "current in epoch    684      batch 2\n",
      "RLoss: 48.42423629760742\n",
      "current in epoch    684      batch 3\n",
      "RLoss: 2.2723453044891357\n",
      "current in epoch    684      batch 4\n",
      "RLoss: 1.7888952493667603\n",
      "current in epoch    684      batch 5\n",
      "RLoss: 72.875244140625\n",
      "========================================\n",
      "Epoch 685/1000 - partial_train_loss: 50.8969 \n",
      "Epoch: [685/1000], TrainLoss: 42.86504118783133\n",
      "training Loss has not improved for 38 epochs.\n",
      "current in epoch    685      batch 0\n",
      "RLoss: 14.370624542236328\n",
      "current in epoch    685      batch 1\n",
      "RLoss: 18.41607666015625\n",
      "current in epoch    685      batch 2\n",
      "RLoss: 122.7874755859375\n",
      "current in epoch    685      batch 3\n",
      "RLoss: 56.607749938964844\n",
      "current in epoch    685      batch 4\n",
      "RLoss: 135.83106994628906\n",
      "current in epoch    685      batch 5\n",
      "RLoss: 23.412721633911133\n",
      "========================================\n",
      "Epoch 686/1000 - partial_train_loss: 60.6569 \n",
      "sorting training set\n",
      "Epoch 686/1000 - Training loss: 23.0876 \n",
      "========================================\n",
      "Epoch: [686/1000], TrainLoss: 24.88474317413667\n",
      "training Loss has not improved for 39 epochs.\n",
      "current in epoch    686      batch 0\n",
      "RLoss: 304.9532165527344\n",
      "current in epoch    686      batch 1\n",
      "RLoss: 42.16726303100586\n",
      "current in epoch    686      batch 2\n",
      "RLoss: 130.0295867919922\n",
      "current in epoch    686      batch 3\n",
      "RLoss: 29.554641723632812\n",
      "current in epoch    686      batch 4\n",
      "RLoss: 19.14200210571289\n",
      "current in epoch    686      batch 5\n",
      "RLoss: 48.14787673950195\n",
      "========================================\n",
      "Epoch 687/1000 - partial_train_loss: 87.0272 \n",
      "Epoch: [687/1000], TrainLoss: 37.77551909855434\n",
      "training Loss has not improved for 40 epochs.\n",
      "current in epoch    687      batch 0\n",
      "RLoss: 206.57229614257812\n",
      "current in epoch    687      batch 1\n",
      "RLoss: 28.205463409423828\n",
      "current in epoch    687      batch 2\n",
      "RLoss: 25.824399948120117\n",
      "current in epoch    687      batch 3\n",
      "RLoss: 23.162221908569336\n",
      "current in epoch    687      batch 4\n",
      "RLoss: 61.51441955566406\n",
      "current in epoch    687      batch 5\n",
      "RLoss: 99.25492095947266\n",
      "========================================\n",
      "Epoch 688/1000 - partial_train_loss: 74.5214 \n",
      "Epoch: [688/1000], TrainLoss: 89.50993401663644\n",
      "training Loss has not improved for 41 epochs.\n",
      "current in epoch    688      batch 0\n",
      "RLoss: 21.715633392333984\n",
      "current in epoch    688      batch 1\n",
      "RLoss: 65.34503936767578\n",
      "current in epoch    688      batch 2\n",
      "RLoss: 70.66767883300781\n",
      "current in epoch    688      batch 3\n",
      "RLoss: 15.500460624694824\n",
      "current in epoch    688      batch 4\n",
      "RLoss: 27.932336807250977\n",
      "current in epoch    688      batch 5\n",
      "RLoss: 81.7693862915039\n",
      "========================================\n",
      "Epoch 689/1000 - partial_train_loss: 45.6350 \n",
      "Epoch: [689/1000], TrainLoss: 82.46717888968331\n",
      "training Loss has not improved for 42 epochs.\n",
      "current in epoch    689      batch 0\n",
      "RLoss: 33.26206588745117\n",
      "current in epoch    689      batch 1\n",
      "RLoss: 65.03557586669922\n",
      "current in epoch    689      batch 2\n",
      "RLoss: 9.441848754882812\n",
      "current in epoch    689      batch 3\n",
      "RLoss: 152.587646484375\n",
      "current in epoch    689      batch 4\n",
      "RLoss: 469.8133850097656\n",
      "current in epoch    689      batch 5\n",
      "RLoss: 26.847028732299805\n",
      "========================================\n",
      "Epoch 690/1000 - partial_train_loss: 148.5313 \n",
      "Epoch: [690/1000], TrainLoss: 29.82988010134016\n",
      "training Loss has not improved for 43 epochs.\n",
      "current in epoch    690      batch 0\n",
      "RLoss: 95.59524536132812\n",
      "current in epoch    690      batch 1\n",
      "RLoss: 63.910255432128906\n",
      "current in epoch    690      batch 2\n",
      "RLoss: 5.5632147789001465\n",
      "current in epoch    690      batch 3\n",
      "RLoss: 90.65991973876953\n",
      "current in epoch    690      batch 4\n",
      "RLoss: 9.770340919494629\n",
      "current in epoch    690      batch 5\n",
      "RLoss: 59.19139099121094\n",
      "========================================\n",
      "Epoch 691/1000 - partial_train_loss: 42.7778 \n",
      "sorting training set\n",
      "Epoch 691/1000 - Training loss: 63.9708 \n",
      "========================================\n",
      "Epoch: [691/1000], TrainLoss: 68.50081939983724\n",
      "training Loss has not improved for 44 epochs.\n",
      "current in epoch    691      batch 0\n",
      "RLoss: 6.435532569885254\n",
      "current in epoch    691      batch 1\n",
      "RLoss: 22.8758487701416\n",
      "current in epoch    691      batch 2\n",
      "RLoss: 22.829483032226562\n",
      "current in epoch    691      batch 3\n",
      "RLoss: 359.5350646972656\n",
      "current in epoch    691      batch 4\n",
      "RLoss: 327.29449462890625\n",
      "current in epoch    691      batch 5\n",
      "RLoss: 111.40487670898438\n",
      "========================================\n",
      "Epoch 692/1000 - partial_train_loss: 161.2557 \n",
      "Epoch: [692/1000], TrainLoss: 106.50702340262276\n",
      "training Loss has not improved for 45 epochs.\n",
      "current in epoch    692      batch 0\n",
      "RLoss: 49.805694580078125\n",
      "current in epoch    692      batch 1\n",
      "RLoss: 207.29278564453125\n",
      "current in epoch    692      batch 2\n",
      "RLoss: 10.103984832763672\n",
      "current in epoch    692      batch 3\n",
      "RLoss: 5.202626705169678\n",
      "current in epoch    692      batch 4\n",
      "RLoss: 58.126678466796875\n",
      "current in epoch    692      batch 5\n",
      "RLoss: 35.045326232910156\n",
      "========================================\n",
      "Epoch 693/1000 - partial_train_loss: 66.4131 \n",
      "Epoch: [693/1000], TrainLoss: 38.90682462283543\n",
      "training Loss has not improved for 46 epochs.\n",
      "current in epoch    693      batch 0\n",
      "RLoss: 56.623844146728516\n",
      "current in epoch    693      batch 1\n",
      "RLoss: 3.603184223175049\n",
      "current in epoch    693      batch 2\n",
      "RLoss: 11.921348571777344\n",
      "current in epoch    693      batch 3\n",
      "RLoss: 31.28390121459961\n",
      "current in epoch    693      batch 4\n",
      "RLoss: 24.56566047668457\n",
      "current in epoch    693      batch 5\n",
      "RLoss: 36.75261688232422\n",
      "========================================\n",
      "Epoch 694/1000 - partial_train_loss: 23.8155 \n",
      "Epoch: [694/1000], TrainLoss: 47.5017478125436\n",
      "training Loss has not improved for 47 epochs.\n",
      "current in epoch    694      batch 0\n",
      "RLoss: 26.44093132019043\n",
      "current in epoch    694      batch 1\n",
      "RLoss: 5.937630653381348\n",
      "current in epoch    694      batch 2\n",
      "RLoss: 107.09339141845703\n",
      "current in epoch    694      batch 3\n",
      "RLoss: 8.397289276123047\n",
      "current in epoch    694      batch 4\n",
      "RLoss: 5.316623210906982\n",
      "current in epoch    694      batch 5\n",
      "RLoss: 11.915295600891113\n",
      "========================================\n",
      "Epoch 695/1000 - partial_train_loss: 44.9302 \n",
      "Epoch: [695/1000], TrainLoss: 9.841864245278495\n",
      "training Loss has not improved for 48 epochs.\n",
      "current in epoch    695      batch 0\n",
      "RLoss: 40.204227447509766\n",
      "current in epoch    695      batch 1\n",
      "RLoss: 87.11087799072266\n",
      "current in epoch    695      batch 2\n",
      "RLoss: 59.9270133972168\n",
      "current in epoch    695      batch 3\n",
      "RLoss: 5.691514492034912\n",
      "current in epoch    695      batch 4\n",
      "RLoss: 17.53223991394043\n",
      "current in epoch    695      batch 5\n",
      "RLoss: 18.161663055419922\n",
      "========================================\n",
      "Epoch 696/1000 - partial_train_loss: 37.6550 \n",
      "sorting training set\n",
      "Epoch 696/1000 - Training loss: 21.5518 \n",
      "========================================\n",
      "Epoch: [696/1000], TrainLoss: 23.09199400559037\n",
      "training Loss has not improved for 49 epochs.\n",
      "current in epoch    696      batch 0\n",
      "RLoss: 17.92038345336914\n",
      "current in epoch    696      batch 1\n",
      "RLoss: 76.20515441894531\n",
      "current in epoch    696      batch 2\n",
      "RLoss: 8.73643970489502\n",
      "current in epoch    696      batch 3\n",
      "RLoss: 307.7716064453125\n",
      "current in epoch    696      batch 4\n",
      "RLoss: 99.27648162841797\n",
      "current in epoch    696      batch 5\n",
      "RLoss: 62.53916931152344\n",
      "========================================\n",
      "Epoch 697/1000 - partial_train_loss: 87.6872 \n",
      "Epoch: [697/1000], TrainLoss: 64.49304485321045\n",
      "training Loss has not improved for 50 epochs.\n",
      "current in epoch    697      batch 0\n",
      "RLoss: 238.16461181640625\n",
      "current in epoch    697      batch 1\n",
      "RLoss: 230.09552001953125\n",
      "current in epoch    697      batch 2\n",
      "RLoss: 22.368118286132812\n",
      "current in epoch    697      batch 3\n",
      "RLoss: 17.230117797851562\n",
      "current in epoch    697      batch 4\n",
      "RLoss: 187.005859375\n",
      "current in epoch    697      batch 5\n",
      "RLoss: 108.73744201660156\n",
      "========================================\n",
      "Epoch 698/1000 - partial_train_loss: 118.8633 \n",
      "Epoch: [698/1000], TrainLoss: 157.57424518040247\n",
      "training Loss has not improved for 51 epochs.\n",
      "current in epoch    698      batch 0\n",
      "RLoss: 467.5924072265625\n",
      "current in epoch    698      batch 1\n",
      "RLoss: 66.64826965332031\n",
      "current in epoch    698      batch 2\n",
      "RLoss: 154.73843383789062\n",
      "current in epoch    698      batch 3\n",
      "RLoss: 31.41990852355957\n",
      "current in epoch    698      batch 4\n",
      "RLoss: 207.35987854003906\n",
      "current in epoch    698      batch 5\n",
      "RLoss: 105.18909454345703\n",
      "========================================\n",
      "Epoch 699/1000 - partial_train_loss: 197.1178 \n",
      "Epoch: [699/1000], TrainLoss: 174.35007885524206\n",
      "training Loss has not improved for 52 epochs.\n",
      "current in epoch    699      batch 0\n",
      "RLoss: 310.1535949707031\n",
      "current in epoch    699      batch 1\n",
      "RLoss: 432.5460205078125\n",
      "current in epoch    699      batch 2\n",
      "RLoss: 10.500848770141602\n",
      "current in epoch    699      batch 3\n",
      "RLoss: 74.65514373779297\n",
      "current in epoch    699      batch 4\n",
      "RLoss: 20.648548126220703\n",
      "current in epoch    699      batch 5\n",
      "RLoss: 202.4161376953125\n",
      "========================================\n",
      "Epoch 700/1000 - partial_train_loss: 176.2701 \n",
      "Epoch: [700/1000], TrainLoss: 195.79287229265486\n",
      "training Loss has not improved for 53 epochs.\n",
      "current in epoch    700      batch 0\n",
      "RLoss: 121.43408203125\n",
      "current in epoch    700      batch 1\n",
      "RLoss: 37.9024658203125\n",
      "current in epoch    700      batch 2\n",
      "RLoss: 44.02350997924805\n",
      "current in epoch    700      batch 3\n",
      "RLoss: 91.57475280761719\n",
      "current in epoch    700      batch 4\n",
      "RLoss: 135.7337646484375\n",
      "current in epoch    700      batch 5\n",
      "RLoss: 66.14519500732422\n",
      "========================================\n",
      "Epoch 701/1000 - partial_train_loss: 129.5304 \n",
      "sorting training set\n",
      "Epoch 701/1000 - Training loss: 66.9461 \n",
      "========================================\n",
      "Epoch: [701/1000], TrainLoss: 71.46932766038984\n",
      "training Loss has not improved for 54 epochs.\n",
      "current in epoch    701      batch 0\n",
      "RLoss: 122.45631408691406\n",
      "current in epoch    701      batch 1\n",
      "RLoss: 122.6923599243164\n",
      "current in epoch    701      batch 2\n",
      "RLoss: 180.9247589111328\n",
      "current in epoch    701      batch 3\n",
      "RLoss: 71.96963500976562\n",
      "current in epoch    701      batch 4\n",
      "RLoss: 136.3313751220703\n",
      "current in epoch    701      batch 5\n",
      "RLoss: 9.105114936828613\n",
      "========================================\n",
      "Epoch 702/1000 - partial_train_loss: 146.0417 \n",
      "Epoch: [702/1000], TrainLoss: 18.23213427407401\n",
      "training Loss has not improved for 55 epochs.\n",
      "current in epoch    702      batch 0\n",
      "RLoss: 190.23489379882812\n",
      "current in epoch    702      batch 1\n",
      "RLoss: 183.86326599121094\n",
      "current in epoch    702      batch 2\n",
      "RLoss: 174.60748291015625\n",
      "current in epoch    702      batch 3\n",
      "RLoss: 143.32749938964844\n",
      "current in epoch    702      batch 4\n",
      "RLoss: 287.5022888183594\n",
      "current in epoch    702      batch 5\n",
      "RLoss: 126.3907241821289\n",
      "========================================\n",
      "Epoch 703/1000 - partial_train_loss: 158.8308 \n",
      "Epoch: [703/1000], TrainLoss: 76.13330166680473\n",
      "training Loss has not improved for 56 epochs.\n",
      "current in epoch    703      batch 0\n",
      "RLoss: 1483.88427734375\n",
      "current in epoch    703      batch 1\n",
      "RLoss: 22.954429626464844\n",
      "current in epoch    703      batch 2\n",
      "RLoss: 227.5523681640625\n",
      "current in epoch    703      batch 3\n",
      "RLoss: 54.50961685180664\n",
      "current in epoch    703      batch 4\n",
      "RLoss: 31.969667434692383\n",
      "current in epoch    703      batch 5\n",
      "RLoss: 187.99176025390625\n",
      "========================================\n",
      "Epoch 704/1000 - partial_train_loss: 324.4720 \n",
      "Epoch: [704/1000], TrainLoss: 163.75747326442175\n",
      "training Loss has not improved for 57 epochs.\n",
      "current in epoch    704      batch 0\n",
      "RLoss: 31.259170532226562\n",
      "current in epoch    704      batch 1\n",
      "RLoss: 959.9463500976562\n",
      "current in epoch    704      batch 2\n",
      "RLoss: 435.51422119140625\n",
      "current in epoch    704      batch 3\n",
      "RLoss: 24.915769577026367\n",
      "current in epoch    704      batch 4\n",
      "RLoss: 325.4950866699219\n",
      "current in epoch    704      batch 5\n",
      "RLoss: 44.472999572753906\n",
      "========================================\n",
      "Epoch 705/1000 - partial_train_loss: 302.2696 \n",
      "Epoch: [705/1000], TrainLoss: 41.89183718817575\n",
      "training Loss has not improved for 58 epochs.\n",
      "current in epoch    705      batch 0\n",
      "RLoss: 332.65545654296875\n",
      "current in epoch    705      batch 1\n",
      "RLoss: 376.8397216796875\n",
      "current in epoch    705      batch 2\n",
      "RLoss: 74.43634033203125\n",
      "current in epoch    705      batch 3\n",
      "RLoss: 52.23584747314453\n",
      "current in epoch    705      batch 4\n",
      "RLoss: 121.16619873046875\n",
      "current in epoch    705      batch 5\n",
      "RLoss: 285.5628356933594\n",
      "========================================\n",
      "Epoch 706/1000 - partial_train_loss: 148.8571 \n",
      "sorting training set\n",
      "Epoch 706/1000 - Training loss: 221.5029 \n",
      "========================================\n",
      "Epoch: [706/1000], TrainLoss: 236.21505137841086\n",
      "training Loss has not improved for 59 epochs.\n",
      "current in epoch    706      batch 0\n",
      "RLoss: 45.31148910522461\n",
      "current in epoch    706      batch 1\n",
      "RLoss: 111.97608184814453\n",
      "current in epoch    706      batch 2\n",
      "RLoss: 14.108604431152344\n",
      "current in epoch    706      batch 3\n",
      "RLoss: 84.6580810546875\n",
      "current in epoch    706      batch 4\n",
      "RLoss: 48.83024597167969\n",
      "current in epoch    706      batch 5\n",
      "RLoss: 76.89990997314453\n",
      "========================================\n",
      "Epoch 707/1000 - partial_train_loss: 157.8353 \n",
      "Epoch: [707/1000], TrainLoss: 73.8252182006836\n",
      "training Loss has not improved for 60 epochs.\n",
      "current in epoch    707      batch 0\n",
      "RLoss: 154.81578063964844\n",
      "current in epoch    707      batch 1\n",
      "RLoss: 10.447741508483887\n",
      "current in epoch    707      batch 2\n",
      "RLoss: 36.395530700683594\n",
      "current in epoch    707      batch 3\n",
      "RLoss: 181.77899169921875\n",
      "current in epoch    707      batch 4\n",
      "RLoss: 114.4620132446289\n",
      "current in epoch    707      batch 5\n",
      "RLoss: 7.994135856628418\n",
      "========================================\n",
      "Epoch 708/1000 - partial_train_loss: 88.0767 \n",
      "Epoch: [708/1000], TrainLoss: 12.863848413739886\n",
      "training Loss has not improved for 61 epochs.\n",
      "current in epoch    708      batch 0\n",
      "RLoss: 106.43885803222656\n",
      "current in epoch    708      batch 1\n",
      "RLoss: 34.67623519897461\n",
      "current in epoch    708      batch 2\n",
      "RLoss: 92.14777374267578\n",
      "current in epoch    708      batch 3\n",
      "RLoss: 32.83319091796875\n",
      "current in epoch    708      batch 4\n",
      "RLoss: 14.826088905334473\n",
      "current in epoch    708      batch 5\n",
      "RLoss: 7.990226745605469\n",
      "========================================\n",
      "Epoch 709/1000 - partial_train_loss: 50.5104 \n",
      "Epoch: [709/1000], TrainLoss: 10.167607579912458\n",
      "training Loss has not improved for 62 epochs.\n",
      "current in epoch    709      batch 0\n",
      "RLoss: 13.20431900024414\n",
      "current in epoch    709      batch 1\n",
      "RLoss: 11.079887390136719\n",
      "current in epoch    709      batch 2\n",
      "RLoss: 306.41912841796875\n",
      "current in epoch    709      batch 3\n",
      "RLoss: 368.4110107421875\n",
      "current in epoch    709      batch 4\n",
      "RLoss: 234.63156127929688\n",
      "current in epoch    709      batch 5\n",
      "RLoss: 90.6082763671875\n",
      "========================================\n",
      "Epoch 710/1000 - partial_train_loss: 141.9829 \n",
      "Epoch: [710/1000], TrainLoss: 89.60368292672294\n",
      "training Loss has not improved for 63 epochs.\n",
      "current in epoch    710      batch 0\n",
      "RLoss: 32.03487014770508\n",
      "current in epoch    710      batch 1\n",
      "RLoss: 9.658416748046875\n",
      "current in epoch    710      batch 2\n",
      "RLoss: 35.64134216308594\n",
      "current in epoch    710      batch 3\n",
      "RLoss: 252.87939453125\n",
      "current in epoch    710      batch 4\n",
      "RLoss: 127.26901245117188\n",
      "current in epoch    710      batch 5\n",
      "RLoss: 15.322066307067871\n",
      "========================================\n",
      "Epoch 711/1000 - partial_train_loss: 99.0102 \n",
      "sorting training set\n",
      "Epoch 711/1000 - Training loss: 19.9979 \n",
      "========================================\n",
      "Epoch: [711/1000], TrainLoss: 21.923529675373214\n",
      "training Loss has not improved for 64 epochs.\n",
      "current in epoch    711      batch 0\n",
      "RLoss: 581.1400756835938\n",
      "current in epoch    711      batch 1\n",
      "RLoss: 581.2384033203125\n",
      "current in epoch    711      batch 2\n",
      "RLoss: 352.0539855957031\n",
      "current in epoch    711      batch 3\n",
      "RLoss: 21.109521865844727\n",
      "current in epoch    711      batch 4\n",
      "RLoss: 149.33929443359375\n",
      "current in epoch    711      batch 5\n",
      "RLoss: 14.331953048706055\n",
      "========================================\n",
      "Epoch 712/1000 - partial_train_loss: 303.0513 \n",
      "Epoch: [712/1000], TrainLoss: 13.33277518408639\n",
      "training Loss has not improved for 65 epochs.\n",
      "current in epoch    712      batch 0\n",
      "RLoss: 421.02777099609375\n",
      "current in epoch    712      batch 1\n",
      "RLoss: 41.48540496826172\n",
      "current in epoch    712      batch 2\n",
      "RLoss: 39.11483383178711\n",
      "current in epoch    712      batch 3\n",
      "RLoss: 17.99203109741211\n",
      "current in epoch    712      batch 4\n",
      "RLoss: 39.53838348388672\n",
      "current in epoch    712      batch 5\n",
      "RLoss: 348.7198791503906\n",
      "========================================\n",
      "Epoch 713/1000 - partial_train_loss: 78.2130 \n",
      "Epoch: [713/1000], TrainLoss: 297.8376263209752\n",
      "training Loss has not improved for 66 epochs.\n",
      "current in epoch    713      batch 0\n",
      "RLoss: 8.441126823425293\n",
      "current in epoch    713      batch 1\n",
      "RLoss: 25.82999610900879\n",
      "current in epoch    713      batch 2\n",
      "RLoss: 234.50880432128906\n",
      "current in epoch    713      batch 3\n",
      "RLoss: 20.248483657836914\n",
      "current in epoch    713      batch 4\n",
      "RLoss: 82.23111724853516\n",
      "current in epoch    713      batch 5\n",
      "RLoss: 116.51048278808594\n",
      "========================================\n",
      "Epoch 714/1000 - partial_train_loss: 131.9042 \n",
      "Epoch: [714/1000], TrainLoss: 100.40948895045689\n",
      "training Loss has not improved for 67 epochs.\n",
      "current in epoch    714      batch 0\n",
      "RLoss: 50.663108825683594\n",
      "current in epoch    714      batch 1\n",
      "RLoss: 24.86004066467285\n",
      "current in epoch    714      batch 2\n",
      "RLoss: 51.57956314086914\n",
      "current in epoch    714      batch 3\n",
      "RLoss: 183.1734619140625\n",
      "current in epoch    714      batch 4\n",
      "RLoss: 40.55720138549805\n",
      "current in epoch    714      batch 5\n",
      "RLoss: 135.9938507080078\n",
      "========================================\n",
      "Epoch 715/1000 - partial_train_loss: 102.4431 \n",
      "Epoch: [715/1000], TrainLoss: 125.03863389151437\n",
      "training Loss has not improved for 68 epochs.\n",
      "current in epoch    715      batch 0\n",
      "RLoss: 281.47509765625\n",
      "current in epoch    715      batch 1\n",
      "RLoss: 34.98576354980469\n",
      "current in epoch    715      batch 2\n",
      "RLoss: 4.2227959632873535\n",
      "current in epoch    715      batch 3\n",
      "RLoss: 58.612483978271484\n",
      "current in epoch    715      batch 4\n",
      "RLoss: 182.576171875\n",
      "current in epoch    715      batch 5\n",
      "RLoss: 127.8403549194336\n",
      "========================================\n",
      "Epoch 716/1000 - partial_train_loss: 102.5382 \n",
      "sorting training set\n",
      "Epoch 716/1000 - Training loss: 125.5303 \n",
      "========================================\n",
      "Epoch: [716/1000], TrainLoss: 134.14247380680806\n",
      "training Loss has not improved for 69 epochs.\n",
      "current in epoch    716      batch 0\n",
      "RLoss: 244.3317108154297\n",
      "current in epoch    716      batch 1\n",
      "RLoss: 34.01180648803711\n",
      "current in epoch    716      batch 2\n",
      "RLoss: 366.8162841796875\n",
      "current in epoch    716      batch 3\n",
      "RLoss: 59.622554779052734\n",
      "current in epoch    716      batch 4\n",
      "RLoss: 5.046873092651367\n",
      "current in epoch    716      batch 5\n",
      "RLoss: 8.768045425415039\n",
      "========================================\n",
      "Epoch 717/1000 - partial_train_loss: 159.4368 \n",
      "Epoch: [717/1000], TrainLoss: 22.53833511897496\n",
      "training Loss has not improved for 70 epochs.\n",
      "current in epoch    717      batch 0\n",
      "RLoss: 247.32110595703125\n",
      "current in epoch    717      batch 1\n",
      "RLoss: 69.83161163330078\n",
      "current in epoch    717      batch 2\n",
      "RLoss: 164.3651885986328\n",
      "current in epoch    717      batch 3\n",
      "RLoss: 430.76385498046875\n",
      "current in epoch    717      batch 4\n",
      "RLoss: 417.10601806640625\n",
      "current in epoch    717      batch 5\n",
      "RLoss: 29.824872970581055\n",
      "========================================\n",
      "Epoch 718/1000 - partial_train_loss: 204.0248 \n",
      "Epoch: [718/1000], TrainLoss: 20.96692568915231\n",
      "training Loss has not improved for 71 epochs.\n",
      "current in epoch    718      batch 0\n",
      "RLoss: 37.71546173095703\n",
      "current in epoch    718      batch 1\n",
      "RLoss: 14.801912307739258\n",
      "current in epoch    718      batch 2\n",
      "RLoss: 63.86801528930664\n",
      "current in epoch    718      batch 3\n",
      "RLoss: 150.8125\n",
      "current in epoch    718      batch 4\n",
      "RLoss: 64.25425720214844\n",
      "current in epoch    718      batch 5\n",
      "RLoss: 59.3331413269043\n",
      "========================================\n",
      "Epoch 719/1000 - partial_train_loss: 60.8950 \n",
      "Epoch: [719/1000], TrainLoss: 105.13233702523368\n",
      "training Loss has not improved for 72 epochs.\n",
      "current in epoch    719      batch 0\n",
      "RLoss: 23.28860855102539\n",
      "current in epoch    719      batch 1\n",
      "RLoss: 184.81707763671875\n",
      "current in epoch    719      batch 2\n",
      "RLoss: 16.910329818725586\n",
      "current in epoch    719      batch 3\n",
      "RLoss: 96.89022064208984\n",
      "current in epoch    719      batch 4\n",
      "RLoss: 59.7294921875\n",
      "current in epoch    719      batch 5\n",
      "RLoss: 33.37870788574219\n",
      "========================================\n",
      "Epoch 720/1000 - partial_train_loss: 74.0599 \n",
      "Epoch: [720/1000], TrainLoss: 52.64833354949951\n",
      "training Loss has not improved for 73 epochs.\n",
      "current in epoch    720      batch 0\n",
      "RLoss: 46.163761138916016\n",
      "current in epoch    720      batch 1\n",
      "RLoss: 72.47611236572266\n",
      "current in epoch    720      batch 2\n",
      "RLoss: 61.64973068237305\n",
      "current in epoch    720      batch 3\n",
      "RLoss: 259.8812561035156\n",
      "current in epoch    720      batch 4\n",
      "RLoss: 12.259160041809082\n",
      "current in epoch    720      batch 5\n",
      "RLoss: 6.718996524810791\n",
      "========================================\n",
      "Epoch 721/1000 - partial_train_loss: 83.4030 \n",
      "sorting training set\n",
      "Epoch 721/1000 - Training loss: 4.3981 \n",
      "========================================\n",
      "Epoch: [721/1000], TrainLoss: 4.690518071419414\n",
      "training Loss has not improved for 74 epochs.\n",
      "current in epoch    721      batch 0\n",
      "RLoss: 44.62813949584961\n",
      "current in epoch    721      batch 1\n",
      "RLoss: 50.601646423339844\n",
      "current in epoch    721      batch 2\n",
      "RLoss: 22.709386825561523\n",
      "current in epoch    721      batch 3\n",
      "RLoss: 167.20553588867188\n",
      "current in epoch    721      batch 4\n",
      "RLoss: 60.01567077636719\n",
      "current in epoch    721      batch 5\n",
      "RLoss: 16.138032913208008\n",
      "========================================\n",
      "Epoch 722/1000 - partial_train_loss: 76.1970 \n",
      "Epoch: [722/1000], TrainLoss: 18.65718569925853\n",
      "training Loss has not improved for 75 epochs.\n",
      "current in epoch    722      batch 0\n",
      "RLoss: 56.30875015258789\n",
      "current in epoch    722      batch 1\n",
      "RLoss: 87.37579345703125\n",
      "current in epoch    722      batch 2\n",
      "RLoss: 221.8737030029297\n",
      "current in epoch    722      batch 3\n",
      "RLoss: 36.53287124633789\n",
      "current in epoch    722      batch 4\n",
      "RLoss: 18.858409881591797\n",
      "current in epoch    722      batch 5\n",
      "RLoss: 65.94857025146484\n",
      "========================================\n",
      "Epoch 723/1000 - partial_train_loss: 71.8718 \n",
      "Epoch: [723/1000], TrainLoss: 52.17162813459124\n",
      "training Loss has not improved for 76 epochs.\n",
      "current in epoch    723      batch 0\n",
      "RLoss: 160.7812957763672\n",
      "current in epoch    723      batch 1\n",
      "RLoss: 340.3292236328125\n",
      "current in epoch    723      batch 2\n",
      "RLoss: 51.67473602294922\n",
      "current in epoch    723      batch 3\n",
      "RLoss: 15.651369094848633\n",
      "current in epoch    723      batch 4\n",
      "RLoss: 37.34315490722656\n",
      "current in epoch    723      batch 5\n",
      "RLoss: 36.94166564941406\n",
      "========================================\n",
      "Epoch 724/1000 - partial_train_loss: 89.6273 \n",
      "Epoch: [724/1000], TrainLoss: 42.02785178593227\n",
      "training Loss has not improved for 77 epochs.\n",
      "current in epoch    724      batch 0\n",
      "RLoss: 105.30182647705078\n",
      "current in epoch    724      batch 1\n",
      "RLoss: 230.14344787597656\n",
      "current in epoch    724      batch 2\n",
      "RLoss: 38.25593948364258\n",
      "current in epoch    724      batch 3\n",
      "RLoss: 422.21966552734375\n",
      "current in epoch    724      batch 4\n",
      "RLoss: 28.762786865234375\n",
      "current in epoch    724      batch 5\n",
      "RLoss: 215.51124572753906\n",
      "========================================\n",
      "Epoch 725/1000 - partial_train_loss: 165.8397 \n",
      "Epoch: [725/1000], TrainLoss: 145.73723084586007\n",
      "training Loss has not improved for 78 epochs.\n",
      "current in epoch    725      batch 0\n",
      "RLoss: 7.942156791687012\n",
      "current in epoch    725      batch 1\n",
      "RLoss: 1291.805908203125\n",
      "current in epoch    725      batch 2\n",
      "RLoss: 8.422553062438965\n",
      "current in epoch    725      batch 3\n",
      "RLoss: 14.437583923339844\n",
      "current in epoch    725      batch 4\n",
      "RLoss: 43.96144485473633\n",
      "current in epoch    725      batch 5\n",
      "RLoss: 17.10673713684082\n",
      "========================================\n",
      "Epoch 726/1000 - partial_train_loss: 256.2708 \n",
      "sorting training set\n",
      "Epoch 726/1000 - Training loss: 23.0879 \n",
      "========================================\n",
      "Epoch: [726/1000], TrainLoss: 25.364462413726923\n",
      "training Loss has not improved for 79 epochs.\n",
      "current in epoch    726      batch 0\n",
      "RLoss: 102.11962890625\n",
      "current in epoch    726      batch 1\n",
      "RLoss: 1407.5859375\n",
      "current in epoch    726      batch 2\n",
      "RLoss: 29.515336990356445\n",
      "current in epoch    726      batch 3\n",
      "RLoss: 37.22201156616211\n",
      "current in epoch    726      batch 4\n",
      "RLoss: 141.7758026123047\n",
      "current in epoch    726      batch 5\n",
      "RLoss: 19.610355377197266\n",
      "========================================\n",
      "Epoch 727/1000 - partial_train_loss: 302.8034 \n",
      "Epoch: [727/1000], TrainLoss: 17.406798703329905\n",
      "training Loss has not improved for 80 epochs.\n",
      "current in epoch    727      batch 0\n",
      "RLoss: 210.41590881347656\n",
      "current in epoch    727      batch 1\n",
      "RLoss: 3.864731788635254\n",
      "current in epoch    727      batch 2\n",
      "RLoss: 22.228694915771484\n",
      "current in epoch    727      batch 3\n",
      "RLoss: 6.545726299285889\n",
      "current in epoch    727      batch 4\n",
      "RLoss: 81.0511703491211\n",
      "current in epoch    727      batch 5\n",
      "RLoss: 51.70427703857422\n",
      "========================================\n",
      "Epoch 728/1000 - partial_train_loss: 49.1104 \n",
      "Epoch: [728/1000], TrainLoss: 67.42706557682583\n",
      "training Loss has not improved for 81 epochs.\n",
      "current in epoch    728      batch 0\n",
      "RLoss: 47.490745544433594\n",
      "current in epoch    728      batch 1\n",
      "RLoss: 77.09404754638672\n",
      "current in epoch    728      batch 2\n",
      "RLoss: 85.51274871826172\n",
      "current in epoch    728      batch 3\n",
      "RLoss: 5.1803178787231445\n",
      "current in epoch    728      batch 4\n",
      "RLoss: 8.139692306518555\n",
      "current in epoch    728      batch 5\n",
      "RLoss: 95.65438079833984\n",
      "========================================\n",
      "Epoch 729/1000 - partial_train_loss: 54.1944 \n",
      "Epoch: [729/1000], TrainLoss: 117.78262117930821\n",
      "training Loss has not improved for 82 epochs.\n",
      "current in epoch    729      batch 0\n",
      "RLoss: 31.653953552246094\n",
      "current in epoch    729      batch 1\n",
      "RLoss: 217.57398986816406\n",
      "current in epoch    729      batch 2\n",
      "RLoss: 34.31932830810547\n",
      "current in epoch    729      batch 3\n",
      "RLoss: 10.788393020629883\n",
      "current in epoch    729      batch 4\n",
      "RLoss: 3.967327117919922\n",
      "current in epoch    729      batch 5\n",
      "RLoss: 133.69326782226562\n",
      "========================================\n",
      "Epoch 730/1000 - partial_train_loss: 68.8597 \n",
      "Epoch: [730/1000], TrainLoss: 182.905335017613\n",
      "training Loss has not improved for 83 epochs.\n",
      "current in epoch    730      batch 0\n",
      "RLoss: 33.30833053588867\n",
      "current in epoch    730      batch 1\n",
      "RLoss: 28.870763778686523\n",
      "current in epoch    730      batch 2\n",
      "RLoss: 21.234586715698242\n",
      "current in epoch    730      batch 3\n",
      "RLoss: 40.450313568115234\n",
      "current in epoch    730      batch 4\n",
      "RLoss: 60.07341003417969\n",
      "current in epoch    730      batch 5\n",
      "RLoss: 65.66732025146484\n",
      "========================================\n",
      "Epoch 731/1000 - partial_train_loss: 58.2445 \n",
      "sorting training set\n",
      "Epoch 731/1000 - Training loss: 128.7477 \n",
      "========================================\n",
      "Epoch: [731/1000], TrainLoss: 137.72034925825753\n",
      "training Loss has not improved for 84 epochs.\n",
      "current in epoch    731      batch 0\n",
      "RLoss: 144.1761016845703\n",
      "current in epoch    731      batch 1\n",
      "RLoss: 77.28650665283203\n",
      "current in epoch    731      batch 2\n",
      "RLoss: 547.36328125\n",
      "current in epoch    731      batch 3\n",
      "RLoss: 19.206823348999023\n",
      "current in epoch    731      batch 4\n",
      "RLoss: 74.123046875\n",
      "current in epoch    731      batch 5\n",
      "RLoss: 23.171873092651367\n",
      "========================================\n",
      "Epoch 732/1000 - partial_train_loss: 236.8250 \n",
      "Epoch: [732/1000], TrainLoss: 78.09170825140816\n",
      "training Loss has not improved for 85 epochs.\n",
      "current in epoch    732      batch 0\n",
      "RLoss: 8.750229835510254\n",
      "current in epoch    732      batch 1\n",
      "RLoss: 86.01992797851562\n",
      "current in epoch    732      batch 2\n",
      "RLoss: 106.64823913574219\n",
      "current in epoch    732      batch 3\n",
      "RLoss: 662.3626708984375\n",
      "current in epoch    732      batch 4\n",
      "RLoss: 225.36549377441406\n",
      "current in epoch    732      batch 5\n",
      "RLoss: 184.60533142089844\n",
      "========================================\n",
      "Epoch 733/1000 - partial_train_loss: 170.3807 \n",
      "Epoch: [733/1000], TrainLoss: 200.76520211356026\n",
      "training Loss has not improved for 86 epochs.\n",
      "current in epoch    733      batch 0\n",
      "RLoss: 158.5021514892578\n",
      "current in epoch    733      batch 1\n",
      "RLoss: 93.50897979736328\n",
      "current in epoch    733      batch 2\n",
      "RLoss: 77.68502807617188\n",
      "current in epoch    733      batch 3\n",
      "RLoss: 238.3160858154297\n",
      "current in epoch    733      batch 4\n",
      "RLoss: 135.68109130859375\n",
      "current in epoch    733      batch 5\n",
      "RLoss: 73.75682067871094\n",
      "========================================\n",
      "Epoch 734/1000 - partial_train_loss: 196.2309 \n",
      "Epoch: [734/1000], TrainLoss: 83.34593881879535\n",
      "training Loss has not improved for 87 epochs.\n",
      "current in epoch    734      batch 0\n",
      "RLoss: 903.6982421875\n",
      "current in epoch    734      batch 1\n",
      "RLoss: 111.78629302978516\n",
      "current in epoch    734      batch 2\n",
      "RLoss: 380.0518798828125\n",
      "current in epoch    734      batch 3\n",
      "RLoss: 17.459396362304688\n",
      "current in epoch    734      batch 4\n",
      "RLoss: 98.05010986328125\n",
      "current in epoch    734      batch 5\n",
      "RLoss: 15.691755294799805\n",
      "========================================\n",
      "Epoch 735/1000 - partial_train_loss: 217.5559 \n",
      "Epoch: [735/1000], TrainLoss: 22.536870377404348\n",
      "training Loss has not improved for 88 epochs.\n",
      "current in epoch    735      batch 0\n",
      "RLoss: 8.04443073272705\n",
      "current in epoch    735      batch 1\n",
      "RLoss: 337.9141845703125\n",
      "current in epoch    735      batch 2\n",
      "RLoss: 41.7054443359375\n",
      "current in epoch    735      batch 3\n",
      "RLoss: 180.41893005371094\n",
      "current in epoch    735      batch 4\n",
      "RLoss: 93.6217041015625\n",
      "current in epoch    735      batch 5\n",
      "RLoss: 63.4661865234375\n",
      "========================================\n",
      "Epoch 736/1000 - partial_train_loss: 127.7789 \n",
      "sorting training set\n",
      "Epoch 736/1000 - Training loss: 84.5441 \n",
      "========================================\n",
      "Epoch: [736/1000], TrainLoss: 90.73941768533668\n",
      "training Loss has not improved for 89 epochs.\n",
      "current in epoch    736      batch 0\n",
      "RLoss: 464.1904296875\n",
      "current in epoch    736      batch 1\n",
      "RLoss: 352.1612854003906\n",
      "current in epoch    736      batch 2\n",
      "RLoss: 507.7169189453125\n",
      "current in epoch    736      batch 3\n",
      "RLoss: 51.41896438598633\n",
      "current in epoch    736      batch 4\n",
      "RLoss: 76.9052505493164\n",
      "current in epoch    736      batch 5\n",
      "RLoss: 143.98109436035156\n",
      "========================================\n",
      "Epoch 737/1000 - partial_train_loss: 302.2403 \n",
      "Epoch: [737/1000], TrainLoss: 139.260708127703\n",
      "training Loss has not improved for 90 epochs.\n",
      "current in epoch    737      batch 0\n",
      "RLoss: 233.47671508789062\n",
      "current in epoch    737      batch 1\n",
      "RLoss: 36.86296844482422\n",
      "current in epoch    737      batch 2\n",
      "RLoss: 115.97216033935547\n",
      "current in epoch    737      batch 3\n",
      "RLoss: 109.56336212158203\n",
      "current in epoch    737      batch 4\n",
      "RLoss: 205.15673828125\n",
      "current in epoch    737      batch 5\n",
      "RLoss: 68.54109954833984\n",
      "========================================\n",
      "Epoch 738/1000 - partial_train_loss: 136.5308 \n",
      "Epoch: [738/1000], TrainLoss: 78.23445156642369\n",
      "training Loss has not improved for 91 epochs.\n",
      "current in epoch    738      batch 0\n",
      "RLoss: 13.172003746032715\n",
      "current in epoch    738      batch 1\n",
      "RLoss: 69.0558090209961\n",
      "current in epoch    738      batch 2\n",
      "RLoss: 62.948455810546875\n",
      "current in epoch    738      batch 3\n",
      "RLoss: 25.89681053161621\n",
      "current in epoch    738      batch 4\n",
      "RLoss: 83.70594787597656\n",
      "current in epoch    738      batch 5\n",
      "RLoss: 109.66769409179688\n",
      "========================================\n",
      "Epoch 739/1000 - partial_train_loss: 75.8942 \n",
      "Epoch: [739/1000], TrainLoss: 123.66265378679547\n",
      "training Loss has not improved for 92 epochs.\n",
      "current in epoch    739      batch 0\n",
      "RLoss: 108.153564453125\n",
      "current in epoch    739      batch 1\n",
      "RLoss: 104.64630126953125\n",
      "current in epoch    739      batch 2\n",
      "RLoss: 30.515962600708008\n",
      "current in epoch    739      batch 3\n",
      "RLoss: 240.78648376464844\n",
      "current in epoch    739      batch 4\n",
      "RLoss: 86.19071197509766\n",
      "current in epoch    739      batch 5\n",
      "RLoss: 99.34111022949219\n",
      "========================================\n",
      "Epoch 740/1000 - partial_train_loss: 101.4794 \n",
      "Epoch: [740/1000], TrainLoss: 116.17188780648368\n",
      "training Loss has not improved for 93 epochs.\n",
      "current in epoch    740      batch 0\n",
      "RLoss: 24.448169708251953\n",
      "current in epoch    740      batch 1\n",
      "RLoss: 56.2921028137207\n",
      "current in epoch    740      batch 2\n",
      "RLoss: 42.27520751953125\n",
      "current in epoch    740      batch 3\n",
      "RLoss: 13.448875427246094\n",
      "current in epoch    740      batch 4\n",
      "RLoss: 25.416614532470703\n",
      "current in epoch    740      batch 5\n",
      "RLoss: 79.306884765625\n",
      "========================================\n",
      "Epoch 741/1000 - partial_train_loss: 41.5322 \n",
      "sorting training set\n",
      "Epoch 741/1000 - Training loss: 97.8978 \n",
      "========================================\n",
      "Epoch: [741/1000], TrainLoss: 103.93659091108186\n",
      "training Loss has not improved for 94 epochs.\n",
      "current in epoch    741      batch 0\n",
      "RLoss: 37.09403991699219\n",
      "current in epoch    741      batch 1\n",
      "RLoss: 5.332207202911377\n",
      "current in epoch    741      batch 2\n",
      "RLoss: 34.772151947021484\n",
      "current in epoch    741      batch 3\n",
      "RLoss: 5.858311653137207\n",
      "current in epoch    741      batch 4\n",
      "RLoss: 6.478460311889648\n",
      "current in epoch    741      batch 5\n",
      "RLoss: 32.615970611572266\n",
      "========================================\n",
      "Epoch 742/1000 - partial_train_loss: 76.7795 \n",
      "Epoch: [742/1000], TrainLoss: 56.9763377053397\n",
      "training Loss has not improved for 95 epochs.\n",
      "current in epoch    742      batch 0\n",
      "RLoss: 38.31232833862305\n",
      "current in epoch    742      batch 1\n",
      "RLoss: 81.97518920898438\n",
      "current in epoch    742      batch 2\n",
      "RLoss: 147.12022399902344\n",
      "current in epoch    742      batch 3\n",
      "RLoss: 5.643126487731934\n",
      "current in epoch    742      batch 4\n",
      "RLoss: 20.522916793823242\n",
      "current in epoch    742      batch 5\n",
      "RLoss: 17.179014205932617\n",
      "========================================\n",
      "Epoch 743/1000 - partial_train_loss: 52.6867 \n",
      "Epoch: [743/1000], TrainLoss: 14.207756008420672\n",
      "training Loss has not improved for 96 epochs.\n",
      "current in epoch    743      batch 0\n",
      "RLoss: 68.04728698730469\n",
      "current in epoch    743      batch 1\n",
      "RLoss: 61.526695251464844\n",
      "current in epoch    743      batch 2\n",
      "RLoss: 51.789466857910156\n",
      "current in epoch    743      batch 3\n",
      "RLoss: 8.720905303955078\n",
      "current in epoch    743      batch 4\n",
      "RLoss: 172.5882568359375\n",
      "current in epoch    743      batch 5\n",
      "RLoss: 203.32664489746094\n",
      "========================================\n",
      "Epoch 744/1000 - partial_train_loss: 68.7078 \n",
      "Epoch: [744/1000], TrainLoss: 156.08559363228935\n",
      "training Loss has not improved for 97 epochs.\n",
      "current in epoch    744      batch 0\n",
      "RLoss: 61.9446907043457\n",
      "current in epoch    744      batch 1\n",
      "RLoss: 274.64434814453125\n",
      "current in epoch    744      batch 2\n",
      "RLoss: 36.057491302490234\n",
      "current in epoch    744      batch 3\n",
      "RLoss: 13.950488090515137\n",
      "current in epoch    744      batch 4\n",
      "RLoss: 5.067774772644043\n",
      "current in epoch    744      batch 5\n",
      "RLoss: 9.584611892700195\n",
      "========================================\n",
      "Epoch 745/1000 - partial_train_loss: 127.3971 \n",
      "Epoch: [745/1000], TrainLoss: 27.444521052496775\n",
      "training Loss has not improved for 98 epochs.\n",
      "current in epoch    745      batch 0\n",
      "RLoss: 81.84172821044922\n",
      "current in epoch    745      batch 1\n",
      "RLoss: 34.48616409301758\n",
      "current in epoch    745      batch 2\n",
      "RLoss: 9.709146499633789\n",
      "current in epoch    745      batch 3\n",
      "RLoss: 268.0210876464844\n",
      "current in epoch    745      batch 4\n",
      "RLoss: 28.50337028503418\n",
      "current in epoch    745      batch 5\n",
      "RLoss: 247.62033081054688\n",
      "========================================\n",
      "Epoch 746/1000 - partial_train_loss: 71.3411 \n",
      "sorting training set\n",
      "Epoch 746/1000 - Training loss: 298.5780 \n",
      "========================================\n",
      "Epoch: [746/1000], TrainLoss: 318.8338349703449\n",
      "training Loss has not improved for 99 epochs.\n",
      "current in epoch    746      batch 0\n",
      "RLoss: 3.7557880878448486\n",
      "current in epoch    746      batch 1\n",
      "RLoss: 2.657916784286499\n",
      "current in epoch    746      batch 2\n",
      "RLoss: 189.3894500732422\n",
      "current in epoch    746      batch 3\n",
      "RLoss: 142.99769592285156\n",
      "current in epoch    746      batch 4\n",
      "RLoss: 92.91785430908203\n",
      "current in epoch    746      batch 5\n",
      "RLoss: 9.099902153015137\n",
      "========================================\n",
      "Epoch 747/1000 - partial_train_loss: 193.1580 \n",
      "Epoch: [747/1000], TrainLoss: 42.241888420922415\n",
      "training Loss has not improved for 100 epochs.\n",
      "current in epoch    747      batch 0\n",
      "RLoss: 38.022987365722656\n",
      "current in epoch    747      batch 1\n",
      "RLoss: 9.168123245239258\n",
      "current in epoch    747      batch 2\n",
      "RLoss: 38.89777755737305\n",
      "current in epoch    747      batch 3\n",
      "RLoss: 11.47062873840332\n",
      "current in epoch    747      batch 4\n",
      "RLoss: 35.76408004760742\n",
      "current in epoch    747      batch 5\n",
      "RLoss: 63.519439697265625\n",
      "========================================\n",
      "Epoch 748/1000 - partial_train_loss: 19.7613 \n",
      "Epoch: [748/1000], TrainLoss: 66.23876299176898\n",
      "training Loss has not improved for 101 epochs.\n",
      "current in epoch    748      batch 0\n",
      "RLoss: 60.704017639160156\n",
      "current in epoch    748      batch 1\n",
      "RLoss: 365.9678649902344\n",
      "current in epoch    748      batch 2\n",
      "RLoss: 133.86317443847656\n",
      "current in epoch    748      batch 3\n",
      "RLoss: 16.295379638671875\n",
      "current in epoch    748      batch 4\n",
      "RLoss: 16.976266860961914\n",
      "current in epoch    748      batch 5\n",
      "RLoss: 26.608253479003906\n",
      "========================================\n",
      "Epoch 749/1000 - partial_train_loss: 120.4825 \n",
      "Epoch: [749/1000], TrainLoss: 23.57132421221052\n",
      "training Loss has not improved for 102 epochs.\n",
      "current in epoch    749      batch 0\n",
      "RLoss: 4.806126594543457\n",
      "current in epoch    749      batch 1\n",
      "RLoss: 0.6395370364189148\n",
      "current in epoch    749      batch 2\n",
      "RLoss: 21.281387329101562\n",
      "current in epoch    749      batch 3\n",
      "RLoss: 25.245046615600586\n",
      "current in epoch    749      batch 4\n",
      "RLoss: 14.391131401062012\n",
      "current in epoch    749      batch 5\n",
      "RLoss: 9.233545303344727\n",
      "========================================\n",
      "Epoch 750/1000 - partial_train_loss: 19.7370 \n",
      "Epoch: [750/1000], TrainLoss: 16.274497764451162\n",
      "training Loss has not improved for 103 epochs.\n",
      "current in epoch    750      batch 0\n",
      "RLoss: 29.33754539489746\n",
      "current in epoch    750      batch 1\n",
      "RLoss: 80.37516021728516\n",
      "current in epoch    750      batch 2\n",
      "RLoss: 233.09359741210938\n",
      "current in epoch    750      batch 3\n",
      "RLoss: 4.86824893951416\n",
      "current in epoch    750      batch 4\n",
      "RLoss: 8.080158233642578\n",
      "current in epoch    750      batch 5\n",
      "RLoss: 36.01931381225586\n",
      "========================================\n",
      "Epoch 751/1000 - partial_train_loss: 54.9206 \n",
      "sorting training set\n",
      "Epoch 751/1000 - Training loss: 29.3201 \n",
      "========================================\n",
      "Epoch: [751/1000], TrainLoss: 31.509844084114572\n",
      "training Loss has not improved for 104 epochs.\n",
      "current in epoch    751      batch 0\n",
      "RLoss: 117.42501831054688\n",
      "current in epoch    751      batch 1\n",
      "RLoss: 97.04940032958984\n",
      "current in epoch    751      batch 2\n",
      "RLoss: 314.9434814453125\n",
      "current in epoch    751      batch 3\n",
      "RLoss: 62.75580978393555\n",
      "current in epoch    751      batch 4\n",
      "RLoss: 46.123985290527344\n",
      "current in epoch    751      batch 5\n",
      "RLoss: 446.57080078125\n",
      "========================================\n",
      "Epoch 752/1000 - partial_train_loss: 110.5799 \n",
      "Epoch: [752/1000], TrainLoss: 506.5050561087472\n",
      "training Loss has not improved for 105 epochs.\n",
      "current in epoch    752      batch 0\n",
      "RLoss: 284.3638610839844\n",
      "current in epoch    752      batch 1\n",
      "RLoss: 58.97495651245117\n",
      "current in epoch    752      batch 2\n",
      "RLoss: 241.2797393798828\n",
      "current in epoch    752      batch 3\n",
      "RLoss: 84.8896255493164\n",
      "current in epoch    752      batch 4\n",
      "RLoss: 410.329833984375\n",
      "current in epoch    752      batch 5\n",
      "RLoss: 134.354248046875\n",
      "========================================\n",
      "Epoch 753/1000 - partial_train_loss: 249.5334 \n",
      "Epoch: [753/1000], TrainLoss: 131.62123707362585\n",
      "training Loss has not improved for 106 epochs.\n",
      "current in epoch    753      batch 0\n",
      "RLoss: 163.1111602783203\n",
      "current in epoch    753      batch 1\n",
      "RLoss: 207.809814453125\n",
      "current in epoch    753      batch 2\n",
      "RLoss: 40.01268768310547\n",
      "current in epoch    753      batch 3\n",
      "RLoss: 97.6750717163086\n",
      "current in epoch    753      batch 4\n",
      "RLoss: 33.7589111328125\n",
      "current in epoch    753      batch 5\n",
      "RLoss: 1473.1246337890625\n",
      "========================================\n",
      "Epoch 754/1000 - partial_train_loss: 122.0320 \n",
      "Epoch: [754/1000], TrainLoss: 1414.8778424944196\n",
      "training Loss has not improved for 107 epochs.\n",
      "current in epoch    754      batch 0\n",
      "RLoss: 191.7336883544922\n",
      "current in epoch    754      batch 1\n",
      "RLoss: 121.99734497070312\n",
      "current in epoch    754      batch 2\n",
      "RLoss: 24.41257667541504\n",
      "current in epoch    754      batch 3\n",
      "RLoss: 24.245817184448242\n",
      "current in epoch    754      batch 4\n",
      "RLoss: 114.40478515625\n",
      "current in epoch    754      batch 5\n",
      "RLoss: 18.8516845703125\n",
      "========================================\n",
      "Epoch 755/1000 - partial_train_loss: 327.4802 \n",
      "Epoch: [755/1000], TrainLoss: 26.32388343129839\n",
      "training Loss has not improved for 108 epochs.\n",
      "current in epoch    755      batch 0\n",
      "RLoss: 38.449745178222656\n",
      "current in epoch    755      batch 1\n",
      "RLoss: 336.6311950683594\n",
      "current in epoch    755      batch 2\n",
      "RLoss: 22.39259910583496\n",
      "current in epoch    755      batch 3\n",
      "RLoss: 31.130332946777344\n",
      "current in epoch    755      batch 4\n",
      "RLoss: 182.15603637695312\n",
      "current in epoch    755      batch 5\n",
      "RLoss: 191.995849609375\n",
      "========================================\n",
      "Epoch 756/1000 - partial_train_loss: 98.7874 \n",
      "sorting training set\n",
      "Epoch 756/1000 - Training loss: 212.6890 \n",
      "========================================\n",
      "Epoch: [756/1000], TrainLoss: 226.36975655805205\n",
      "training Loss has not improved for 109 epochs.\n",
      "current in epoch    756      batch 0\n",
      "RLoss: 309.9875793457031\n",
      "current in epoch    756      batch 1\n",
      "RLoss: 644.9921875\n",
      "current in epoch    756      batch 2\n",
      "RLoss: 30.012134552001953\n",
      "current in epoch    756      batch 3\n",
      "RLoss: 28.452163696289062\n",
      "current in epoch    756      batch 4\n",
      "RLoss: 680.5028076171875\n",
      "current in epoch    756      batch 5\n",
      "RLoss: 563.9403686523438\n",
      "========================================\n",
      "Epoch 757/1000 - partial_train_loss: 407.3334 \n",
      "Epoch: [757/1000], TrainLoss: 535.6700243268695\n",
      "training Loss has not improved for 110 epochs.\n",
      "current in epoch    757      batch 0\n",
      "RLoss: 131.89669799804688\n",
      "current in epoch    757      batch 1\n",
      "RLoss: 51.021461486816406\n",
      "current in epoch    757      batch 2\n",
      "RLoss: 270.927978515625\n",
      "current in epoch    757      batch 3\n",
      "RLoss: 17.109392166137695\n",
      "current in epoch    757      batch 4\n",
      "RLoss: 12.083455085754395\n",
      "current in epoch    757      batch 5\n",
      "RLoss: 138.80780029296875\n",
      "========================================\n",
      "Epoch 758/1000 - partial_train_loss: 266.6262 \n",
      "Epoch: [758/1000], TrainLoss: 116.41229493277413\n",
      "training Loss has not improved for 111 epochs.\n",
      "current in epoch    758      batch 0\n",
      "RLoss: 55.84616470336914\n",
      "current in epoch    758      batch 1\n",
      "RLoss: 76.22819519042969\n",
      "current in epoch    758      batch 2\n",
      "RLoss: 143.19480895996094\n",
      "current in epoch    758      batch 3\n",
      "RLoss: 551.709228515625\n",
      "current in epoch    758      batch 4\n",
      "RLoss: 89.6670913696289\n",
      "current in epoch    758      batch 5\n",
      "RLoss: 123.57373046875\n",
      "========================================\n",
      "Epoch 759/1000 - partial_train_loss: 173.1759 \n",
      "Epoch: [759/1000], TrainLoss: 82.13926778520856\n",
      "training Loss has not improved for 112 epochs.\n",
      "current in epoch    759      batch 0\n",
      "RLoss: 763.23876953125\n",
      "current in epoch    759      batch 1\n",
      "RLoss: 258.4608459472656\n",
      "current in epoch    759      batch 2\n",
      "RLoss: 18.677753448486328\n",
      "current in epoch    759      batch 3\n",
      "RLoss: 4.096360683441162\n",
      "current in epoch    759      batch 4\n",
      "RLoss: 175.3691864013672\n",
      "current in epoch    759      batch 5\n",
      "RLoss: 21.0808048248291\n",
      "========================================\n",
      "Epoch 760/1000 - partial_train_loss: 167.6729 \n",
      "Epoch: [760/1000], TrainLoss: 36.18093459946768\n",
      "training Loss has not improved for 113 epochs.\n",
      "current in epoch    760      batch 0\n",
      "RLoss: 32.32888412475586\n",
      "current in epoch    760      batch 1\n",
      "RLoss: 159.11306762695312\n",
      "current in epoch    760      batch 2\n",
      "RLoss: 31.406978607177734\n",
      "current in epoch    760      batch 3\n",
      "RLoss: 155.903076171875\n",
      "current in epoch    760      batch 4\n",
      "RLoss: 64.65815734863281\n",
      "current in epoch    760      batch 5\n",
      "RLoss: 45.21145248413086\n",
      "========================================\n",
      "Epoch 761/1000 - partial_train_loss: 116.1829 \n",
      "sorting training set\n",
      "Epoch 761/1000 - Training loss: 37.7298 \n",
      "========================================\n",
      "Epoch: [761/1000], TrainLoss: 40.39290102825925\n",
      "training Loss has not improved for 114 epochs.\n",
      "current in epoch    761      batch 0\n",
      "RLoss: 104.95284271240234\n",
      "current in epoch    761      batch 1\n",
      "RLoss: 12.343672752380371\n",
      "current in epoch    761      batch 2\n",
      "RLoss: 107.60639953613281\n",
      "current in epoch    761      batch 3\n",
      "RLoss: 74.81747436523438\n",
      "current in epoch    761      batch 4\n",
      "RLoss: 5.591927528381348\n",
      "current in epoch    761      batch 5\n",
      "RLoss: 12.695792198181152\n",
      "========================================\n",
      "Epoch 762/1000 - partial_train_loss: 78.1351 \n",
      "Epoch: [762/1000], TrainLoss: 12.145918573651995\n",
      "training Loss has not improved for 115 epochs.\n",
      "current in epoch    762      batch 0\n",
      "RLoss: 24.026809692382812\n",
      "current in epoch    762      batch 1\n",
      "RLoss: 53.30510330200195\n",
      "current in epoch    762      batch 2\n",
      "RLoss: 55.959415435791016\n",
      "current in epoch    762      batch 3\n",
      "RLoss: 89.89761352539062\n",
      "current in epoch    762      batch 4\n",
      "RLoss: 54.14605712890625\n",
      "current in epoch    762      batch 5\n",
      "RLoss: 5.351658344268799\n",
      "========================================\n",
      "Epoch 763/1000 - partial_train_loss: 42.8302 \n",
      "Epoch: [763/1000], TrainLoss: 6.05373786176954\n",
      "training Loss has not improved for 116 epochs.\n",
      "current in epoch    763      batch 0\n",
      "RLoss: 39.33779525756836\n",
      "current in epoch    763      batch 1\n",
      "RLoss: 221.147705078125\n",
      "current in epoch    763      batch 2\n",
      "RLoss: 664.8443603515625\n",
      "current in epoch    763      batch 3\n",
      "RLoss: 11.763733863830566\n",
      "current in epoch    763      batch 4\n",
      "RLoss: 2.592508316040039\n",
      "current in epoch    763      batch 5\n",
      "RLoss: 759.630859375\n",
      "========================================\n",
      "Epoch 764/1000 - partial_train_loss: 131.4114 \n",
      "Epoch: [764/1000], TrainLoss: 747.3270939418247\n",
      "training Loss has not improved for 117 epochs.\n",
      "current in epoch    764      batch 0\n",
      "RLoss: 347.0801696777344\n",
      "current in epoch    764      batch 1\n",
      "RLoss: 31.926414489746094\n",
      "current in epoch    764      batch 2\n",
      "RLoss: 93.78990936279297\n",
      "current in epoch    764      batch 3\n",
      "RLoss: 22.517690658569336\n",
      "current in epoch    764      batch 4\n",
      "RLoss: 5.534152030944824\n",
      "current in epoch    764      batch 5\n",
      "RLoss: 45.84043884277344\n",
      "========================================\n",
      "Epoch 765/1000 - partial_train_loss: 211.2387 \n",
      "Epoch: [765/1000], TrainLoss: 39.39911508560181\n",
      "training Loss has not improved for 118 epochs.\n",
      "current in epoch    765      batch 0\n",
      "RLoss: 25.597782135009766\n",
      "current in epoch    765      batch 1\n",
      "RLoss: 522.6328125\n",
      "current in epoch    765      batch 2\n",
      "RLoss: 39.424381256103516\n",
      "current in epoch    765      batch 3\n",
      "RLoss: 79.8522720336914\n",
      "current in epoch    765      batch 4\n",
      "RLoss: 81.64144134521484\n",
      "current in epoch    765      batch 5\n",
      "RLoss: 135.16201782226562\n",
      "========================================\n",
      "Epoch 766/1000 - partial_train_loss: 137.1280 \n",
      "sorting training set\n",
      "Epoch 766/1000 - Training loss: 117.5605 \n",
      "========================================\n",
      "Epoch: [766/1000], TrainLoss: 125.7047033271525\n",
      "training Loss has not improved for 119 epochs.\n",
      "current in epoch    766      batch 0\n",
      "RLoss: 68.85904693603516\n",
      "current in epoch    766      batch 1\n",
      "RLoss: 45.09606170654297\n",
      "current in epoch    766      batch 2\n",
      "RLoss: 45.50322723388672\n",
      "current in epoch    766      batch 3\n",
      "RLoss: 47.2643928527832\n",
      "current in epoch    766      batch 4\n",
      "RLoss: 171.1426239013672\n",
      "current in epoch    766      batch 5\n",
      "RLoss: 64.62519073486328\n",
      "========================================\n",
      "Epoch 767/1000 - partial_train_loss: 132.1452 \n",
      "Epoch: [767/1000], TrainLoss: 45.01506832667759\n",
      "training Loss has not improved for 120 epochs.\n",
      "current in epoch    767      batch 0\n",
      "RLoss: 17.518266677856445\n",
      "current in epoch    767      batch 1\n",
      "RLoss: 34.143123626708984\n",
      "current in epoch    767      batch 2\n",
      "RLoss: 7.19205904006958\n",
      "current in epoch    767      batch 3\n",
      "RLoss: 8.558738708496094\n",
      "current in epoch    767      batch 4\n",
      "RLoss: 33.92201232910156\n",
      "current in epoch    767      batch 5\n",
      "RLoss: 74.17696380615234\n",
      "========================================\n",
      "Epoch 768/1000 - partial_train_loss: 29.3936 \n",
      "Epoch: [768/1000], TrainLoss: 115.67167854309082\n",
      "training Loss has not improved for 121 epochs.\n",
      "current in epoch    768      batch 0\n",
      "RLoss: 5.721909523010254\n",
      "current in epoch    768      batch 1\n",
      "RLoss: 12.818594932556152\n",
      "current in epoch    768      batch 2\n",
      "RLoss: 212.8388214111328\n",
      "current in epoch    768      batch 3\n",
      "RLoss: 132.178466796875\n",
      "current in epoch    768      batch 4\n",
      "RLoss: 115.9268798828125\n",
      "current in epoch    768      batch 5\n",
      "RLoss: 62.5070915222168\n",
      "========================================\n",
      "Epoch 769/1000 - partial_train_loss: 96.9696 \n",
      "Epoch: [769/1000], TrainLoss: 47.604365825653076\n",
      "training Loss has not improved for 122 epochs.\n",
      "current in epoch    769      batch 0\n",
      "RLoss: 90.60279846191406\n",
      "current in epoch    769      batch 1\n",
      "RLoss: 104.80809783935547\n",
      "current in epoch    769      batch 2\n",
      "RLoss: 45.37746047973633\n",
      "current in epoch    769      batch 3\n",
      "RLoss: 383.06085205078125\n",
      "current in epoch    769      batch 4\n",
      "RLoss: 5.332464218139648\n",
      "current in epoch    769      batch 5\n",
      "RLoss: 54.65766143798828\n",
      "========================================\n",
      "Epoch 770/1000 - partial_train_loss: 118.2524 \n",
      "Epoch: [770/1000], TrainLoss: 91.0153887612479\n",
      "training Loss has not improved for 123 epochs.\n",
      "current in epoch    770      batch 0\n",
      "RLoss: 163.4597625732422\n",
      "current in epoch    770      batch 1\n",
      "RLoss: 167.7967529296875\n",
      "current in epoch    770      batch 2\n",
      "RLoss: 109.23420715332031\n",
      "current in epoch    770      batch 3\n",
      "RLoss: 9.40243911743164\n",
      "current in epoch    770      batch 4\n",
      "RLoss: 98.93162536621094\n",
      "current in epoch    770      batch 5\n",
      "RLoss: 25.545089721679688\n",
      "========================================\n",
      "Epoch 771/1000 - partial_train_loss: 101.1790 \n",
      "sorting training set\n",
      "Epoch 771/1000 - Training loss: 80.7644 \n",
      "========================================\n",
      "Epoch: [771/1000], TrainLoss: 86.39686618203085\n",
      "training Loss has not improved for 124 epochs.\n",
      "current in epoch    771      batch 0\n",
      "RLoss: 131.5966796875\n",
      "current in epoch    771      batch 1\n",
      "RLoss: 34.29848861694336\n",
      "current in epoch    771      batch 2\n",
      "RLoss: 25.459922790527344\n",
      "current in epoch    771      batch 3\n",
      "RLoss: 104.76368713378906\n",
      "current in epoch    771      batch 4\n",
      "RLoss: 56.12652587890625\n",
      "current in epoch    771      batch 5\n",
      "RLoss: 98.21583557128906\n",
      "========================================\n",
      "Epoch 772/1000 - partial_train_loss: 138.6683 \n",
      "Epoch: [772/1000], TrainLoss: 73.94345133645194\n",
      "training Loss has not improved for 125 epochs.\n",
      "current in epoch    772      batch 0\n",
      "RLoss: 82.90058898925781\n",
      "current in epoch    772      batch 1\n",
      "RLoss: 278.1263122558594\n",
      "current in epoch    772      batch 2\n",
      "RLoss: 24.650470733642578\n",
      "current in epoch    772      batch 3\n",
      "RLoss: 396.7054443359375\n",
      "current in epoch    772      batch 4\n",
      "RLoss: 17.236501693725586\n",
      "current in epoch    772      batch 5\n",
      "RLoss: 20.46636390686035\n",
      "========================================\n",
      "Epoch 773/1000 - partial_train_loss: 156.3520 \n",
      "Epoch: [773/1000], TrainLoss: 37.36587572097778\n",
      "training Loss has not improved for 126 epochs.\n",
      "current in epoch    773      batch 0\n",
      "RLoss: 57.657569885253906\n",
      "current in epoch    773      batch 1\n",
      "RLoss: 47.85120391845703\n",
      "current in epoch    773      batch 2\n",
      "RLoss: 191.37110900878906\n",
      "current in epoch    773      batch 3\n",
      "RLoss: 92.93790435791016\n",
      "current in epoch    773      batch 4\n",
      "RLoss: 92.68990325927734\n",
      "current in epoch    773      batch 5\n",
      "RLoss: 68.4455337524414\n",
      "========================================\n",
      "Epoch 774/1000 - partial_train_loss: 100.5489 \n",
      "Epoch: [774/1000], TrainLoss: 82.12046269008091\n",
      "training Loss has not improved for 127 epochs.\n",
      "current in epoch    774      batch 0\n",
      "RLoss: 226.95704650878906\n",
      "current in epoch    774      batch 1\n",
      "RLoss: 44.33527755737305\n",
      "current in epoch    774      batch 2\n",
      "RLoss: 29.78326416015625\n",
      "current in epoch    774      batch 3\n",
      "RLoss: 3196.179443359375\n",
      "current in epoch    774      batch 4\n",
      "RLoss: 57.28145980834961\n",
      "current in epoch    774      batch 5\n",
      "RLoss: 44.66231155395508\n",
      "========================================\n",
      "Epoch 775/1000 - partial_train_loss: 571.8501 \n",
      "Epoch: [775/1000], TrainLoss: 29.55135420390538\n",
      "training Loss has not improved for 128 epochs.\n",
      "current in epoch    775      batch 0\n",
      "RLoss: 505.4107971191406\n",
      "current in epoch    775      batch 1\n",
      "RLoss: 45.97211456298828\n",
      "current in epoch    775      batch 2\n",
      "RLoss: 179.524658203125\n",
      "current in epoch    775      batch 3\n",
      "RLoss: 94.33232116699219\n",
      "current in epoch    775      batch 4\n",
      "RLoss: 83.22250366210938\n",
      "current in epoch    775      batch 5\n",
      "RLoss: 329.2606506347656\n",
      "========================================\n",
      "Epoch 776/1000 - partial_train_loss: 298.6795 \n",
      "sorting training set\n",
      "Epoch 776/1000 - Training loss: 293.5935 \n",
      "========================================\n",
      "Epoch: [776/1000], TrainLoss: 313.487142574474\n",
      "training Loss has not improved for 129 epochs.\n",
      "current in epoch    776      batch 0\n",
      "RLoss: 89.33092498779297\n",
      "current in epoch    776      batch 1\n",
      "RLoss: 582.3888549804688\n",
      "current in epoch    776      batch 2\n",
      "RLoss: 159.22671508789062\n",
      "current in epoch    776      batch 3\n",
      "RLoss: 239.5697479248047\n",
      "current in epoch    776      batch 4\n",
      "RLoss: 683.8348388671875\n",
      "current in epoch    776      batch 5\n",
      "RLoss: 212.7512969970703\n",
      "========================================\n",
      "Epoch 777/1000 - partial_train_loss: 556.5052 \n",
      "Epoch: [777/1000], TrainLoss: 235.08953857421875\n",
      "training Loss has not improved for 130 epochs.\n",
      "current in epoch    777      batch 0\n",
      "RLoss: 35.090179443359375\n",
      "current in epoch    777      batch 1\n",
      "RLoss: 87.0709457397461\n",
      "current in epoch    777      batch 2\n",
      "RLoss: 32.2893180847168\n",
      "current in epoch    777      batch 3\n",
      "RLoss: 6.1015472412109375\n",
      "current in epoch    777      batch 4\n",
      "RLoss: 13.128313064575195\n",
      "current in epoch    777      batch 5\n",
      "RLoss: 22.300588607788086\n",
      "========================================\n",
      "Epoch 778/1000 - partial_train_loss: 71.3999 \n",
      "Epoch: [778/1000], TrainLoss: 18.82181719371251\n",
      "training Loss has not improved for 131 epochs.\n",
      "current in epoch    778      batch 0\n",
      "RLoss: 99.48585510253906\n",
      "current in epoch    778      batch 1\n",
      "RLoss: 60.293216705322266\n",
      "current in epoch    778      batch 2\n",
      "RLoss: 491.9222412109375\n",
      "current in epoch    778      batch 3\n",
      "RLoss: 602.024169921875\n",
      "current in epoch    778      batch 4\n",
      "RLoss: 12.689114570617676\n",
      "current in epoch    778      batch 5\n",
      "RLoss: 7.831213474273682\n",
      "========================================\n",
      "Epoch 779/1000 - partial_train_loss: 185.5550 \n",
      "Epoch: [779/1000], TrainLoss: 14.967222724642072\n",
      "training Loss has not improved for 132 epochs.\n",
      "current in epoch    779      batch 0\n",
      "RLoss: 109.07794952392578\n",
      "current in epoch    779      batch 1\n",
      "RLoss: 938.2015991210938\n",
      "current in epoch    779      batch 2\n",
      "RLoss: 47.75082015991211\n",
      "current in epoch    779      batch 3\n",
      "RLoss: 7.483181476593018\n",
      "current in epoch    779      batch 4\n",
      "RLoss: 18.878896713256836\n",
      "current in epoch    779      batch 5\n",
      "RLoss: 111.3130111694336\n",
      "========================================\n",
      "Epoch 780/1000 - partial_train_loss: 191.0998 \n",
      "Epoch: [780/1000], TrainLoss: 125.53747149876186\n",
      "training Loss has not improved for 133 epochs.\n",
      "current in epoch    780      batch 0\n",
      "RLoss: 103.51786041259766\n",
      "current in epoch    780      batch 1\n",
      "RLoss: 217.110107421875\n",
      "current in epoch    780      batch 2\n",
      "RLoss: 61.59532165527344\n",
      "current in epoch    780      batch 3\n",
      "RLoss: 19.291011810302734\n",
      "current in epoch    780      batch 4\n",
      "RLoss: 78.73075103759766\n",
      "current in epoch    780      batch 5\n",
      "RLoss: 22.74672508239746\n",
      "========================================\n",
      "Epoch 781/1000 - partial_train_loss: 85.4906 \n",
      "sorting training set\n",
      "Epoch 781/1000 - Training loss: 15.3898 \n",
      "========================================\n",
      "Epoch: [781/1000], TrainLoss: 16.6257916036566\n",
      "training Loss has not improved for 134 epochs.\n",
      "current in epoch    781      batch 0\n",
      "RLoss: 28.521364212036133\n",
      "current in epoch    781      batch 1\n",
      "RLoss: 1.1742165088653564\n",
      "current in epoch    781      batch 2\n",
      "RLoss: 1.700015664100647\n",
      "current in epoch    781      batch 3\n",
      "RLoss: 49.21470260620117\n",
      "current in epoch    781      batch 4\n",
      "RLoss: 28.650917053222656\n",
      "current in epoch    781      batch 5\n",
      "RLoss: 10.337559700012207\n",
      "========================================\n",
      "Epoch 782/1000 - partial_train_loss: 25.9647 \n",
      "Epoch: [782/1000], TrainLoss: 8.466185161045619\n",
      "training Loss has not improved for 135 epochs.\n",
      "current in epoch    782      batch 0\n",
      "RLoss: 41.53887939453125\n",
      "current in epoch    782      batch 1\n",
      "RLoss: 55.23300552368164\n",
      "current in epoch    782      batch 2\n",
      "RLoss: 115.26068878173828\n",
      "current in epoch    782      batch 3\n",
      "RLoss: 130.73526000976562\n",
      "current in epoch    782      batch 4\n",
      "RLoss: 48.095035552978516\n",
      "current in epoch    782      batch 5\n",
      "RLoss: 65.37370300292969\n",
      "========================================\n",
      "Epoch 783/1000 - partial_train_loss: 60.5639 \n",
      "Epoch: [783/1000], TrainLoss: 59.16622734069824\n",
      "training Loss has not improved for 136 epochs.\n",
      "current in epoch    783      batch 0\n",
      "RLoss: 38.738155364990234\n",
      "current in epoch    783      batch 1\n",
      "RLoss: 96.20227813720703\n",
      "current in epoch    783      batch 2\n",
      "RLoss: 111.3127670288086\n",
      "current in epoch    783      batch 3\n",
      "RLoss: 24.95579719543457\n",
      "current in epoch    783      batch 4\n",
      "RLoss: 325.2099609375\n",
      "current in epoch    783      batch 5\n",
      "RLoss: 43.717872619628906\n",
      "========================================\n",
      "Epoch 784/1000 - partial_train_loss: 98.9856 \n",
      "Epoch: [784/1000], TrainLoss: 34.314362457820344\n",
      "training Loss has not improved for 137 epochs.\n",
      "current in epoch    784      batch 0\n",
      "RLoss: 35.258262634277344\n",
      "current in epoch    784      batch 1\n",
      "RLoss: 519.6768188476562\n",
      "current in epoch    784      batch 2\n",
      "RLoss: 30.384212493896484\n",
      "current in epoch    784      batch 3\n",
      "RLoss: 114.36681365966797\n",
      "current in epoch    784      batch 4\n",
      "RLoss: 47.78206253051758\n",
      "current in epoch    784      batch 5\n",
      "RLoss: 166.92364501953125\n",
      "========================================\n",
      "Epoch 785/1000 - partial_train_loss: 115.7100 \n",
      "Epoch: [785/1000], TrainLoss: 176.8129027230399\n",
      "training Loss has not improved for 138 epochs.\n",
      "current in epoch    785      batch 0\n",
      "RLoss: 481.69915771484375\n",
      "current in epoch    785      batch 1\n",
      "RLoss: 840.7731323242188\n",
      "current in epoch    785      batch 2\n",
      "RLoss: 206.24720764160156\n",
      "current in epoch    785      batch 3\n",
      "RLoss: 2.6643412113189697\n",
      "current in epoch    785      batch 4\n",
      "RLoss: 221.9267578125\n",
      "current in epoch    785      batch 5\n",
      "RLoss: 139.7706756591797\n",
      "========================================\n",
      "Epoch 786/1000 - partial_train_loss: 230.9473 \n",
      "sorting training set\n",
      "Epoch 786/1000 - Training loss: 112.9960 \n",
      "========================================\n",
      "Epoch: [786/1000], TrainLoss: 120.65259731488615\n",
      "training Loss has not improved for 139 epochs.\n",
      "current in epoch    786      batch 0\n",
      "RLoss: 6.664484977722168\n",
      "current in epoch    786      batch 1\n",
      "RLoss: 5.330475807189941\n",
      "current in epoch    786      batch 2\n",
      "RLoss: 23.736507415771484\n",
      "current in epoch    786      batch 3\n",
      "RLoss: 78.4294204711914\n",
      "current in epoch    786      batch 4\n",
      "RLoss: 95.79293060302734\n",
      "current in epoch    786      batch 5\n",
      "RLoss: 42.82847213745117\n",
      "========================================\n",
      "Epoch 787/1000 - partial_train_loss: 134.4480 \n",
      "Epoch: [787/1000], TrainLoss: 29.266854533127376\n",
      "training Loss has not improved for 140 epochs.\n",
      "current in epoch    787      batch 0\n",
      "RLoss: 42.953189849853516\n",
      "current in epoch    787      batch 1\n",
      "RLoss: 18.316020965576172\n",
      "current in epoch    787      batch 2\n",
      "RLoss: 71.30376434326172\n",
      "current in epoch    787      batch 3\n",
      "RLoss: 123.6623306274414\n",
      "current in epoch    787      batch 4\n",
      "RLoss: 180.13369750976562\n",
      "current in epoch    787      batch 5\n",
      "RLoss: 386.99310302734375\n",
      "========================================\n",
      "Epoch 788/1000 - partial_train_loss: 90.8416 \n",
      "Epoch: [788/1000], TrainLoss: 256.4411209651402\n",
      "training Loss has not improved for 141 epochs.\n",
      "current in epoch    788      batch 0\n",
      "RLoss: 22.083044052124023\n",
      "current in epoch    788      batch 1\n",
      "RLoss: 290.65496826171875\n",
      "current in epoch    788      batch 2\n",
      "RLoss: 387.455322265625\n",
      "current in epoch    788      batch 3\n",
      "RLoss: 27.03371238708496\n",
      "current in epoch    788      batch 4\n",
      "RLoss: 71.62776947021484\n",
      "current in epoch    788      batch 5\n",
      "RLoss: 14.980443954467773\n",
      "========================================\n",
      "Epoch 789/1000 - partial_train_loss: 263.5174 \n",
      "Epoch: [789/1000], TrainLoss: 16.63765857900892\n",
      "training Loss has not improved for 142 epochs.\n",
      "current in epoch    789      batch 0\n",
      "RLoss: 385.31689453125\n",
      "current in epoch    789      batch 1\n",
      "RLoss: 19.221914291381836\n",
      "current in epoch    789      batch 2\n",
      "RLoss: 14.739690780639648\n",
      "current in epoch    789      batch 3\n",
      "RLoss: 26.222993850708008\n",
      "current in epoch    789      batch 4\n",
      "RLoss: 16.81191062927246\n",
      "current in epoch    789      batch 5\n",
      "RLoss: 45.2545051574707\n",
      "========================================\n",
      "Epoch 790/1000 - partial_train_loss: 75.5975 \n",
      "Epoch: [790/1000], TrainLoss: 39.65429207256862\n",
      "training Loss has not improved for 143 epochs.\n",
      "current in epoch    790      batch 0\n",
      "RLoss: 108.5025863647461\n",
      "current in epoch    790      batch 1\n",
      "RLoss: 4.8830647468566895\n",
      "current in epoch    790      batch 2\n",
      "RLoss: 1044.7596435546875\n",
      "current in epoch    790      batch 3\n",
      "RLoss: 7.751471519470215\n",
      "current in epoch    790      batch 4\n",
      "RLoss: 41.394561767578125\n",
      "current in epoch    790      batch 5\n",
      "RLoss: 53.74919128417969\n",
      "========================================\n",
      "Epoch 791/1000 - partial_train_loss: 176.4534 \n",
      "sorting training set\n",
      "Epoch 791/1000 - Training loss: 43.4569 \n",
      "========================================\n",
      "Epoch: [791/1000], TrainLoss: 46.66600099150596\n",
      "training Loss has not improved for 144 epochs.\n",
      "current in epoch    791      batch 0\n",
      "RLoss: 10.994837760925293\n",
      "current in epoch    791      batch 1\n",
      "RLoss: 65.02386474609375\n",
      "current in epoch    791      batch 2\n",
      "RLoss: 33.84284210205078\n",
      "current in epoch    791      batch 3\n",
      "RLoss: 68.45233917236328\n",
      "current in epoch    791      batch 4\n",
      "RLoss: 1.7560688257217407\n",
      "current in epoch    791      batch 5\n",
      "RLoss: 6.998487949371338\n",
      "========================================\n",
      "Epoch 792/1000 - partial_train_loss: 73.2935 \n",
      "Epoch: [792/1000], TrainLoss: 5.996199233191354\n",
      "training Loss has not improved for 145 epochs.\n",
      "current in epoch    792      batch 0\n",
      "RLoss: 15.957528114318848\n",
      "current in epoch    792      batch 1\n",
      "RLoss: 49.64748001098633\n",
      "current in epoch    792      batch 2\n",
      "RLoss: 9.559316635131836\n",
      "current in epoch    792      batch 3\n",
      "RLoss: 24.994787216186523\n",
      "current in epoch    792      batch 4\n",
      "RLoss: 44.49148941040039\n",
      "current in epoch    792      batch 5\n",
      "RLoss: 24.66301155090332\n",
      "========================================\n",
      "Epoch 793/1000 - partial_train_loss: 26.4660 \n",
      "Epoch: [793/1000], TrainLoss: 43.990292412894114\n",
      "training Loss has not improved for 146 epochs.\n",
      "current in epoch    793      batch 0\n",
      "RLoss: 24.348724365234375\n",
      "current in epoch    793      batch 1\n",
      "RLoss: 28.23921775817871\n",
      "current in epoch    793      batch 2\n",
      "RLoss: 20.14102554321289\n",
      "current in epoch    793      batch 3\n",
      "RLoss: 83.1837387084961\n",
      "current in epoch    793      batch 4\n",
      "RLoss: 32.59978485107422\n",
      "current in epoch    793      batch 5\n",
      "RLoss: 107.75069427490234\n",
      "========================================\n",
      "Epoch 794/1000 - partial_train_loss: 34.0959 \n",
      "Epoch: [794/1000], TrainLoss: 99.86172049386161\n",
      "training Loss has not improved for 147 epochs.\n",
      "current in epoch    794      batch 0\n",
      "RLoss: 146.49913024902344\n",
      "current in epoch    794      batch 1\n",
      "RLoss: 73.39859008789062\n",
      "current in epoch    794      batch 2\n",
      "RLoss: 61.974891662597656\n",
      "current in epoch    794      batch 3\n",
      "RLoss: 31.058168411254883\n",
      "current in epoch    794      batch 4\n",
      "RLoss: 57.24980163574219\n",
      "current in epoch    794      batch 5\n",
      "RLoss: 63.783817291259766\n",
      "========================================\n",
      "Epoch 795/1000 - partial_train_loss: 83.9299 \n",
      "Epoch: [795/1000], TrainLoss: 51.5867486681257\n",
      "training Loss has not improved for 148 epochs.\n",
      "current in epoch    795      batch 0\n",
      "RLoss: 43.381622314453125\n",
      "current in epoch    795      batch 1\n",
      "RLoss: 60.57353210449219\n",
      "current in epoch    795      batch 2\n",
      "RLoss: 6.011407852172852\n",
      "current in epoch    795      batch 3\n",
      "RLoss: 8.949197769165039\n",
      "current in epoch    795      batch 4\n",
      "RLoss: 10.0968599319458\n",
      "current in epoch    795      batch 5\n",
      "RLoss: 20.08701515197754\n",
      "========================================\n",
      "Epoch 796/1000 - partial_train_loss: 33.9095 \n",
      "sorting training set\n",
      "Epoch 796/1000 - Training loss: 18.0686 \n",
      "========================================\n",
      "Epoch: [796/1000], TrainLoss: 19.67273006270305\n",
      "training Loss has not improved for 149 epochs.\n",
      "current in epoch    796      batch 0\n",
      "RLoss: 110.75169372558594\n",
      "current in epoch    796      batch 1\n",
      "RLoss: 14.690197944641113\n",
      "current in epoch    796      batch 2\n",
      "RLoss: 108.37467193603516\n",
      "current in epoch    796      batch 3\n",
      "RLoss: 41.45077133178711\n",
      "current in epoch    796      batch 4\n",
      "RLoss: 17.797086715698242\n",
      "current in epoch    796      batch 5\n",
      "RLoss: 100.31046295166016\n",
      "========================================\n",
      "Epoch 797/1000 - partial_train_loss: 67.1697 \n",
      "Epoch: [797/1000], TrainLoss: 69.27208055768695\n",
      "training Loss has not improved for 150 epochs.\n",
      "current in epoch    797      batch 0\n",
      "RLoss: 16.720731735229492\n",
      "current in epoch    797      batch 1\n",
      "RLoss: 80.9800796508789\n",
      "current in epoch    797      batch 2\n",
      "RLoss: 57.927696228027344\n",
      "current in epoch    797      batch 3\n",
      "RLoss: 20.504640579223633\n",
      "current in epoch    797      batch 4\n",
      "RLoss: 34.23265075683594\n",
      "current in epoch    797      batch 5\n",
      "RLoss: 28.09307289123535\n",
      "========================================\n",
      "Epoch 798/1000 - partial_train_loss: 65.8444 \n",
      "Epoch: [798/1000], TrainLoss: 23.269934926714217\n",
      "training Loss has not improved for 151 epochs.\n",
      "current in epoch    798      batch 0\n",
      "RLoss: 127.2834243774414\n",
      "current in epoch    798      batch 1\n",
      "RLoss: 43.83509063720703\n",
      "current in epoch    798      batch 2\n",
      "RLoss: 19.357418060302734\n",
      "current in epoch    798      batch 3\n",
      "RLoss: 219.24261474609375\n",
      "current in epoch    798      batch 4\n",
      "RLoss: 12.41641616821289\n",
      "current in epoch    798      batch 5\n",
      "RLoss: 13.259164810180664\n",
      "========================================\n",
      "Epoch 799/1000 - partial_train_loss: 73.9833 \n",
      "Epoch: [799/1000], TrainLoss: 13.99179550579616\n",
      "training Loss has not improved for 152 epochs.\n",
      "current in epoch    799      batch 0\n",
      "RLoss: 456.9931640625\n",
      "current in epoch    799      batch 1\n",
      "RLoss: 4.852099418640137\n",
      "current in epoch    799      batch 2\n",
      "RLoss: 20.214834213256836\n",
      "current in epoch    799      batch 3\n",
      "RLoss: 99.40631103515625\n",
      "current in epoch    799      batch 4\n",
      "RLoss: 33.97074890136719\n",
      "current in epoch    799      batch 5\n",
      "RLoss: 36.648773193359375\n",
      "========================================\n",
      "Epoch 800/1000 - partial_train_loss: 82.2228 \n",
      "Epoch: [800/1000], TrainLoss: 44.83172273635864\n",
      "training Loss has not improved for 153 epochs.\n",
      "current in epoch    800      batch 0\n",
      "RLoss: 152.53672790527344\n",
      "current in epoch    800      batch 1\n",
      "RLoss: 946.8423461914062\n",
      "current in epoch    800      batch 2\n",
      "RLoss: 148.2010498046875\n",
      "current in epoch    800      batch 3\n",
      "RLoss: 56.124725341796875\n",
      "current in epoch    800      batch 4\n",
      "RLoss: 66.19146728515625\n",
      "current in epoch    800      batch 5\n",
      "RLoss: 34.38742446899414\n",
      "========================================\n",
      "Epoch 801/1000 - partial_train_loss: 203.4494 \n",
      "sorting training set\n",
      "Epoch 801/1000 - Training loss: 35.4178 \n",
      "========================================\n",
      "Epoch: [801/1000], TrainLoss: 36.63248353362534\n",
      "training Loss has not improved for 154 epochs.\n",
      "current in epoch    801      batch 0\n",
      "RLoss: 347.6453857421875\n",
      "current in epoch    801      batch 1\n",
      "RLoss: 64.75636291503906\n",
      "current in epoch    801      batch 2\n",
      "RLoss: 29.613555908203125\n",
      "current in epoch    801      batch 3\n",
      "RLoss: 20.571142196655273\n",
      "current in epoch    801      batch 4\n",
      "RLoss: 40.61924362182617\n",
      "current in epoch    801      batch 5\n",
      "RLoss: 34.730770111083984\n",
      "========================================\n",
      "Epoch 802/1000 - partial_train_loss: 77.7486 \n",
      "Epoch: [802/1000], TrainLoss: 23.013060450553894\n",
      "training Loss has not improved for 155 epochs.\n",
      "current in epoch    802      batch 0\n",
      "RLoss: 162.31484985351562\n",
      "current in epoch    802      batch 1\n",
      "RLoss: 26.093914031982422\n",
      "current in epoch    802      batch 2\n",
      "RLoss: 75.75209045410156\n",
      "current in epoch    802      batch 3\n",
      "RLoss: 17.59086036682129\n",
      "current in epoch    802      batch 4\n",
      "RLoss: 33.92074966430664\n",
      "current in epoch    802      batch 5\n",
      "RLoss: 50.26103210449219\n",
      "========================================\n",
      "Epoch 803/1000 - partial_train_loss: 72.5528 \n",
      "Epoch: [803/1000], TrainLoss: 45.05557543890817\n",
      "training Loss has not improved for 156 epochs.\n",
      "current in epoch    803      batch 0\n",
      "RLoss: 20.545333862304688\n",
      "current in epoch    803      batch 1\n",
      "RLoss: 31.302825927734375\n",
      "current in epoch    803      batch 2\n",
      "RLoss: 300.07318115234375\n",
      "current in epoch    803      batch 3\n",
      "RLoss: 139.85240173339844\n",
      "current in epoch    803      batch 4\n",
      "RLoss: 49.316890716552734\n",
      "current in epoch    803      batch 5\n",
      "RLoss: 103.76444244384766\n",
      "========================================\n",
      "Epoch 804/1000 - partial_train_loss: 89.7623 \n",
      "Epoch: [804/1000], TrainLoss: 192.33781678336007\n",
      "training Loss has not improved for 157 epochs.\n",
      "current in epoch    804      batch 0\n",
      "RLoss: 104.55915069580078\n",
      "current in epoch    804      batch 1\n",
      "RLoss: 33.10689163208008\n",
      "current in epoch    804      batch 2\n",
      "RLoss: 10.023295402526855\n",
      "current in epoch    804      batch 3\n",
      "RLoss: 6.580212593078613\n",
      "current in epoch    804      batch 4\n",
      "RLoss: 11.16770076751709\n",
      "current in epoch    804      batch 5\n",
      "RLoss: 77.7120132446289\n",
      "========================================\n",
      "Epoch 805/1000 - partial_train_loss: 65.6500 \n",
      "Epoch: [805/1000], TrainLoss: 87.30743435450962\n",
      "training Loss has not improved for 158 epochs.\n",
      "current in epoch    805      batch 0\n",
      "RLoss: 441.877197265625\n",
      "current in epoch    805      batch 1\n",
      "RLoss: 81.48931121826172\n",
      "current in epoch    805      batch 2\n",
      "RLoss: 4.787610054016113\n",
      "current in epoch    805      batch 3\n",
      "RLoss: 14.472742080688477\n",
      "current in epoch    805      batch 4\n",
      "RLoss: 112.9375228881836\n",
      "current in epoch    805      batch 5\n",
      "RLoss: 37.851905822753906\n",
      "========================================\n",
      "Epoch 806/1000 - partial_train_loss: 99.9945 \n",
      "sorting training set\n",
      "Epoch 806/1000 - Training loss: 34.9507 \n",
      "========================================\n",
      "Epoch: [806/1000], TrainLoss: 37.19427573366029\n",
      "training Loss has not improved for 159 epochs.\n",
      "current in epoch    806      batch 0\n",
      "RLoss: 357.6155700683594\n",
      "current in epoch    806      batch 1\n",
      "RLoss: 52.56890106201172\n",
      "current in epoch    806      batch 2\n",
      "RLoss: 47.514583587646484\n",
      "current in epoch    806      batch 3\n",
      "RLoss: 50.07893371582031\n",
      "current in epoch    806      batch 4\n",
      "RLoss: 20.141523361206055\n",
      "current in epoch    806      batch 5\n",
      "RLoss: 69.2001953125\n",
      "========================================\n",
      "Epoch 807/1000 - partial_train_loss: 97.4265 \n",
      "Epoch: [807/1000], TrainLoss: 83.83547810145787\n",
      "training Loss has not improved for 160 epochs.\n",
      "current in epoch    807      batch 0\n",
      "RLoss: 94.51372528076172\n",
      "current in epoch    807      batch 1\n",
      "RLoss: 11.892451286315918\n",
      "current in epoch    807      batch 2\n",
      "RLoss: 6.070527076721191\n",
      "current in epoch    807      batch 3\n",
      "RLoss: 48.74287796020508\n",
      "current in epoch    807      batch 4\n",
      "RLoss: 103.29703521728516\n",
      "current in epoch    807      batch 5\n",
      "RLoss: 172.40957641601562\n",
      "========================================\n",
      "Epoch 808/1000 - partial_train_loss: 62.5280 \n",
      "Epoch: [808/1000], TrainLoss: 175.4459457397461\n",
      "training Loss has not improved for 161 epochs.\n",
      "current in epoch    808      batch 0\n",
      "RLoss: 396.65802001953125\n",
      "current in epoch    808      batch 1\n",
      "RLoss: 116.82494354248047\n",
      "current in epoch    808      batch 2\n",
      "RLoss: 254.4273681640625\n",
      "current in epoch    808      batch 3\n",
      "RLoss: 18.309650421142578\n",
      "current in epoch    808      batch 4\n",
      "RLoss: 63.24063491821289\n",
      "current in epoch    808      batch 5\n",
      "RLoss: 216.87319946289062\n",
      "========================================\n",
      "Epoch 809/1000 - partial_train_loss: 155.6473 \n",
      "Epoch: [809/1000], TrainLoss: 290.86298697335377\n",
      "training Loss has not improved for 162 epochs.\n",
      "current in epoch    809      batch 0\n",
      "RLoss: 88.0453872680664\n",
      "current in epoch    809      batch 1\n",
      "RLoss: 116.4952163696289\n",
      "current in epoch    809      batch 2\n",
      "RLoss: 22.195640563964844\n",
      "current in epoch    809      batch 3\n",
      "RLoss: 4.358428955078125\n",
      "current in epoch    809      batch 4\n",
      "RLoss: 11.514652252197266\n",
      "current in epoch    809      batch 5\n",
      "RLoss: 11.326822280883789\n",
      "========================================\n",
      "Epoch 810/1000 - partial_train_loss: 68.0878 \n",
      "Epoch: [810/1000], TrainLoss: 6.815674415656498\n",
      "training Loss has not improved for 163 epochs.\n",
      "current in epoch    810      batch 0\n",
      "RLoss: 27.46811294555664\n",
      "current in epoch    810      batch 1\n",
      "RLoss: 256.0818176269531\n",
      "current in epoch    810      batch 2\n",
      "RLoss: 42.74681854248047\n",
      "current in epoch    810      batch 3\n",
      "RLoss: 28.727113723754883\n",
      "current in epoch    810      batch 4\n",
      "RLoss: 14.265769004821777\n",
      "current in epoch    810      batch 5\n",
      "RLoss: 32.129127502441406\n",
      "========================================\n",
      "Epoch 811/1000 - partial_train_loss: 52.2007 \n",
      "sorting training set\n",
      "Epoch 811/1000 - Training loss: 22.0627 \n",
      "========================================\n",
      "Epoch: [811/1000], TrainLoss: 23.6975314560338\n",
      "training Loss has not improved for 164 epochs.\n",
      "current in epoch    811      batch 0\n",
      "RLoss: 51.423683166503906\n",
      "current in epoch    811      batch 1\n",
      "RLoss: 1.9657273292541504\n",
      "current in epoch    811      batch 2\n",
      "RLoss: 129.163330078125\n",
      "current in epoch    811      batch 3\n",
      "RLoss: 66.29203796386719\n",
      "current in epoch    811      batch 4\n",
      "RLoss: 254.07302856445312\n",
      "current in epoch    811      batch 5\n",
      "RLoss: 14.796883583068848\n",
      "========================================\n",
      "Epoch 812/1000 - partial_train_loss: 135.0454 \n",
      "Epoch: [812/1000], TrainLoss: 12.190114770616804\n",
      "training Loss has not improved for 165 epochs.\n",
      "current in epoch    812      batch 0\n",
      "RLoss: 26.207534790039062\n",
      "current in epoch    812      batch 1\n",
      "RLoss: 5.023531913757324\n",
      "current in epoch    812      batch 2\n",
      "RLoss: 98.79380798339844\n",
      "current in epoch    812      batch 3\n",
      "RLoss: 51.91636276245117\n",
      "current in epoch    812      batch 4\n",
      "RLoss: 57.82438659667969\n",
      "current in epoch    812      batch 5\n",
      "RLoss: 131.08157348632812\n",
      "========================================\n",
      "Epoch 813/1000 - partial_train_loss: 53.9372 \n",
      "Epoch: [813/1000], TrainLoss: 105.62270055498395\n",
      "training Loss has not improved for 166 epochs.\n",
      "current in epoch    813      batch 0\n",
      "RLoss: 6.962067604064941\n",
      "current in epoch    813      batch 1\n",
      "RLoss: 15.276778221130371\n",
      "current in epoch    813      batch 2\n",
      "RLoss: 49.87112808227539\n",
      "current in epoch    813      batch 3\n",
      "RLoss: 21.521526336669922\n",
      "current in epoch    813      batch 4\n",
      "RLoss: 66.26626586914062\n",
      "current in epoch    813      batch 5\n",
      "RLoss: 315.32965087890625\n",
      "========================================\n",
      "Epoch 814/1000 - partial_train_loss: 95.8029 \n",
      "Epoch: [814/1000], TrainLoss: 267.0289753505162\n",
      "training Loss has not improved for 167 epochs.\n",
      "current in epoch    814      batch 0\n",
      "RLoss: 18.365140914916992\n",
      "current in epoch    814      batch 1\n",
      "RLoss: 47.419620513916016\n",
      "current in epoch    814      batch 2\n",
      "RLoss: 372.0508117675781\n",
      "current in epoch    814      batch 3\n",
      "RLoss: 98.51557159423828\n",
      "current in epoch    814      batch 4\n",
      "RLoss: 106.2238540649414\n",
      "current in epoch    814      batch 5\n",
      "RLoss: 60.1756591796875\n",
      "========================================\n",
      "Epoch 815/1000 - partial_train_loss: 192.9858 \n",
      "Epoch: [815/1000], TrainLoss: 38.189807823726106\n",
      "training Loss has not improved for 168 epochs.\n",
      "current in epoch    815      batch 0\n",
      "RLoss: 17.488943099975586\n",
      "current in epoch    815      batch 1\n",
      "RLoss: 105.5624771118164\n",
      "current in epoch    815      batch 2\n",
      "RLoss: 125.07109069824219\n",
      "current in epoch    815      batch 3\n",
      "RLoss: 186.2641143798828\n",
      "current in epoch    815      batch 4\n",
      "RLoss: 273.5116882324219\n",
      "current in epoch    815      batch 5\n",
      "RLoss: 1071.9527587890625\n",
      "========================================\n",
      "Epoch 816/1000 - partial_train_loss: 125.8924 \n",
      "sorting training set\n",
      "Epoch 816/1000 - Training loss: 500.0504 \n",
      "========================================\n",
      "Epoch: [816/1000], TrainLoss: 534.0568347374752\n",
      "training Loss has not improved for 169 epochs.\n",
      "current in epoch    816      batch 0\n",
      "RLoss: 684.4685668945312\n",
      "current in epoch    816      batch 1\n",
      "RLoss: 59.59333419799805\n",
      "current in epoch    816      batch 2\n",
      "RLoss: 38.229339599609375\n",
      "current in epoch    816      batch 3\n",
      "RLoss: 40.30333709716797\n",
      "current in epoch    816      batch 4\n",
      "RLoss: 60.153663635253906\n",
      "current in epoch    816      batch 5\n",
      "RLoss: 26.53192901611328\n",
      "========================================\n",
      "Epoch 817/1000 - partial_train_loss: 627.1266 \n",
      "Epoch: [817/1000], TrainLoss: 78.34358535494123\n",
      "training Loss has not improved for 170 epochs.\n",
      "current in epoch    817      batch 0\n",
      "RLoss: 18.99563980102539\n",
      "current in epoch    817      batch 1\n",
      "RLoss: 223.3952178955078\n",
      "current in epoch    817      batch 2\n",
      "RLoss: 9.61169147491455\n",
      "current in epoch    817      batch 3\n",
      "RLoss: 30.847900390625\n",
      "current in epoch    817      batch 4\n",
      "RLoss: 22.231794357299805\n",
      "current in epoch    817      batch 5\n",
      "RLoss: 8.988569259643555\n",
      "========================================\n",
      "Epoch 818/1000 - partial_train_loss: 62.5360 \n",
      "Epoch: [818/1000], TrainLoss: 7.303141985620771\n",
      "training Loss has not improved for 171 epochs.\n",
      "current in epoch    818      batch 0\n",
      "RLoss: 99.38775634765625\n",
      "current in epoch    818      batch 1\n",
      "RLoss: 12.847208976745605\n",
      "current in epoch    818      batch 2\n",
      "RLoss: 356.17822265625\n",
      "current in epoch    818      batch 3\n",
      "RLoss: 27.73745346069336\n",
      "current in epoch    818      batch 4\n",
      "RLoss: 27.61315155029297\n",
      "current in epoch    818      batch 5\n",
      "RLoss: 63.74215316772461\n",
      "========================================\n",
      "Epoch 819/1000 - partial_train_loss: 71.6727 \n",
      "Epoch: [819/1000], TrainLoss: 42.34890638078962\n",
      "training Loss has not improved for 172 epochs.\n",
      "current in epoch    819      batch 0\n",
      "RLoss: 23.233356475830078\n",
      "current in epoch    819      batch 1\n",
      "RLoss: 1.3486825227737427\n",
      "current in epoch    819      batch 2\n",
      "RLoss: 400.3704528808594\n",
      "current in epoch    819      batch 3\n",
      "RLoss: 47.41777420043945\n",
      "current in epoch    819      batch 4\n",
      "RLoss: 491.6033935546875\n",
      "current in epoch    819      batch 5\n",
      "RLoss: 53.40449142456055\n",
      "========================================\n",
      "Epoch 820/1000 - partial_train_loss: 152.6809 \n",
      "Epoch: [820/1000], TrainLoss: 37.768483366285054\n",
      "training Loss has not improved for 173 epochs.\n",
      "current in epoch    820      batch 0\n",
      "RLoss: 53.11991500854492\n",
      "current in epoch    820      batch 1\n",
      "RLoss: 9.602073669433594\n",
      "current in epoch    820      batch 2\n",
      "RLoss: 298.6609802246094\n",
      "current in epoch    820      batch 3\n",
      "RLoss: 102.3441390991211\n",
      "current in epoch    820      batch 4\n",
      "RLoss: 30.746633529663086\n",
      "current in epoch    820      batch 5\n",
      "RLoss: 147.58680725097656\n",
      "========================================\n",
      "Epoch 821/1000 - partial_train_loss: 89.5950 \n",
      "sorting training set\n",
      "Epoch 821/1000 - Training loss: 124.4237 \n",
      "========================================\n",
      "Epoch: [821/1000], TrainLoss: 132.94464620608525\n",
      "training Loss has not improved for 174 epochs.\n",
      "current in epoch    821      batch 0\n",
      "RLoss: 59.98553466796875\n",
      "current in epoch    821      batch 1\n",
      "RLoss: 44.184810638427734\n",
      "current in epoch    821      batch 2\n",
      "RLoss: 85.07549285888672\n",
      "current in epoch    821      batch 3\n",
      "RLoss: 17.85441780090332\n",
      "current in epoch    821      batch 4\n",
      "RLoss: 139.65655517578125\n",
      "current in epoch    821      batch 5\n",
      "RLoss: 51.86199188232422\n",
      "========================================\n",
      "Epoch 822/1000 - partial_train_loss: 127.5057 \n",
      "Epoch: [822/1000], TrainLoss: 74.50934389659336\n",
      "training Loss has not improved for 175 epochs.\n",
      "current in epoch    822      batch 0\n",
      "RLoss: 14.30368709564209\n",
      "current in epoch    822      batch 1\n",
      "RLoss: 13.603292465209961\n",
      "current in epoch    822      batch 2\n",
      "RLoss: 86.1444091796875\n",
      "current in epoch    822      batch 3\n",
      "RLoss: 10.856230735778809\n",
      "current in epoch    822      batch 4\n",
      "RLoss: 10.917436599731445\n",
      "current in epoch    822      batch 5\n",
      "RLoss: 3.701030969619751\n",
      "========================================\n",
      "Epoch 823/1000 - partial_train_loss: 21.8024 \n",
      "Epoch: [823/1000], TrainLoss: 6.242789242948804\n",
      "training Loss has not improved for 176 epochs.\n",
      "current in epoch    823      batch 0\n",
      "RLoss: 2.3638174533843994\n",
      "current in epoch    823      batch 1\n",
      "RLoss: 9.18539047241211\n",
      "current in epoch    823      batch 2\n",
      "RLoss: 70.06888580322266\n",
      "current in epoch    823      batch 3\n",
      "RLoss: 101.72318267822266\n",
      "current in epoch    823      batch 4\n",
      "RLoss: 172.49703979492188\n",
      "current in epoch    823      batch 5\n",
      "RLoss: 149.67608642578125\n",
      "========================================\n",
      "Epoch 824/1000 - partial_train_loss: 54.4697 \n",
      "Epoch: [824/1000], TrainLoss: 120.57081406457084\n",
      "training Loss has not improved for 177 epochs.\n",
      "current in epoch    824      batch 0\n",
      "RLoss: 88.87720489501953\n",
      "current in epoch    824      batch 1\n",
      "RLoss: 405.2265625\n",
      "current in epoch    824      batch 2\n",
      "RLoss: 197.36439514160156\n",
      "current in epoch    824      batch 3\n",
      "RLoss: 378.7923278808594\n",
      "current in epoch    824      batch 4\n",
      "RLoss: 181.1310577392578\n",
      "current in epoch    824      batch 5\n",
      "RLoss: 29.90846824645996\n",
      "========================================\n",
      "Epoch 825/1000 - partial_train_loss: 334.8687 \n",
      "Epoch: [825/1000], TrainLoss: 25.232242277690343\n",
      "training Loss has not improved for 178 epochs.\n",
      "current in epoch    825      batch 0\n",
      "RLoss: 25.21212387084961\n",
      "current in epoch    825      batch 1\n",
      "RLoss: 4.606293678283691\n",
      "current in epoch    825      batch 2\n",
      "RLoss: 338.1171875\n",
      "current in epoch    825      batch 3\n",
      "RLoss: 6.683128833770752\n",
      "current in epoch    825      batch 4\n",
      "RLoss: 22.96967124938965\n",
      "current in epoch    825      batch 5\n",
      "RLoss: 28.992050170898438\n",
      "========================================\n",
      "Epoch 826/1000 - partial_train_loss: 69.9394 \n",
      "sorting training set\n",
      "Epoch 826/1000 - Training loss: 33.7645 \n",
      "========================================\n",
      "Epoch: [826/1000], TrainLoss: 37.00381350419392\n",
      "training Loss has not improved for 179 epochs.\n",
      "current in epoch    826      batch 0\n",
      "RLoss: 53.54945373535156\n",
      "current in epoch    826      batch 1\n",
      "RLoss: 131.79490661621094\n",
      "current in epoch    826      batch 2\n",
      "RLoss: 193.4113311767578\n",
      "current in epoch    826      batch 3\n",
      "RLoss: 9.940230369567871\n",
      "current in epoch    826      batch 4\n",
      "RLoss: 198.1439971923828\n",
      "current in epoch    826      batch 5\n",
      "RLoss: 102.12613677978516\n",
      "========================================\n",
      "Epoch 827/1000 - partial_train_loss: 141.3688 \n",
      "Epoch: [827/1000], TrainLoss: 87.25275829860142\n",
      "training Loss has not improved for 180 epochs.\n",
      "current in epoch    827      batch 0\n",
      "RLoss: 142.90672302246094\n",
      "current in epoch    827      batch 1\n",
      "RLoss: 42.30719757080078\n",
      "current in epoch    827      batch 2\n",
      "RLoss: 28.50162696838379\n",
      "current in epoch    827      batch 3\n",
      "RLoss: 11.378814697265625\n",
      "current in epoch    827      batch 4\n",
      "RLoss: 129.37533569335938\n",
      "current in epoch    827      batch 5\n",
      "RLoss: 243.20358276367188\n",
      "========================================\n",
      "Epoch 828/1000 - partial_train_loss: 76.6386 \n",
      "Epoch: [828/1000], TrainLoss: 233.02017266409737\n",
      "training Loss has not improved for 181 epochs.\n",
      "current in epoch    828      batch 0\n",
      "RLoss: 190.37374877929688\n",
      "current in epoch    828      batch 1\n",
      "RLoss: 74.77276611328125\n",
      "current in epoch    828      batch 2\n",
      "RLoss: 126.51607513427734\n",
      "current in epoch    828      batch 3\n",
      "RLoss: 21.009965896606445\n",
      "current in epoch    828      batch 4\n",
      "RLoss: 14.82685375213623\n",
      "current in epoch    828      batch 5\n",
      "RLoss: 12.050061225891113\n",
      "========================================\n",
      "Epoch 829/1000 - partial_train_loss: 155.1641 \n",
      "Epoch: [829/1000], TrainLoss: 12.683978693825859\n",
      "training Loss has not improved for 182 epochs.\n",
      "current in epoch    829      batch 0\n",
      "RLoss: 106.8206787109375\n",
      "current in epoch    829      batch 1\n",
      "RLoss: 30.780874252319336\n",
      "current in epoch    829      batch 2\n",
      "RLoss: 184.10800170898438\n",
      "current in epoch    829      batch 3\n",
      "RLoss: 53.490821838378906\n",
      "current in epoch    829      batch 4\n",
      "RLoss: 36.4559440612793\n",
      "current in epoch    829      batch 5\n",
      "RLoss: 196.40765380859375\n",
      "========================================\n",
      "Epoch 830/1000 - partial_train_loss: 67.9047 \n",
      "Epoch: [830/1000], TrainLoss: 208.05867494855607\n",
      "training Loss has not improved for 183 epochs.\n",
      "current in epoch    830      batch 0\n",
      "RLoss: 91.49343872070312\n",
      "current in epoch    830      batch 1\n",
      "RLoss: 34.802101135253906\n",
      "current in epoch    830      batch 2\n",
      "RLoss: 19.610612869262695\n",
      "current in epoch    830      batch 3\n",
      "RLoss: 44.85390853881836\n",
      "current in epoch    830      batch 4\n",
      "RLoss: 34.53224182128906\n",
      "current in epoch    830      batch 5\n",
      "RLoss: 59.93707275390625\n",
      "========================================\n",
      "Epoch 831/1000 - partial_train_loss: 114.7086 \n",
      "sorting training set\n",
      "Epoch 831/1000 - Training loss: 51.8970 \n",
      "========================================\n",
      "Epoch: [831/1000], TrainLoss: 55.59258995280839\n",
      "training Loss has not improved for 184 epochs.\n",
      "current in epoch    831      batch 0\n",
      "RLoss: 13.547772407531738\n",
      "current in epoch    831      batch 1\n",
      "RLoss: 120.5140380859375\n",
      "current in epoch    831      batch 2\n",
      "RLoss: 1.27707040309906\n",
      "current in epoch    831      batch 3\n",
      "RLoss: 2.4371845722198486\n",
      "current in epoch    831      batch 4\n",
      "RLoss: 8.579872131347656\n",
      "current in epoch    831      batch 5\n",
      "RLoss: 29.817543029785156\n",
      "========================================\n",
      "Epoch 832/1000 - partial_train_loss: 52.8542 \n",
      "Epoch: [832/1000], TrainLoss: 27.66470159803118\n",
      "training Loss has not improved for 185 epochs.\n",
      "current in epoch    832      batch 0\n",
      "RLoss: 229.931884765625\n",
      "current in epoch    832      batch 1\n",
      "RLoss: 125.25962829589844\n",
      "current in epoch    832      batch 2\n",
      "RLoss: 0.931308925151825\n",
      "current in epoch    832      batch 3\n",
      "RLoss: 16.741479873657227\n",
      "current in epoch    832      batch 4\n",
      "RLoss: 62.58544158935547\n",
      "current in epoch    832      batch 5\n",
      "RLoss: 76.0973892211914\n",
      "========================================\n",
      "Epoch 833/1000 - partial_train_loss: 81.7632 \n",
      "Epoch: [833/1000], TrainLoss: 240.17527825491769\n",
      "training Loss has not improved for 186 epochs.\n",
      "current in epoch    833      batch 0\n",
      "RLoss: 10.927593231201172\n",
      "current in epoch    833      batch 1\n",
      "RLoss: 610.7362060546875\n",
      "current in epoch    833      batch 2\n",
      "RLoss: 78.28715515136719\n",
      "current in epoch    833      batch 3\n",
      "RLoss: 329.17333984375\n",
      "current in epoch    833      batch 4\n",
      "RLoss: 28.716276168823242\n",
      "current in epoch    833      batch 5\n",
      "RLoss: 4.669310092926025\n",
      "========================================\n",
      "Epoch 834/1000 - partial_train_loss: 175.7249 \n",
      "Epoch: [834/1000], TrainLoss: 12.356284167085375\n",
      "training Loss has not improved for 187 epochs.\n",
      "current in epoch    834      batch 0\n",
      "RLoss: 250.3429412841797\n",
      "current in epoch    834      batch 1\n",
      "RLoss: 163.09483337402344\n",
      "current in epoch    834      batch 2\n",
      "RLoss: 62.106414794921875\n",
      "current in epoch    834      batch 3\n",
      "RLoss: 5.939084529876709\n",
      "current in epoch    834      batch 4\n",
      "RLoss: 97.28437805175781\n",
      "current in epoch    834      batch 5\n",
      "RLoss: 40.24740219116211\n",
      "========================================\n",
      "Epoch 835/1000 - partial_train_loss: 99.1709 \n",
      "Epoch: [835/1000], TrainLoss: 165.57467327799117\n",
      "training Loss has not improved for 188 epochs.\n",
      "current in epoch    835      batch 0\n",
      "RLoss: 73.01506805419922\n",
      "current in epoch    835      batch 1\n",
      "RLoss: 69.39509582519531\n",
      "current in epoch    835      batch 2\n",
      "RLoss: 354.7806396484375\n",
      "current in epoch    835      batch 3\n",
      "RLoss: 35.25591278076172\n",
      "current in epoch    835      batch 4\n",
      "RLoss: 1197.6822509765625\n",
      "current in epoch    835      batch 5\n",
      "RLoss: 86.1634750366211\n",
      "========================================\n",
      "Epoch 836/1000 - partial_train_loss: 281.9829 \n",
      "sorting training set\n",
      "Epoch 836/1000 - Training loss: 272.4704 \n",
      "========================================\n",
      "Epoch: [836/1000], TrainLoss: 290.7862094404359\n",
      "training Loss has not improved for 189 epochs.\n",
      "current in epoch    836      batch 0\n",
      "RLoss: 148.7514190673828\n",
      "current in epoch    836      batch 1\n",
      "RLoss: 28.823427200317383\n",
      "current in epoch    836      batch 2\n",
      "RLoss: 22.265811920166016\n",
      "current in epoch    836      batch 3\n",
      "RLoss: 18.607921600341797\n",
      "current in epoch    836      batch 4\n",
      "RLoss: 16.14170265197754\n",
      "current in epoch    836      batch 5\n",
      "RLoss: 52.393714904785156\n",
      "========================================\n",
      "Epoch 837/1000 - partial_train_loss: 196.3070 \n",
      "Epoch: [837/1000], TrainLoss: 66.94206721442086\n",
      "training Loss has not improved for 190 epochs.\n",
      "current in epoch    837      batch 0\n",
      "RLoss: 13.843709945678711\n",
      "current in epoch    837      batch 1\n",
      "RLoss: 25.877477645874023\n",
      "current in epoch    837      batch 2\n",
      "RLoss: 105.55342102050781\n",
      "current in epoch    837      batch 3\n",
      "RLoss: 26.52080726623535\n",
      "current in epoch    837      batch 4\n",
      "RLoss: 52.30958557128906\n",
      "current in epoch    837      batch 5\n",
      "RLoss: 3.716825008392334\n",
      "========================================\n",
      "Epoch 838/1000 - partial_train_loss: 46.8625 \n",
      "Epoch: [838/1000], TrainLoss: 2.348000073007175\n",
      "training Loss has not improved for 191 epochs.\n",
      "current in epoch    838      batch 0\n",
      "RLoss: 10.465388298034668\n",
      "current in epoch    838      batch 1\n",
      "RLoss: 16.921489715576172\n",
      "current in epoch    838      batch 2\n",
      "RLoss: 28.526968002319336\n",
      "current in epoch    838      batch 3\n",
      "RLoss: 113.8556900024414\n",
      "current in epoch    838      batch 4\n",
      "RLoss: 108.15401458740234\n",
      "current in epoch    838      batch 5\n",
      "RLoss: 21.59246063232422\n",
      "========================================\n",
      "Epoch 839/1000 - partial_train_loss: 51.6817 \n",
      "Epoch: [839/1000], TrainLoss: 33.67707702091762\n",
      "training Loss has not improved for 192 epochs.\n",
      "current in epoch    839      batch 0\n",
      "RLoss: 41.86737060546875\n",
      "current in epoch    839      batch 1\n",
      "RLoss: 35.74997329711914\n",
      "current in epoch    839      batch 2\n",
      "RLoss: 34.48052215576172\n",
      "current in epoch    839      batch 3\n",
      "RLoss: 6.471212387084961\n",
      "current in epoch    839      batch 4\n",
      "RLoss: 46.34093475341797\n",
      "current in epoch    839      batch 5\n",
      "RLoss: 61.90718460083008\n",
      "========================================\n",
      "Epoch 840/1000 - partial_train_loss: 30.3379 \n",
      "Epoch: [840/1000], TrainLoss: 137.96728038787842\n",
      "training Loss has not improved for 193 epochs.\n",
      "current in epoch    840      batch 0\n",
      "RLoss: 7.73676061630249\n",
      "current in epoch    840      batch 1\n",
      "RLoss: 35.62844467163086\n",
      "current in epoch    840      batch 2\n",
      "RLoss: 80.8959732055664\n",
      "current in epoch    840      batch 3\n",
      "RLoss: 27.090179443359375\n",
      "current in epoch    840      batch 4\n",
      "RLoss: 18.81060218811035\n",
      "current in epoch    840      batch 5\n",
      "RLoss: 18.796531677246094\n",
      "========================================\n",
      "Epoch 841/1000 - partial_train_loss: 55.0326 \n",
      "sorting training set\n",
      "Epoch 841/1000 - Training loss: 25.4218 \n",
      "========================================\n",
      "Epoch: [841/1000], TrainLoss: 27.329066498995058\n",
      "training Loss has not improved for 194 epochs.\n",
      "current in epoch    841      batch 0\n",
      "RLoss: 116.10093688964844\n",
      "current in epoch    841      batch 1\n",
      "RLoss: 32.77873611450195\n",
      "current in epoch    841      batch 2\n",
      "RLoss: 18.788623809814453\n",
      "current in epoch    841      batch 3\n",
      "RLoss: 365.2032775878906\n",
      "current in epoch    841      batch 4\n",
      "RLoss: 79.15167999267578\n",
      "current in epoch    841      batch 5\n",
      "RLoss: 42.33390808105469\n",
      "========================================\n",
      "Epoch 842/1000 - partial_train_loss: 108.2086 \n",
      "Epoch: [842/1000], TrainLoss: 46.395602634974885\n",
      "training Loss has not improved for 195 epochs.\n",
      "current in epoch    842      batch 0\n",
      "RLoss: 86.7693099975586\n",
      "current in epoch    842      batch 1\n",
      "RLoss: 15.979058265686035\n",
      "current in epoch    842      batch 2\n",
      "RLoss: 1.9425004720687866\n",
      "current in epoch    842      batch 3\n",
      "RLoss: 9.926135063171387\n",
      "current in epoch    842      batch 4\n",
      "RLoss: 50.100303649902344\n",
      "current in epoch    842      batch 5\n",
      "RLoss: 40.29147720336914\n",
      "========================================\n",
      "Epoch 843/1000 - partial_train_loss: 52.2722 \n",
      "Epoch: [843/1000], TrainLoss: 129.2652453013829\n",
      "training Loss has not improved for 196 epochs.\n",
      "current in epoch    843      batch 0\n",
      "RLoss: 128.20103454589844\n",
      "current in epoch    843      batch 1\n",
      "RLoss: 37.020137786865234\n",
      "current in epoch    843      batch 2\n",
      "RLoss: 9.690442085266113\n",
      "current in epoch    843      batch 3\n",
      "RLoss: 26.1583194732666\n",
      "current in epoch    843      batch 4\n",
      "RLoss: 24.043346405029297\n",
      "current in epoch    843      batch 5\n",
      "RLoss: 229.07363891601562\n",
      "========================================\n",
      "Epoch 844/1000 - partial_train_loss: 48.1238 \n",
      "Epoch: [844/1000], TrainLoss: 272.0342363630022\n",
      "training Loss has not improved for 197 epochs.\n",
      "current in epoch    844      batch 0\n",
      "RLoss: 297.08740234375\n",
      "current in epoch    844      batch 1\n",
      "RLoss: 4.554839134216309\n",
      "current in epoch    844      batch 2\n",
      "RLoss: 280.9210510253906\n",
      "current in epoch    844      batch 3\n",
      "RLoss: 290.1225280761719\n",
      "current in epoch    844      batch 4\n",
      "RLoss: 36.64398956298828\n",
      "current in epoch    844      batch 5\n",
      "RLoss: 62.38350296020508\n",
      "========================================\n",
      "Epoch 845/1000 - partial_train_loss: 196.4905 \n",
      "Epoch: [845/1000], TrainLoss: 44.37978145054409\n",
      "training Loss has not improved for 198 epochs.\n",
      "current in epoch    845      batch 0\n",
      "RLoss: 643.62744140625\n",
      "current in epoch    845      batch 1\n",
      "RLoss: 69.78411102294922\n",
      "current in epoch    845      batch 2\n",
      "RLoss: 104.30419921875\n",
      "current in epoch    845      batch 3\n",
      "RLoss: 58.39232635498047\n",
      "current in epoch    845      batch 4\n",
      "RLoss: 11.996296882629395\n",
      "current in epoch    845      batch 5\n",
      "RLoss: 29.12714195251465\n",
      "========================================\n",
      "Epoch 846/1000 - partial_train_loss: 147.6257 \n",
      "sorting training set\n",
      "Epoch 846/1000 - Training loss: 36.9018 \n",
      "========================================\n",
      "Epoch: [846/1000], TrainLoss: 39.64766672973376\n",
      "training Loss has not improved for 199 epochs.\n",
      "current in epoch    846      batch 0\n",
      "RLoss: 250.24928283691406\n",
      "current in epoch    846      batch 1\n",
      "RLoss: 281.0203552246094\n",
      "current in epoch    846      batch 2\n",
      "RLoss: 40.71355438232422\n",
      "current in epoch    846      batch 3\n",
      "RLoss: 212.22021484375\n",
      "current in epoch    846      batch 4\n",
      "RLoss: 52.13594055175781\n",
      "current in epoch    846      batch 5\n",
      "RLoss: 224.78707885742188\n",
      "========================================\n",
      "Epoch 847/1000 - partial_train_loss: 178.5464 \n",
      "Epoch: [847/1000], TrainLoss: 186.69971902029855\n",
      "training Loss has not improved for 200 epochs.\n",
      "current in epoch    847      batch 0\n",
      "RLoss: 199.11080932617188\n",
      "current in epoch    847      batch 1\n",
      "RLoss: 77.10519409179688\n",
      "current in epoch    847      batch 2\n",
      "RLoss: 39.73838806152344\n",
      "current in epoch    847      batch 3\n",
      "RLoss: 25.859033584594727\n",
      "current in epoch    847      batch 4\n",
      "RLoss: 4.999350070953369\n",
      "current in epoch    847      batch 5\n",
      "RLoss: 14.661745071411133\n",
      "========================================\n",
      "Epoch 848/1000 - partial_train_loss: 140.3276 \n",
      "Epoch: [848/1000], TrainLoss: 15.640123844146729\n",
      "training Loss has not improved for 201 epochs.\n",
      "current in epoch    848      batch 0\n",
      "RLoss: 12.876118659973145\n",
      "current in epoch    848      batch 1\n",
      "RLoss: 7.817866802215576\n",
      "current in epoch    848      batch 2\n",
      "RLoss: 9.731833457946777\n",
      "current in epoch    848      batch 3\n",
      "RLoss: 17.390270233154297\n",
      "current in epoch    848      batch 4\n",
      "RLoss: 30.116817474365234\n",
      "current in epoch    848      batch 5\n",
      "RLoss: 24.03628158569336\n",
      "========================================\n",
      "Epoch 849/1000 - partial_train_loss: 17.5865 \n",
      "Epoch: [849/1000], TrainLoss: 35.824484484536306\n",
      "training Loss has not improved for 202 epochs.\n",
      "current in epoch    849      batch 0\n",
      "RLoss: 45.11904525756836\n",
      "current in epoch    849      batch 1\n",
      "RLoss: 15.990106582641602\n",
      "current in epoch    849      batch 2\n",
      "RLoss: 5.148997783660889\n",
      "current in epoch    849      batch 3\n",
      "RLoss: 81.42733001708984\n",
      "current in epoch    849      batch 4\n",
      "RLoss: 21.100126266479492\n",
      "current in epoch    849      batch 5\n",
      "RLoss: 157.08523559570312\n",
      "========================================\n",
      "Epoch 850/1000 - partial_train_loss: 40.5490 \n",
      "Epoch: [850/1000], TrainLoss: 141.272522517613\n",
      "training Loss has not improved for 203 epochs.\n",
      "current in epoch    850      batch 0\n",
      "RLoss: 77.9593505859375\n",
      "current in epoch    850      batch 1\n",
      "RLoss: 55.35338592529297\n",
      "current in epoch    850      batch 2\n",
      "RLoss: 39.559635162353516\n",
      "current in epoch    850      batch 3\n",
      "RLoss: 97.00453186035156\n",
      "current in epoch    850      batch 4\n",
      "RLoss: 533.4633178710938\n",
      "current in epoch    850      batch 5\n",
      "RLoss: 19.0295352935791\n",
      "========================================\n",
      "Epoch 851/1000 - partial_train_loss: 148.4530 \n",
      "sorting training set\n",
      "Epoch 851/1000 - Training loss: 32.0866 \n",
      "========================================\n",
      "Epoch: [851/1000], TrainLoss: 34.45105102237538\n",
      "training Loss has not improved for 204 epochs.\n",
      "current in epoch    851      batch 0\n",
      "RLoss: 5.503802299499512\n",
      "current in epoch    851      batch 1\n",
      "RLoss: 260.78826904296875\n",
      "current in epoch    851      batch 2\n",
      "RLoss: 154.88229370117188\n",
      "current in epoch    851      batch 3\n",
      "RLoss: 185.39694213867188\n",
      "current in epoch    851      batch 4\n",
      "RLoss: 7.682018756866455\n",
      "current in epoch    851      batch 5\n",
      "RLoss: 88.48126220703125\n",
      "========================================\n",
      "Epoch 852/1000 - partial_train_loss: 124.6429 \n",
      "Epoch: [852/1000], TrainLoss: 88.36681583949498\n",
      "training Loss has not improved for 205 epochs.\n",
      "current in epoch    852      batch 0\n",
      "RLoss: 55.06985855102539\n",
      "current in epoch    852      batch 1\n",
      "RLoss: 61.0211067199707\n",
      "current in epoch    852      batch 2\n",
      "RLoss: 25.185565948486328\n",
      "current in epoch    852      batch 3\n",
      "RLoss: 143.2577362060547\n",
      "current in epoch    852      batch 4\n",
      "RLoss: 89.55366516113281\n",
      "current in epoch    852      batch 5\n",
      "RLoss: 181.4215545654297\n",
      "========================================\n",
      "Epoch 853/1000 - partial_train_loss: 79.4178 \n",
      "Epoch: [853/1000], TrainLoss: 185.11042295183455\n",
      "training Loss has not improved for 206 epochs.\n",
      "current in epoch    853      batch 0\n",
      "RLoss: 110.58865356445312\n",
      "current in epoch    853      batch 1\n",
      "RLoss: 51.09664535522461\n",
      "current in epoch    853      batch 2\n",
      "RLoss: 8.445392608642578\n",
      "current in epoch    853      batch 3\n",
      "RLoss: 21.653287887573242\n",
      "current in epoch    853      batch 4\n",
      "RLoss: 28.015941619873047\n",
      "current in epoch    853      batch 5\n",
      "RLoss: 192.59786987304688\n",
      "========================================\n",
      "Epoch 854/1000 - partial_train_loss: 54.3321 \n",
      "Epoch: [854/1000], TrainLoss: 176.86261531284876\n",
      "training Loss has not improved for 207 epochs.\n",
      "current in epoch    854      batch 0\n",
      "RLoss: 25.42573356628418\n",
      "current in epoch    854      batch 1\n",
      "RLoss: 422.53497314453125\n",
      "current in epoch    854      batch 2\n",
      "RLoss: 321.1747131347656\n",
      "current in epoch    854      batch 3\n",
      "RLoss: 6.849394798278809\n",
      "current in epoch    854      batch 4\n",
      "RLoss: 9.073868751525879\n",
      "current in epoch    854      batch 5\n",
      "RLoss: 37.33113479614258\n",
      "========================================\n",
      "Epoch 855/1000 - partial_train_loss: 168.7911 \n",
      "Epoch: [855/1000], TrainLoss: 35.65641866411482\n",
      "training Loss has not improved for 208 epochs.\n",
      "current in epoch    855      batch 0\n",
      "RLoss: 21.779972076416016\n",
      "current in epoch    855      batch 1\n",
      "RLoss: 8.852441787719727\n",
      "current in epoch    855      batch 2\n",
      "RLoss: 255.89361572265625\n",
      "current in epoch    855      batch 3\n",
      "RLoss: 11.38772201538086\n",
      "current in epoch    855      batch 4\n",
      "RLoss: 125.78945922851562\n",
      "current in epoch    855      batch 5\n",
      "RLoss: 8.33957290649414\n",
      "========================================\n",
      "Epoch 856/1000 - partial_train_loss: 90.0399 \n",
      "sorting training set\n",
      "Epoch 856/1000 - Training loss: 15.3131 \n",
      "========================================\n",
      "Epoch: [856/1000], TrainLoss: 16.279829317455793\n",
      "training Loss has not improved for 209 epochs.\n",
      "current in epoch    856      batch 0\n",
      "RLoss: 7.100225925445557\n",
      "current in epoch    856      batch 1\n",
      "RLoss: 69.69190216064453\n",
      "current in epoch    856      batch 2\n",
      "RLoss: 180.57081604003906\n",
      "current in epoch    856      batch 3\n",
      "RLoss: 13.125266075134277\n",
      "current in epoch    856      batch 4\n",
      "RLoss: 466.262939453125\n",
      "current in epoch    856      batch 5\n",
      "RLoss: 239.05386352539062\n",
      "========================================\n",
      "Epoch 857/1000 - partial_train_loss: 136.5701 \n",
      "Epoch: [857/1000], TrainLoss: 185.69696044921875\n",
      "training Loss has not improved for 210 epochs.\n",
      "current in epoch    857      batch 0\n",
      "RLoss: 86.91649627685547\n",
      "current in epoch    857      batch 1\n",
      "RLoss: 91.30918884277344\n",
      "current in epoch    857      batch 2\n",
      "RLoss: 127.75092315673828\n",
      "current in epoch    857      batch 3\n",
      "RLoss: 48.818687438964844\n",
      "current in epoch    857      batch 4\n",
      "RLoss: 321.6208190917969\n",
      "current in epoch    857      batch 5\n",
      "RLoss: 392.63397216796875\n",
      "========================================\n",
      "Epoch 858/1000 - partial_train_loss: 158.3484 \n",
      "Epoch: [858/1000], TrainLoss: 177.08063752310616\n",
      "training Loss has not improved for 211 epochs.\n",
      "current in epoch    858      batch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MmBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_attack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_flag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m rtheta \u001b[38;5;241m=\u001b[39m method\u001b[38;5;241m.\u001b[39mr([[data, target]], alpha\u001b[38;5;241m=\u001b[39malpha \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    113\u001b[0m method\u001b[38;5;241m.\u001b[39mtheta_grad \u001b[38;5;241m=\u001b[39m grad(rtheta, \u001b[38;5;28mlist\u001b[39m(method\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 206\u001b[0m, in \u001b[0;36mStableAL.train_theta\u001b[0;34m(self, data, epochs, epoch_attack, gamma, end_flag)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m end_flag:\n\u001b[0;32m--> 206\u001b[0m         images_adv, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_attack\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madv_again \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madversarial_data\n",
      "Cell \u001b[0;32mIn[1], line 187\u001b[0m, in \u001b[0;36mStableAL.attack\u001b[0;34m(self, gamma, data, step)\u001b[0m\n\u001b[1;32m    183\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_criterion(\n\u001b[1;32m    185\u001b[0m         outputs, labels) \u001b[38;5;241m-\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost_function(images, images_adv)\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     images_adv\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(images_adv\u001b[38;5;241m.\u001b[39mgrad, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, images_adv)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m*\u001b[39m attack_lr \u001b[38;5;241m*\u001b[39m (images_adv \u001b[38;5;241m-\u001b[39m images)\n",
      "File \u001b[0;32m~/anaconda3/envs/OOD/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/OOD/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'MmBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "min_weight = torch.min(method.weights)\n",
    "attack_gamma = (1.0 / min_weight).data\n",
    "criterion = nn.MSELoss()\n",
    "deltaall = 20\n",
    "alpha = 0.5\n",
    "\n",
    "epoch = 0\n",
    "num_epochs=1000\n",
    "\n",
    "zero_list = []\n",
    "epoch_theta=10\n",
    "epoch_attack=2\n",
    "\n",
    "end_flag = False\n",
    "\n",
    "method.model =IRNet_intorch(150).to(device)\n",
    "method.model.optimizer = optim.Adam(method.model.parameters(), lr=0.001)\n",
    "\n",
    "partialbatch = 5\n",
    "\n",
    "\n",
    "train_best_loss      = float('inf')\n",
    "partial_train_best_loss      = float('inf')\n",
    "Rsplt_test_best_loss = float('inf')\n",
    "Xshft_test_best_loss = float('inf')\n",
    "pizeo_test_best_loss = float('inf')\n",
    "statY_test_best_loss = float('inf')\n",
    "infoY_test_best_loss = float('inf')\n",
    "\n",
    "Rsplt_testset1_best_loss = float('inf')\n",
    "Rsplt_testset2_best_loss = float('inf')\n",
    "Rsplt_testset3_best_loss = float('inf')\n",
    "Rsplt_testset4_best_loss = float('inf')\n",
    "Rsplt_testset5_best_loss = float('inf')\n",
    "\n",
    "loss_df = pd.DataFrame(columns=[\n",
    "                        'epoch', \n",
    "                        'train',\n",
    "                        'partialtrain',\n",
    "                        'Rsplt1',\n",
    "                        'Rsplt2',\n",
    "                        'Rsplt3',\n",
    "                        'Rsplt4',\n",
    "                        'Rsplt5',\n",
    "                        'RspltAVE',\n",
    "                        'Xshft',\n",
    "                        'pizeo',\n",
    "                        'statY',\n",
    "                        'infoY',\n",
    "                        'BLANK',\n",
    "                        'best_train',\n",
    "                        'best_partialtrain',\n",
    "                        'best_Rsplt1',\n",
    "                        'best_Rsplt2',\n",
    "                        'best_Rsplt3',\n",
    "                        'best_Rsplt4',\n",
    "                        'best_Rsplt5',\n",
    "                        'best_Rsplt_AVE',\n",
    "                        'best_Xshft',\n",
    "                        'best_pizeo',\n",
    "                        'best_statY',\n",
    "                        'best_infoY',\n",
    "                        'save',\n",
    "                        'attack_gamma'\n",
    "                        ])\n",
    "\n",
    "while epoch <=num_epochs:\n",
    "    train_loss = 0.0\n",
    "\n",
    "    \n",
    "    Rsplt1_test_mse_loss = 0\n",
    "    Rsplt2_test_mse_loss = 0\n",
    "    Rsplt3_test_mse_loss = 0\n",
    "    Rsplt4_test_mse_loss = 0\n",
    "    Rsplt5_test_mse_loss = 0\n",
    "\n",
    "    Rsplt_test_mse_loss = 0\n",
    "    Xshft_test_mse_loss = 0\n",
    "    pizeo_test_mse_loss = 0\n",
    "    statY_test_mse_loss = 0\n",
    "    infoY_test_mse_loss = 0\n",
    "\n",
    "    partial_train_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    minima = []\n",
    "    optimizer = optim.Adam(method.model.parameters(), lr=0.001)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(f\"current in epoch    {epoch}      batch {batch_idx}\")\n",
    "        \n",
    "         # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs.to(device) , target.float().to(device))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        partial_train_loss += loss.cpu().item() \n",
    "        if epoch <5:\n",
    "            continue\n",
    "        \n",
    "        method.train_theta((data, target), epoch_theta, epoch_attack, attack_gamma, end_flag)\n",
    "        rtheta = method.r([[data, target]], alpha=alpha / math.sqrt(epoch + 1))\n",
    "        method.theta_grad = grad(rtheta, list(method.model.parameters()), create_graph=True, allow_unused=True)\n",
    "        dr_dx = torch.matmul(method.theta_grad[grad_layer].reshape(-1), method.xa_grad).squeeze()\n",
    "        deltaw = dr_dx * method.weight_grad\n",
    "        deltaw = torch.sum(deltaw, 0)\n",
    "        \n",
    "        deltaw[zero_list] = 0.0\n",
    "        max_grad = torch.max(torch.abs(deltaw))\n",
    "        deltastep = deltaall\n",
    "        lr_weight = (deltastep / max_grad).detach()\n",
    "        print(f'RLoss: {rtheta.data}')\n",
    "\n",
    "\n",
    "\n",
    "        if epoch %20==0:\n",
    "            #save adv ori advadv data\n",
    "            images, labels = method.adversarial_data \n",
    "            adv_data_list = data.numpy()\n",
    "            #adv_labels_list = labels.tolist()\n",
    "            \n",
    "            images, labels = method.adv_based_on \n",
    "            ori_data_list = data.numpy()\n",
    "            ori_labels_list = labels.numpy()\n",
    "            \n",
    "            #images, labels = method.adv_again \n",
    "            #advadv_data_list = data.numpy()\n",
    "            #advadv_labels_list = labels.tolist()\n",
    "            \n",
    "            \n",
    "            csv_filename = f'adv_data_labels_{epoch}_{batch_idx}.csv' if method.adv_again is None else f'adv_adv_data_labels_{epoch}_{batch_idx}.csv'\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "            \n",
    "                # Write header\n",
    "                header = [f'feature_{i}' for i in range(ori_data_list.shape[1])] + ['label']\n",
    "                writer.writerow(header)\n",
    "            \n",
    "                # Write data and labels for each group\n",
    "                rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "                rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "            \n",
    "                # Check if data_group3 is not None before including it\n",
    "                if method.adv_again is not None:\n",
    "                    images, labels = method.adv_again \n",
    "                    advadv_data_list = data.numpy()\n",
    "                    rows_group3 = np.column_stack((advadv_data_list, ori_labels_list))\n",
    "            \n",
    "                    # Combine data from all groups\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2, rows_group3))\n",
    "                else:\n",
    "                    # Combine data from the first two groups only\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2))\n",
    "            \n",
    "                writer.writerows(all_rows)\n",
    "\n",
    "        if epoch >5:\n",
    "\n",
    "            if batch_idx ==partialbatch:# train partial trainset\n",
    "                break\n",
    "        \n",
    "    \n",
    "\n",
    "    partial_train_loss /= (batch_idx+1)        \n",
    "    print(\"==\"*20)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - partial_train_loss: {partial_train_loss:.4f} \")\n",
    "    if epoch %5==0:\n",
    "        with torch.no_grad():\n",
    "                print(\"sorting training set\")\n",
    "                # for sorting Training set\n",
    "                sort_MAE=pd.DataFrame(columns = ['data', 'label', 'loss'])\n",
    "                method.model.eval()\n",
    "                for i in range(len(train_dataset.inputs)):\n",
    "                    inp = train_dataset.inputs[i]\n",
    "                    tar = train_dataset.labels[i]\n",
    "\n",
    "                    x = torch.tensor([inp.tolist()], dtype=torch.float32).to(device) \n",
    "                    y = torch.tensor(tar.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "                    output = method.model(x)\n",
    "                    loss = criterion(output, y).cpu()\n",
    "                    # Accumulate the training loss\n",
    "                    train_loss += loss.item() \n",
    "                    #print(f\"loss:       {loss}\")\n",
    "                    sort_MAE = sort_MAE.append({'data' : inp, \n",
    "                                                'label' :tar, \n",
    "                                                'loss' : loss},\n",
    "                                                ignore_index = True)\n",
    "                    #if i%1000==0:\n",
    "                    #    print(i)\n",
    "                #print(sort_MAE)\n",
    "                if epoch%50==0:\n",
    "                    sort_MAE.to_csv(f\"./train_set_sorting_{epoch}.csv\",index=False)\n",
    "                new_train = sort_MAE.sort_values(by=['loss'],ascending=False)\n",
    "                new_train_dataset = RecurrentDataset(new_train)\n",
    "                train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "                train_loss /= len(train_dataset.inputs)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Training loss: {train_loss:.4f} \")\n",
    "                print(\"==\"*20)\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            train_loss += loss.item() \n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch: [{(epoch + 1)}/{num_epochs}], TrainLoss: {train_loss}')\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset1_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt1_test_mse_loss += loss.item() \n",
    "    Rsplt1_test_mse_loss /= len(Rsplt_testset1_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset2_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt2_test_mse_loss += loss.item() \n",
    "    Rsplt2_test_mse_loss /= len(Rsplt_testset2_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset3_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt3_test_mse_loss += loss.item() \n",
    "    Rsplt3_test_mse_loss /= len(Rsplt_testset3_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset4_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt4_test_mse_loss += loss.item() \n",
    "    Rsplt4_test_mse_loss /= len(Rsplt_testset4_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset5_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt5_test_mse_loss += loss.item() \n",
    "    Rsplt5_test_mse_loss /= len(Rsplt_testset5_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt_test_mse_loss += loss.item() \n",
    "    Rsplt_test_mse_loss /= len(Rsplt_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Xshft_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Xshft_test_mse_loss += loss.item() \n",
    "    Xshft_test_mse_loss /= len(Xshft_test_loader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(pizeo_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        pizeo_test_mse_loss += loss.item() \n",
    "    pizeo_test_mse_loss /= len(pizeo_test_loader)\n",
    "            \n",
    "    for batch_idx, (data, target) in enumerate(statY_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        statY_test_mse_loss += loss.item() \n",
    "    statY_test_mse_loss /= len(statY_test_loader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(infoY_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        infoY_test_mse_loss += loss.item() \n",
    "    infoY_test_mse_loss /= len(infoY_test_loader)\n",
    "\n",
    "\n",
    "    rsplt_ave = np.average([\n",
    "                                Rsplt1_test_mse_loss,\n",
    "                                Rsplt2_test_mse_loss,\n",
    "                                Rsplt3_test_mse_loss,\n",
    "                                Rsplt4_test_mse_loss,\n",
    "                                Rsplt5_test_mse_loss\n",
    "                                ])\n",
    "\n",
    "    save =[]\n",
    "    if partial_train_loss < partial_train_best_loss:\n",
    "        partial_train_best_loss = partial_train_loss\n",
    "        save.append(\"partialTrain\")\n",
    "    if Rsplt1_test_mse_loss < Rsplt_testset1_best_loss:\n",
    "            Rsplt_testset1_best_loss  =  Rsplt1_test_mse_loss\n",
    "            save.append(\"Rsplt1\")\n",
    "    if Rsplt2_test_mse_loss < Rsplt_testset2_best_loss:\n",
    "            Rsplt_testset2_best_loss  =  Rsplt2_test_mse_loss\n",
    "            save.append(\"Rsplt2\")\n",
    "    if Rsplt3_test_mse_loss < Rsplt_testset3_best_loss:\n",
    "            Rsplt_testset3_best_loss  =  Rsplt3_test_mse_loss\n",
    "            save.append(\"Rsplt3\")\n",
    "    if Rsplt4_test_mse_loss < Rsplt_testset4_best_loss:\n",
    "            Rsplt_testset4_best_loss  =  Rsplt4_test_mse_loss\n",
    "            save.append(\"Rsplt4\")\n",
    "    if Rsplt5_test_mse_loss < Rsplt_testset5_best_loss:\n",
    "            Rsplt_testset5_best_loss  =  Rsplt5_test_mse_loss\n",
    "            save.append(\"Rsplt5\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if Xshft_test_mse_loss < Xshft_test_best_loss:\n",
    "            Xshft_test_best_loss  =  Xshft_test_mse_loss\n",
    "            save.append(\"Xshft\")\n",
    "    if pizeo_test_mse_loss < pizeo_test_best_loss:\n",
    "            pizeo_test_best_loss  =  pizeo_test_mse_loss\n",
    "            save.append(\"pizeo\")\n",
    "    if statY_test_mse_loss < statY_test_best_loss:\n",
    "            statY_test_best_loss  =  statY_test_mse_loss\n",
    "            save.append(\"statY\")\n",
    "    if infoY_test_mse_loss < infoY_test_best_loss:\n",
    "            infoY_test_best_loss  =  infoY_test_mse_loss\n",
    "            save.append(\"infoY\")\n",
    "\n",
    "    \n",
    "    # Stop the training process if the training loss has stopped decreasing or has started to increase\n",
    "    if train_loss < train_best_loss:\n",
    "        train_best_loss = train_loss\n",
    "        counter = 0\n",
    "        torch.save(method.model, f'IR3_epoch_{epoch}.pt')\n",
    "        torch.save(method.weights, f\"SAL_weight_{epoch}_gamma_{attack_gamma}.pt\")\n",
    "        save.append(\"Train\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        print(f'training Loss has not improved for {counter} epochs.')\n",
    "            \n",
    "   \n",
    "   \n",
    "    if rsplt_ave < Rsplt_test_best_loss:\n",
    "            Rsplt_test_best_loss  =  rsplt_ave\n",
    "            counter_val = 0\n",
    "            torch.save(method.model,\"IR3_SAL-bset-Rsplt_test_mse_loss.pt\")\n",
    "            save.append(\"Rsplt_AVE\")\n",
    "    else:\n",
    "        counter_val += 1\n",
    "        if counter_val >= 500:\n",
    "            print(f'Training stopped. Valid (rand) Loss has not improved for {500} epochs.')\n",
    "            break\n",
    "\n",
    "\n",
    "    entry = [epoch, \n",
    "                                    f\"{train_loss:.4f}\",\n",
    "                                    f\"{partial_train_loss:.4f}\",\n",
    "                                    f\"{Rsplt1_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt2_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt3_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt4_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt5_test_mse_loss:.4f}\",\n",
    "                                    f\"{rsplt_ave:.4f}\",\n",
    "\n",
    "                                    f\"{Xshft_test_mse_loss:.4f}\",\n",
    "                                    f\"{pizeo_test_mse_loss:.4f}\",\n",
    "                                    f\"{statY_test_mse_loss:.4f}\",\n",
    "                                    f\"{infoY_test_mse_loss:.4f}\",\n",
    "\n",
    "                                    f\"                         \", \n",
    "\n",
    "                                    f\"{train_best_loss:.4f}\", \n",
    "                                    f\"{partial_train_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset1_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset2_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset3_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset4_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset5_best_loss:.4f}\", \n",
    "\n",
    "                                    f\"{Rsplt_test_best_loss:.4f}\", \n",
    "                                    f\"{Xshft_test_best_loss:.4f}\", \n",
    "                                    f\"{pizeo_test_best_loss:.4f}\", \n",
    "                                    f\"{statY_test_best_loss:.4f}\", \n",
    "                                    f\"{infoY_test_best_loss:.4f}\", \n",
    "\n",
    "                                    save    ,\n",
    "                                    f'{attack_gamma}'\n",
    "                                    ]\n",
    "    loss_df.loc[len(loss_df)] = entry\n",
    "\n",
    "    loss_df.to_csv('IR3_plain-training_loss.csv', index=False)\n",
    "    epoch=epoch+1\n",
    "\n",
    "    # adjust gamma according to min(weight)\n",
    "    min_weight = 1e8\n",
    "    for i in range(method.weights.shape[0]):\n",
    "        if method.weights[i] > 0.0 and method.weights[i] < min_weight:\n",
    "            min_weight = method.weights[i]\n",
    "        if method.weights[i] < 0.0:\n",
    "            method.weights[i] = 1.0\n",
    "            zero_list.append(i)\n",
    "\n",
    "    attack_gamma = (1.0 / min_weight).data\n",
    "    if epoch <=5:\n",
    "        continue\n",
    "    method.weights -= lr_weight * deltaw.detach().reshape(method.weights.shape)\n",
    "    del rtheta\n",
    "    del dr_dx\n",
    "    del deltaw\n",
    "    del max_grad\n",
    "    del deltastep\n",
    "    del lr_weight\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.weights, f\"Whole_SAL_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.model, f\"IR3_epoch_{epoch}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('OOD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9527c1aa2febaf9c7bab479ad701127eaf7cbb2c19ab2b26ad381c63904c9a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
